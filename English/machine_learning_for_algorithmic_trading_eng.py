# -*- coding: utf-8 -*-
"""Machine_Learning_for_Algorithmic_Trading_ENG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lYvGM37kE_X-XyfgSIFDklw40_tmMwDc

# **Data Extraction**

###<u>Quotation Data</u>

In this first part of the code, we proceed with the extraction of variables known as OHLC (Open, High, Low, and Close) from NASDAQ quotation data. This is done using the "yfinance" library, which extracts data directly from the popular website Yahoo Finance.

As observed in the code output, we have variables such as Volume, Dividends, and Stock Splits in addition to OHLC. The index of the data is the date of the session in which those data points were recorded. The date range for the study spans 10 years, but initially, more data is extracted as the initial dates will be subsequently removed due to containing many null values in the technical indicators.

Documentation: https://pypi.org/project/yfinance/
"""

import yfinance as yf

# El ticker del NASDAQ Composite en Yahoo Finance es ^IXIC

nasdaq = yf.Ticker('^IXIC')

# Definir las fechas de inicio y fin en el formato "YYYY-MM-DD"

from_date = "2012-09-30"
to_date = "2023-06-30"

# Obtener el historial de precios en el rango de fechas

quote_data = nasdaq.history(start=from_date, end=to_date)


quote_data.head()

"""###<u>Technical Indicators</u>

Below is defined a function called "technical_indicators" that calculates up to 14 technical indicators based on the previously extracted data. The calculations are performed using mathematical formulas employed by the "pandas_ta" library.

Documentation: https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#momentum-indicators
"""

# "/dev/null" evita que el desarrollo de la instalación se imprima por pantalla

!pip install pandas_ta > /dev/null
import pandas_ta as ta

def technical_indicators(stock_data, MAshort=9, MAmedium=50, MAlong=200, RSIlength=14, MOMlength=10, BBlength=20, BBstd=2):

  #Media móvil simple (solo medio y largo plazo porque el corto plazo ya viene dentro del RSI y Bandas de Bollinger)
  sma50 = stock_data.ta.sma(length = MAmedium, append = True) #el append=True hace que los datos se añadan al data frame stock_data
  sma200 = stock_data.ta.sma(length = MAlong, append = True)

  #Media móvil exponencial
  ema9 = stock_data.ta.ema(length = MAshort, append = True)
  ema50 = stock_data.ta.ema(length = MAmedium, append = True)
  ema200 = stock_data.ta.ema(length = MAlong, append = True)

  #Media móvil ponderada
  wma9 = stock_data.ta.wma(length = MAshort, append = True)
  wma50 = stock_data.ta.wma(length = MAmedium, append = True)
  wma200 = stock_data.ta.wma(length = MAlong, append = True)

  #Relative Strength Index (RSI)
  rsi = stock_data.ta.rsi(length = RSIlength, append = True)
  rsi_df = rsi.to_frame(name = 'close') #tiene que llamarse "close" para que la detecte bien
  rsi_ma = rsi_df.ta.sma(length = RSIlength, append = True) #media movil de 14 del RSI

  #Moving Average Convergence Divergence (MACD)
  macd = stock_data.ta.macd(append = True)

  #Stochastic Oscillator
  stoch = stock_data.ta.stoch(append = True)

  #Momentum
  momentum = stock_data.ta.mom(length = MOMlength, append = True)

  #Bandas de Bollinger
  bbands = stock_data.ta.bbands(length = BBlength, bb_std = BBstd, append = True) #la banda central (BBM) es la SMA de 20 periodos en este caso

  return stock_data


quote_tech = technical_indicators(quote_data)

quote_tech.head()

"""###<u>Supports and Resistances</u>

Supports and resistances will be included in the dataframe through two binary variables that take a value of 1 when the closing price is at support or resistance levels, and 0 otherwise. These levels can be obtained using two functions that establish a support or resistance level based on a number of candles with decreasing or increasing lows or highs, and subsequent or previous.

In this case, to declare a support level, the requirement is that the immediately preceding 4 consecutive candles have decreasing lows, and the following 4 candles have increasing lows. The same applies to resistances but in reverse.

The output of the code is a list of tuples formed by the row number in which the support or resistance is declared and the price level of it.
"""

def support(stock_data, l, n1, n2):
  for i in range(l-n1+1, l+1):
    if(stock_data.Low[i]>stock_data.Low[i-1]):
      return 0
  for i in range(l+1, l+n2+1):
    if(stock_data.Low[i]<stock_data.Low[i-1]):
      return 0
  return 1


def resistance(stock_data, l, n1, n2):
  for i in range(l-n1+1, l+1):
    if(stock_data.High[i]<stock_data.High[i-1]):
      return 0
  for i in range(l+1, l+n2+1):
    if(stock_data.High[i]>stock_data.High[i-1]):
      return 0
  return 1

supports = []
resistances = []

n1, n2 = 4, 4

for row in range(n1, len(quote_data)):
  if support(quote_data, row, n1, n2):
    supports.append((row, quote_data.Low[row]))
  if resistance(quote_data, row, n1, n2):
    resistances.append((row, quote_data.High[row]))

print("Soportes:", supports)
print("Resistencias:",resistances)

"""Since this part involves an iterative trial-and-error process, we will print a chart on the screen with the results using the "mplfinance" library, which prints Japanese candlestick charts.

Documentation: https://pypi.org/project/mplfinance/
"""

!pip install mplfinance > /dev/null
import mplfinance as mpf

# Crear el gráfico de velas japonesas

stock_fig, axes = mpf.plot(quote_data, type='candle', style='charles', title=f'Gráfico de velas japonesas: NASDAQ Composite', ylabel='Price',
                           figsize=(15, 6), returnfig=True, warn_too_much_data=3000)

# Pintar las resitencias en verde

c = 0
while c < len(supports):
    axes[0].axhline(y=supports[c][1], color='green', linestyle='--', linewidth=1)
    c += 1

# Pintar las resitencias en naranja

c = 0
while c < len(resistances):
    axes[0].axhline(y=resistances[c][1], color='orange', linestyle='--', linewidth=1)
    c += 1

mpf.show()

"""As mentioned earlier, supports and resistances, rather than lines, are treated as zones or areas. In these zones, buyers and sellers enter into conflict until one of the two groups yields to the other. To ensure that the binary variables capture all supports and resistances without missing any, we will convert the lines into areas by adding a small margin on both sides equivalent to 1% of the price on each side."""

stock_fig, axes = mpf.plot(quote_data, type='candle', style='charles', title=f'Gráfico de velas japonesas: NASDAQ Composite',
                           ylabel='Price', figsize=(15, 6), returnfig=True, warn_too_much_data=3000)

bound = 0.01

# Date tiene que ser columna y no índice porque si no da error

quote_data.reset_index(inplace=True)

# Dibujar áreas para soportes
for support in supports:
    value = support[1]
    lower_bound = value * (1-bound)
    upper_bound = value * (1+bound)
    axes[0].fill_between(quote_data['Date'].index, lower_bound, upper_bound, color='green', alpha=0.2)

# Dibujar áreas para resistencias
for resistance in resistances:
    value = resistance[1]
    lower_bound = value * (1-bound)
    upper_bound = value * (1+bound)
    axes[0].fill_between(quote_data['Date'].index, lower_bound, upper_bound, color='orange', alpha=0.2)

# Volvemos a poner las fechas como índice
quote_data = quote_data.set_index('Date')


mpf.show()

"""
Given that the validity of supports and resistances is not infinite and the closer they are, the more validity they have, we will limit them on the X-axis.

The idea is to create variables that are related to the price and have a significant impact on it. A support formed in 2013 will have little or no validity in 2023, so we will limit the number of sessions for a support to be valid to 250 sessions, equivalent to one year of trading.

This limit of 250 sessions will be from the day the support or resistance is formed to the following 250 sessions. Given the limitation of this algorithm to detect some supports and resistances where the condition of 4 previous and subsequent descending or ascending highs and lows does not occur but they do exist, one might wonder if it is a good idea to make this duration period of support and resistance zones also retroactive. That is, instead of day of support < support zone < day of support+250, use the formula day of support - 125 < support zone < day of support+125. It has been visually verified that in this way many lost supports and resistances are recovered, thus enriching the information available to the model for training.

The problem with this approach is that we would be giving the model an advantage that will not exist in reality. If we use this support and resistance detection algorithm to put it into production in reality, we will never have information about future supports and resistances that have not yet formed because it is impossible. Therefore, the dummy variable will only take a value of 1 when it is in a support and resistance zone that, according to the detection algorithm, has already been created and exists, to equalize the conditions in which the model is trained with those of reality."""

stock_fig, axes = mpf.plot(quote_data, type='candle', style='charles', title=f'Gráfico de velas japonesas: NASDAQ Composite',
                           ylabel='Price', figsize=(15, 6), returnfig=True, warn_too_much_data=3000)

sessions = 250

# Dibujar áreas para soportes limitadas a +- 125 velas
for support in supports:
    x_axis = support[0]
    y_axis = support[1]
    lower_x = x_axis
    upper_x = x_axis + sessions
    lower_y = y_axis * (1-bound)
    upper_y = y_axis * (1+bound)
    axes[0].fill_betweenx([lower_y, upper_y], lower_x, upper_x, color='green', alpha=0.2)

# Dibujar áreas para resistencias limitadas a +- 125 velas
for resistance in resistances:
    x_axis = resistance[0]
    y_axis = resistance[1]
    lower_x = x_axis
    upper_x = x_axis + sessions
    lower_y = y_axis * (1-bound)
    upper_y = y_axis * (1+bound)
    axes[0].fill_betweenx([lower_y, upper_y], lower_x, upper_x, color='orange', alpha=0.2)


mpf.show()

"""Finally, the "dummy_sr" function creates a dictionary where the key is the row number, and the value is 1 if the closing price is in a support/resistance area or 0 if it is not."""

def dummy_sr(sr_list):
  dummy_supres = {}
  for index, closing_price in enumerate(quote_data['Close']):
    dummy_supres[index] = 0  # Inicializa con 0 por defecto
    if any(sr[1]*(1-bound) <= closing_price <= sr[1]*(1+bound) for sr in sr_list): #Márgenes en el precio (eje y)
      if any(sr[0] <= index <= sr[0] + sessions for sr in sr_list): #Márgenes en las fechas (eje x)
        dummy_supres[index] = 1
    else:
        dummy_supres[index] = 0
  return(dummy_supres)

dummy_supports = dummy_sr(supports)
dummy_resistances = dummy_sr(resistances)

print(dummy_supports)
print(dummy_resistances)

"""Both variables are included in the dataframe "quote_tech"
"""

import pandas as pd

# Crear un nuevo DataFrame para dummy_supports
dummy_supports_df = pd.DataFrame.from_dict(dummy_supports, orient='index', columns=['dummy_support'])

# Crear un nuevo DataFrame para dummy_resistances
dummy_resistances_df = pd.DataFrame.from_dict(dummy_resistances, orient='index', columns=['dummy_resistance'])

# Resetear el índice para poder hacer el join
quote_tech.reset_index(inplace=True)

# Combinar los DataFrames usando el índice como clave
quote_tech = quote_tech.join(dummy_supports_df, how='outer')
quote_tech = quote_tech.join(dummy_resistances_df, how='outer')

# Volver a poner la fecha como índice
quote_tech = quote_tech.set_index('Date')

quote_tech.drop('index', axis=1, inplace=True)

quote_tech.head()

"""###<u>Macroeconomic Indicators</u>

For obtaining the macroeconomic variables, we will be using the FRED API. Once the library is imported and the API key is entered, for which we have registered with our email address, we perform an initial search for the employment indicator ordered by popularity so that the most commonly used indicators appear first.

API Documentation: https://fred.stlouisfed.org/docs/api/fred/

Library Documentation: https://frb.readthedocs.io/en/latest/
"""

!pip install fredapi > /dev/null
from fredapi import Fred

fred = Fred(api_key = '9868db94d0ab2e936e5a2812121fef5d')

sp_search = fred.search('Unemployment', order_by='popularity')

sp_search.head()

"""In this case, the indicator we are looking for has the identifier "UNRATE" (column "series id"), but the frequency is monthly and our data is daily. Therefore, we will try to find an employment indicator by filtering for daily frequency."""

unemp_search = fred.search('unemployment', filter=('frequency', 'Daily'))

unemp_search

"""Since there is no daily frequency available, we will proceed with monthly data. The extraction of macroeconomic data is done using a dictionary with the names and identifiers of each indicator and with a function. The output of the function will be two dataframes, one for daily indicators and another for monthly indicators.

All monthly indicators have 130 rows for the selected dates. Regarding daily indicators, most have 2805 rows, but there are some that have more, probably due to holidays or the nature of the indicators themselves. In these cases, the code removes the excess data, which are those for which there is no match in the date index.
"""

#diccionario de índices con sus nombres y claves para la API

indicators = {'unemployment':'UNRATE',
              '10y-2y yield curve':'T10Y2Y',
              'volatility':'VIXCLS',
              'inflation':'CORESTICKM159SFRBATL',
              'consumer expectation':'CSCICP03USM665S',
              'industrial production':'INDPRO',
              '5-year interest rate': 'DGS5',
              'federal interest rate': 'DFF',
              'junk bonds spread': 'BAMLH0A0HYM2'}

def fred_import(indicator_dict):
  indicators_monthly = pd.DataFrame()
  indicators_daily = pd.DataFrame()
  for key, value in indicator_dict.items():
    indicator = fred.get_series(series_id = value, observation_start = from_date, observation_end = to_date)
    info = fred.get_series_info(series_id=value) #información sobre el indicador, incluida su frecuencia
    if info['frequency'] == 'Monthly':
      indicators_monthly[key] = indicator
    else:
      indicators_daily[key] = indicator

  return indicators_monthly, indicators_daily

macro_monthly, macro_daily = fred_import(indicators)

# Dataframe de variables macroeconómicas mensuales

macro_monthly

# Dataframe de variables macroeconómicas diarias

macro_daily

"""###<u>Other Assets Quotation</u>

To conclude the data extraction process, we will import data on the quotation of other assets that may be correlated with the NASDAQ, such as stock indices from the US and other countries, gold and oil futures.

The data obtained will consist of daily closing prices. Due to the time difference in the trading sessions of global indices, it is necessary to remove the timestamp from the index and leave only the date so that all assets can be in the same dataframe.
"""

from datetime import datetime

indices_world = {'S&P500':'^GSPC',
                'Wilshire 5000':'^FTW5000',
                'Dow Jones Industrial Average':'^DJI',
                'Nikkei 225':'^N225',
                'Euro STOXX 50':'^STOXX50E',
                'Hang Seng Index':'^HSI',
                'Shanghai Stock Exchange':'000001.SS',
                'FTSE 100':'^FTSE',
                'Gold futures':'GC=F',
                'Crude oil futures':'CL=F',
                }

def yf_import(indicator_dict):
  other_assets = pd.DataFrame()
  for key, value in indicator_dict.items():
    asset_ticker = yf.Ticker(value)
    hist = asset_ticker.history(start=from_date, end=to_date)
    # Debido a la diferencia horaria entre índices, eliminamos las horas con la siguiente línea
    hist.index = hist.index.strftime('%Y-%m-%d')
    other_assets[key] = hist['Close']

  return other_assets

assets_quote = yf_import(indices_world)

# Dataframe con precios de cierre diarios de diferentes índices y materias primas

assets_quote

"""###<u>Connection to the SQL database</u>

Once the raw data has been extracted, it is loaded into an SQL database. The objective is to store the data in a persistent database for performing checkpoints throughout the code. Non-relational databases will not be necessary as the data has a relational structure.

For compatibility and pragmatism reasons, a free SQL database in the cloud has been chosen, and its credentials are stored in the config dictionary. This free database has been obtained from the website db4free.net (https://www.db4free.net/index.php?language=es). MySQL has been chosen as the open-source relational database management system. For managing and monitoring the database, phpMyAdmin (https://www.phpmyadmin.co) is used, which is a web-based database administration tool used to manage MySQL databases. It is not a database itself but a web interface that allows performing administrative tasks on MySQL databases in a simpler and more visual way.
"""

# Instalación e importación de la librería myql.connnector

!pip install protobuf==3.20.3 mysql-connector-python > /dev/null
import mysql.connector

"""Documentation mysql.connector: https://dev.mysql.com/doc/connector-python/en/"""

# Se configura la conexión con las credenciales de la base de datos
config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

# Se establecer la conexión con mensajes de confirmación
try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

# Se lanza un mensaje de error en caso de que se produzca un error

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

"""First, we will proceed with the creation of tables within the database "sql7644199", one for each of the dataframes that we need to upload. Since in this case, the indices contain valuable information that we do not want to lose (the dates), we will convert the four indices into columns to avoid any loss of information due to incompatibility."""

# Definición de dataframes y tablas SQL

dataframes = [quote_tech, macro_monthly, macro_daily, assets_quote]
table_names = ['quote_tech', 'macro_monthly', 'macro_daily', 'assets_quote']

for df in dataframes:
  df.reset_index(inplace=True)

"""We define a function that creates tables in the SQL database from SQL queries that iterate through the dataframes and their columns, classifying them according to the data type they contain."""

def crear_tabla_desde_dataframe(df, tabla, connection):
    cursor = connection.cursor()
    create_table_query = f"CREATE TABLE {tabla} (id INT AUTO_INCREMENT PRIMARY KEY"

    for columna, tipo in zip(df.columns, df.dtypes):
        if tipo == "float64":
            create_table_query += f", `{columna}` FLOAT"
        elif tipo == "int64":
            create_table_query += f", `{columna}` INT"
        elif tipo == "datetime64[ns]":
            create_table_query += f", `{columna}` DATE"
        elif tipo == "bool":
            create_table_query += f", `{columna}` BOOLEAN"

    create_table_query += ");"
    cursor.execute(create_table_query)

# Llamada a la función con las listas anteriores

for df, table_name in zip(dataframes, table_names):
    crear_tabla_desde_dataframe(df, table_name, connection)

"""Finally, we use the sqlalchemy library to populate the created tables with the data from the dataframe. The connection is closed after the loading is complete to save resources.

Sqlalchemy library documentation: https://docs.sqlalchemy.org/en/20/intro.html#documentation-overview
"""

from sqlalchemy import create_engine

# Configuración de la conexión a la base de datos MySQL con las credenciales y el host

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

# if_exists='replace' hace que, si existe la base de datos, se reemplace con los nuevos datos

def cargar_dataframe_en_tabla(df, tabla):
    df.to_sql(tabla, con = engine, if_exists='replace', index=False)

# index=False hace que no se tenga en cuenta el índice, ya que este figura como una columna más

cargar_dataframe_en_tabla(quote_tech, 'quote_tech')
cargar_dataframe_en_tabla(macro_monthly, 'macro_monthly')
cargar_dataframe_en_tabla(macro_daily, 'macro_daily')
cargar_dataframe_en_tabla(assets_quote, 'assets_quote')

# Se cierra la conexión

connection.close()
engine.dispose()

"""# **Data Transformation and Integration**

In this second phase of the project, we will clean and transform the raw data, and then integrate the four datasets into a single dataset.

###<u>Connection to the SQL database and retrieval of dataframes</u>

To retrieve the data without having to execute the previous code first, the connection is established again, as done previously.
"""

!pip install protobuf==3.20.3 mysql-connector-python > /dev/null
import mysql.connector

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

"""Using the SELECT * FROM query, the data is retrieved one by one from the tables. Once all the data is extracted and dataframes are created, the connection is closed."""

import pandas as pd
from sqlalchemy import create_engine, text

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

conn = engine.connect()

# Definición de la función para importar una tabla de SQL y convertirla en un DataFrame

def importar_tabla_a_dataframe(tabla, engine):
    query = text(f'SELECT * FROM {tabla}')
    df = pd.read_sql(query, con=conn)
    return df

quote_tech_sql = importar_tabla_a_dataframe('quote_tech', engine)
macro_monthly_sql = importar_tabla_a_dataframe('macro_monthly', engine)
macro_daily_sql = importar_tabla_a_dataframe('macro_daily', engine)
assets_quote_sql = importar_tabla_a_dataframe('assets_quote', engine)

# Cerrar la conexión correctamente

conn.close()
connection.close()

"""###<u>Review and Transformation</u>

Now we will proceed with the review and transformation of the dataframes one by one, starting with the dataframe containing the quotation data and technical indicators.
"""

quote_tech_sql

print(quote_tech_sql.columns)

# Calcular la frecuencia de cada valor en las columnas 'Dividends' y 'Stock Splits'

frecuencia_div = quote_tech_sql['Dividends'].value_counts()
frecuencia_split = quote_tech_sql['Stock Splits'].value_counts()

print('Valores de Dividends:', frecuencia_div, '\nValores de Stock Split:', frecuencia_split)

"""The columns "Dividends" and "Stock Splits" are constants with no explanatory value for the daily variation in the target variable, so we will delete them along with the "index" column."""

drop_columns = ['Dividends', 'Stock Splits']

quote_tech_sql = quote_tech_sql.drop(drop_columns, axis=1)

print(quote_tech_sql.columns)

"""We set the "date" column as the index, but first we need to ensure that all date columns are in date format to avoid integration problems. If any column turns out to be of a different type, it will need to be modified."""

print(quote_tech_sql['Date'].dtypes)
print(macro_monthly_sql['index'].dtypes)
print(macro_daily_sql['index'].dtypes)
print(assets_quote_sql['Date'].dtypes)

quote_tech_sql = quote_tech_sql.set_index('Date')

quote_tech_sql.head()

"""Moving on to the next dataframe."""

macro_monthly_sql

"""Since the same modifications need to be made for the next dataframe, we define a function to avoid repeating the code."""

def transform_df(df):
  df = df.rename(columns={'index': 'Date'})

  df = df.set_index('Date')

  return df

macro_monthly_sql = transform_df(macro_monthly_sql)

macro_monthly_sql.head()

macro_daily_sql

macro_daily_sql = transform_df(macro_daily_sql)

macro_daily_sql.head()

assets_quote_sql

assets_quote_sql['Date'] = pd.to_datetime(assets_quote_sql['Date'])

# Cambiamos el formato de la columna 'Date' a fecha para que sea compatible con los demas en el merge

assets_quote_sql = assets_quote_sql.set_index('Date')

assets_quote_sql.head()

# Comprobamos que efectivamente el índice es tipo fecha

print(type(assets_quote_sql.index))

"""###<u>Integration</u>

Before performing the integration, since it will be horizontal and not vertical, let's visualize the number of rows for each of the 4 dataframes.
"""

print("Número de filas en quote_tech_sql:", quote_tech_sql.shape[0])
print("Número de filas en macro_monthly_sql:", macro_monthly_sql.shape[0])
print("Número de filas en macro_daily_sql:", macro_daily_sql.shape[0])
print("Número de filas en assets_quote_sql:", assets_quote_sql.shape[0])

"""To perform the horizontal integration, we will first join the 3 dataframes with the most rows. When using the merge function without the 'how='outer'' parameter, only rows whose indices match all indices of the dataframes being merged will be joined. If a row has an index that does not match at least one of the dataframes, it will be eliminated.

As shown below, the resulting dataframe from the union has 2703 rows, the same as the NASDAQ quotation data, while the excess in macro_daily_sql has been eliminated.
"""

# Utilizamos left_index=True y right_index=True para que se unan por los índices

nasdaq_df = pd.merge(quote_tech_sql, macro_daily_sql, left_index=True, right_index=True)
nasdaq_df = pd.merge(nasdaq_df, assets_quote_sql, left_index=True, right_index=True)

nasdaq_df

"""For the union of the dataframe with macro_monthly_sql, we need to use the 'how'='outer' parameter so that it assigns a null value to all rows whose index does not match the other index. If we didn't use this parameter, we would end up with a dataframe of at most 130 rows.

The result is a dataframe of 2749 rows. This increase is due to the fact that in the monthly indices, there are values on days (probably holidays or weekends) when the NASDAQ is not trading.
"""

# Utilizamos how='outer' para que los índices no coincidentes se rellenen con valores nulos

nasdaq_df = pd.merge(nasdaq_df, macro_monthly_sql, left_index=True, right_index=True, how='outer')

nasdaq_df

"""###<u>Handling Missing Values</u>

First, we count the total number of null values for each column.
"""

nasdaq_df.isnull().sum()

"""The monthly macroeconomic variables are the ones with the most null values. This is because they only have one value per month. We will fill the rest of the days in the month with the same value as the first day of the month, as the data that comes out on the first day is what investors take into account for the entire month."""

monthly = ['unemployment', 'inflation', 'consumer expectation', 'industrial production']

for col in monthly:

    # Rellenar los valores nulos utilizando el método "forward fill" (ffill)

    nasdaq_df[col] = nasdaq_df[col].fillna(method='ffill')

    print('Nulos en', col, ':', nasdaq_df[col].isna().sum())

"""In the technical indicators variables, most of the nulls are in the first rows. This is because in the first few days, there is not enough data to calculate them. For example, the 200-day moving average will not be available until after the first 200 sessions. Therefore, we will delete the first rows."""

nasdaq_df = nasdaq_df.iloc[203:]

nasdaq_df.isnull().sum()

"""Most variables now have 42 nulls. This is probably due to the merge with the macro_monthly dataframe. Let's remove the rows where there are nulls in the 'Close' variable."""

nasdaq_df = nasdaq_df.dropna(subset=['Close'])

nasdaq_df.isnull().sum()

nasdaq_df

"""
After these modifications, the dataset has lost more than 200 records. To avoid losing even more information, we will opt to fill in the missing data instead of deleting it, as this would result in losing hundreds of rows that will be necessary for training the models.

The chosen method for this is the interpolate method from the pandas library. The interpolate function with the 'time' method in pandas is used to perform time-based interpolation on a time series or DataFrame containing time-based data, such as ours containing financial time series. This technique is especially useful when filling null values in time series where the data is related to time.

The 'time' method works as follows:

1. It examines the data in the time series for null values.

2. When it finds a null value, it looks for the nearest data points in time that have non-null values before and after the null value.

3. It uses linear interpolation to estimate the missing value based on the surrounding non-null values and their relative temporal location.

4. It fills the null value with the calculated estimate using linear interpolation.

5. It repeats this process for all null values in the time series.

Pandas documentation (interpolate): https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html"""

import pandas as pd

# Utilizamos la función interpolate con el método 'time' para rellenar los valores nulos basados en el tiempo

nasdaq_inter = nasdaq_df.interpolate(method='time')

print(nasdaq_inter.isnull().sum())

"""Finally, we have managed to keep the number of rows while eliminating all null values."""

nasdaq_inter.shape

"""We display the final result on the screen, resetting the index so as not to lose the dates when uploading the dataframe to the SQL database."""

nasdaq_inter.reset_index(inplace=True)

nasdaq_inter

"""Finally, we create a table in the database and upload the dataframe to make a checkpoint. We can recycle the code used in the data extraction for this purpose."""

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

def crear_tabla_desde_dataframe(df, tabla, connection):
    cursor = connection.cursor()

    create_table_query = f"CREATE TABLE {tabla} (id INT AUTO_INCREMENT PRIMARY KEY"

    for columna, tipo in zip(df.columns, df.dtypes):
        if tipo == "float64":
            create_table_query += f", `{columna}` FLOAT"
        elif tipo == "int64":
            create_table_query += f", `{columna}` INT"
        elif tipo == "datetime64[ns]":
            create_table_query += f", `{columna}` DATE"
        elif tipo == "bool":
            create_table_query += f", `{columna}` BOOLEAN"

    create_table_query += ");"
    cursor.execute(create_table_query)

# Llamada a la función con las listas anteriores
crear_tabla_desde_dataframe(nasdaq_inter, 'nasdaq', connection)

from sqlalchemy import create_engine

# Configuración de la conexión a la base de datos MySQL con las credenciales y el host

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

# if_exists='replace' hace que, si existe la base de datos, se reemplace con los nuevos datos

def cargar_dataframe_en_tabla(df, tabla):
    df.to_sql(tabla, con = engine, if_exists='replace', index=False)

# index=False hace que no se tenga en cuenta el índice, ya que este figura como una columna más

cargar_dataframe_en_tabla(nasdaq_inter, 'nasdaq')

# Se cierra la conexión

connection.close()
engine.dispose()

"""# **Exploratory Data Analysis**

Before proceeding with feature engineering, let's perform an exploratory data analysis (EDA) to gain a deeper understanding of the variables and ask some questions of the data.

To avoid having to run all the previous code again, let's import the 'nasdaq' table from our persistent database.

###<u>SQL Data Import</u>
"""

!pip install protobuf==3.20.3 mysql-connector-python > /dev/null
import mysql.connector

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

import pandas as pd
from sqlalchemy import create_engine, text

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

conn = engine.connect()

# Importamos la tabla de SQL y la convertimos en un DataFrame

query = text(f'SELECT * FROM nasdaq')
nasdaq_sql = pd.read_sql(query, con=conn)

# Cerrar la conexión correctamente

conn.close()
connection.close()

# Volvemos a fijar la fecha como índice

nasdaq = nasdaq_sql.set_index('Date')

"""###<u>Initial Statistical Summary</u>

With the 'shape' attribute, we retrieve the number of rows and columns of the dataframe, 2504 and 46, respectively.
"""

nasdaq.shape

"""We visualize all the variables in the dataframe."""

nasdaq.columns

"""All these variables are of type float, including the binary variables for support and resistance. We could change the type of these columns to boolean since this would better reflect the binary nature of the variables and occupy less memory space. However, changing them to boolean will make the analysis of these variables in the EDA more difficult, so for now, we will leave them as float."""

nasdaq.dtypes

"""When we print a dataframe with many columns, many of them are hidden to abbreviate it. Let's increase the abbreviation limit to 50 columns so we can visualize all the columns of our dataframe and the first 10 rows."""

# Con esta linea podemos hacer que salgan todas las columnas por pantalla, hasta un máximo de 50 columnas
pd.set_option('display.max_columns', 50)

nasdaq.head(10)

"""We check if there are any values of the 'Close' variable that are repeated multiple times. This could give us hints that there is an error."""

# Número de veces que se repite cada valor

nasdaq['Close'].value_counts()

"""The highest number of repeated values is 2, so it seems that there are no duplicate rows. We can ensure this by using the following line of code."""

# Comprobar si hay filas duplicadas

nasdaq.loc[nasdaq.duplicated()]

"""This study focuses on the sign of the percentage difference between the closing price and the opening price of the next day. At this point, we create the target variable to analyze it in the EDA.

The reason why we do not use the difference between the current closing price and the closing price of the previous day is that we want to check the feasibility of a strategy consisting of buying every day at the opening session and selling when it closes. Buying at the closing price is not feasible in reality because when it is known, the market is already closed, and it is only possible to buy the next day at the opening price. A strategy could be used to buy at the opening price only if it is equal to or lower than the closing price of the previous day, but this would greatly limit the days of operation.

The 'return' variable measures the difference in percentage terms between the opening price and the closing price. Absolute terms are not used because these differences increase with the increase in the value of the NASDAQ. Therefore, if we want to analyze this variable over a period of 10 years in an equitable manner, this is a solution.

The 'target_return' variable takes the value 1 if 'return' is positive and 0 if it is negative. It is shifted down one position to indicate 1 if the return of the next day is positive and 0 if it is negative.
"""

# Creamos la variable objetivo rodándola una posición.

nasdaq['return'] = (nasdaq['Close'] - nasdaq['Open'])/nasdaq['Open']

nasdaq['target_return'] = nasdaq['return'].apply(lambda x: 1 if x > 0 else 0)

nasdaq['target_return'] = nasdaq['target_return'].shift(-1)

nasdaq.dropna(inplace=True)

nasdaq

"""We print a dataframe with the basic statistics of each variable using the describe method. Among these, we find the total number of values, the mean, the standard deviation, the minimums and maximums, and the 3 quartiles.

1. It can be observed that certain technical indicators, such as RSI or the stochastic oscillator, are bounded between 0 and 100.

2. From the mean of the 'dummy_support' and 'dummy_resistance' variables, we can infer that 17.30% of the time the NASDAQ price closes in a support zone, while 17.62% of the time it closes in a resistance zone.

3. The price of crude oil futures has reached negative values. This likely occurred in March 2020, during the pandemic.

4. The 75th percentile of volatility is at 21.30, while the maximum it has reached is 82.69, indicating that this variable has likely experienced a significant outlier.

5. The average unemployment and inflation rates over the past 10 years in the United States have been 5.06% and 2.81%, respectively. These variables have also experienced peaks, as their 75th percentiles are 5.80% and 2.64%, but their maximums are 14.70% and 6.62%.

6. The average daily return of the NASDAQ is 0.03%. Most of the time it has been between -0.44% and 0.57%, although it has reached a maximum of 7.04% and a minimum of -6.60%. 54.29% of the time, the price has closed above its daily opening.

In general, in most variables, the distance between maximums and minimums and the quartiles suggests the existence of outliers. In the next section, a more in-depth analysis of these will be conducted.
"""

nasdaq.describe()

"""Now we visualize the count of the 'target_return' variable using the Matplotlib and Seaborn libraries.

Matplotlib documentation: https://matplotlib.org/stable/index.html

Seaborn documentation: https://seaborn.pydata.org/
"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style="whitegrid")
plt.figure(figsize=(8, 6))

sns.countplot(x='target_return', data=nasdaq, palette='Set2')

plt.title('Total de velas positivas y negativas')
plt.ylabel('Conteo')

plt.show()

"""To visualize the proportion, the best option is to use a pie chart."""

# Calculamos el recuento de cada valor en 'target_return'
counts = nasdaq['target_return'].value_counts()

# Creamos un pie chart en verde para las velas positivas y rojo para las negativas
plt.figure(figsize=(6, 6))
plt.pie(counts, labels=['Velas Positivas', 'Velas Negativas'],
        autopct='%1.1f%%', startangle=140, colors=['#58FA82', '#FF9999'])

plt.title('Porcentaje de velas positivas y negativas')

plt.show()

"""###<u>Outliers</u>

To confirm the existence of possible outliers that we observed in the statistical summary, we will obtain a Box Plot of each of the float variables in the dataframe.
"""

# Crear una matriz con múltiples subplots de 9x5
fig, axes = plt.subplots(nrows=9, ncols=5, figsize=(18, 25))

# Aplanar la matriz de subplots para recorrerla fácilmente
axes = axes.flatten()

# Eliminamos las variables dummies
del_cols = ['target_return', 'dummy_resistance', 'dummy_support']

nasdaq_boxplot = nasdaq.drop(del_cols, axis=1)

# Bucle for para generar un boxplot para cada variable
for i, col in enumerate(nasdaq_boxplot.columns):
    ax = axes[i]
    nasdaq_boxplot.boxplot(column=col, ax=ax)
    ax.set_xlabel('')
    ax.set_ylabel('')

# Ajustar el espacio entre los subplots
plt.tight_layout()

plt.show()

"""Using the definition of outliers based on the interquartile range, we can confirm the abundance of these outlier values in several variables, as shown in the table below. The decision has been made not to remove the outlier values for the following reasons:

1. Removing these outlier values would result in too much information loss, as hundreds of records would need to be deleted.

2. These outlier values do not seem to be due to measurement errors.

3. Outlier values may contain valuable information about unusual events or extreme events in financial data. These events may be of interest, as they could indicate significant changes in the market, important news, or unexpected events that may impact our investment strategy.

4. Outlier values are part of the nature of financial data. Financial markets are volatile and subject to rapid and unexpected changes. Removing all outlier values could lead to an unrealistic representation of the data and conceal the true nature of the market.

5. In some cases, financial models may benefit from the inclusion of outlier values. Robust models may be able to handle outlier values without significantly affecting their predictions. Additionally, outlier values can help detect structural changes in the data.

6. Removing outlier values can distort the metrics and indicators we will use for the models.
"""

# Crear una función para contar outliers basados en el rango intercuartil (IQR)
def count_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = (series < lower_bound) | (series > upper_bound)
    return outliers.sum()

# Calcular el número de outliers por cada variable en el DataFrame
outliers_count = nasdaq_boxplot.apply(count_outliers)

outliers_matrix = pd.DataFrame({'Número de Outliers': outliers_count})


print(outliers_matrix)

"""Having decided not to directly remove the outlier values, it will be necessary to use a normalization method that is robust to outliers. Additionally, we will partially mitigate the effects of outlier values, without directly removing them, through noise reduction techniques in feature engineering.

###<u>Normality</u>

To analyze the normality of the variables, the first step is to plot a histogram of each variable indicating its kurtosis and skewness in the legend.

At first glance, it appears that there are many variables that do not follow a normal distribution. Among those that do appear to follow a normal distribution, we can find the 3 variables of the MACD technical indicator, the Momentum technical indicator, and the daily return of the NASDAQ. It is worth noting that the latter most closely resembles a Gaussian distribution, with (approximately) a mean of 0 and standard deviation of 0.1. The kurtosis and skewness are close to 3 and 0 respectively, indicating a mesokurtic and symmetric bell shape around its mean, typical of a standard normal distribution
"""

# Crear una figura con múltiples subplots
fig, axes = plt.subplots(nrows=16, ncols=3, figsize=(18, 60))

# Aplanar la matriz de subplots para recorrerla fácilmente
axes = axes.flatten()

# Bucle para generar histogramas con kurtosis y skewness para cada variable
for i, col in enumerate(nasdaq.columns):
    ax = axes[i]
    data = nasdaq[col]
    kurtosis = data.kurtosis()
    skewness = data.skew()

    sns.histplot(data, kde=True, color='blue', ax=ax)
    ax.legend([f'Kurtosis: {kurtosis:.2f}\nSkewness: {skewness:.2f}'],
               fontsize=16, facecolor='lightgray', edgecolor='black', framealpha=1)
    ax.set_title(col)
    ax.set_xlabel('')
    ax.set_ylabel('')

# Ajustar el espacio entre los subplots
plt.tight_layout()


plt.show()

"""To visualize the daily return of the NASDAQ (the 'return' variable) more closely, we will plot a histogram adding the mean and standard deviations to analyze its possible normality.

Assuming that this frequency follows a normal distribution, 68% of the values should fall within one standard deviation, and 95% should fall within two standard deviations. At first glance, this condition seems to be met, but further tests will be needed before confirming the normality of its distribution.

Documentation for 'norm' in scipy.stats: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html

Documentation for NumPy: https://numpy.org/doc/
"""

from scipy.stats import norm
import numpy as np
plt.style.use('ggplot')

plt.figure(figsize=(8, 6))

# Plotear el histograma de 'return'
sns.histplot(nasdaq['return'], kde=True, color='blue', element='step', lw=1)

# Calcular la media y la desviación estándar de 'return'
media = nasdaq['return'].mean()
desviacion_estandar = nasdaq['return'].std()

# Crear un array de valores X para la campana de Gauss
x = np.linspace(nasdaq['return'].min(), nasdaq['return'].max(), 1000)
# Calcular los valores Y de la campana de Gauss usando la media y la desviación estándar
y = norm.pdf(x, media, desviacion_estandar)

# Plotear la campana de Gauss en el mismo gráfico
plt.plot(x, y)
plt.axvline(media, color='red', linestyle='dashed', linewidth=2, label='Media')
plt.axvline(media + desviacion_estandar, color='green', linestyle='dashed', linewidth=2, label='1 Desviación Estándar')
plt.axvline(media - desviacion_estandar, color='green', linestyle='dashed', linewidth=2)
plt.axvline(media + 2 * desviacion_estandar, color='purple', linestyle='dashed', linewidth=2, label='2 Desviaciones Estándar')
plt.axvline(media - 2 * desviacion_estandar, color='purple', linestyle='dashed', linewidth=2)
plt.axvline(media + 3 * desviacion_estandar, color='orange', linestyle='dashed', linewidth=2, label='3 Desviaciones Estándar')
plt.axvline(media - 3 * desviacion_estandar, color='orange', linestyle='dashed', linewidth=2)

plt.title('Distribución normal del rendimiento diario del NASDAQ')
plt.xlabel('return')
plt.legend()

plt.show()

"""To continue with the normality analysis, we will visualize a Q-Q plot. In this plot, the non-normality of the distribution of many variables is confirmed. In many variables, substantial deviations from the diagonal line can be observed, rejecting the null hypothesis of similarity with a normal distribution.

Documentation for the Statsmodels library: https://www.statsmodels.org/stable/index.html
"""

import statsmodels.api as sm
plt.style.use('default')

fig, axes = plt.subplots(nrows=12, ncols=4, figsize=(18, 30))

axes = axes.flatten()

# Bucle para generar QQ plots para cada variable
for i, col in enumerate(nasdaq.columns):
    ax = axes[i]
    data = nasdaq[col]
    sm.qqplot(data, line='s', ax=ax)
    ax.set_title(col)

plt.tight_layout()

plt.show()

"""Finally, we perform the Shapiro-Wilk test to determine the normality of the variables.

The Shapiro-Wilk test is one of the most powerful tests for checking the normality of a sample. It is widely recommended due to its greater power to detect deviations from normality compared to other normality tests.

The null hypothesis (H0) is that the data are normally distributed. The alternative hypothesis (H1) is that the data are not normally distributed. The test calculates a test statistic (W) and a p-value.

If the p-value is less than a chosen significance level (in this case, we have chosen 0.05, but the results are the same for 0.01), the null hypothesis is rejected, indicating that the data do not follow a normal distribution.

As can be seen in the following dataframe, none of the p-values exceeds the significance threshold established, so the null hypothesis of normal distribution must be rejected for all variables.

Documentation for the Scipy library: https://docs.scipy.org/doc/scipy/
"""

from scipy import stats

shapiro_wilk_results = []

# Bucle para realizar la prueba de Shapiro-Wilk para cada variable
for col in nasdaq.columns:
    data = nasdaq[col]
    statistic, p_value = stats.shapiro(data)
    is_normal = 'Si' if p_value > 0.05 else 'No'
    shapiro_wilk_results.append((col, statistic, p_value, is_normal))

shapiro_wilk_df = pd.DataFrame(shapiro_wilk_results, columns=['Variable', 'Statistic', 'p-value', 'Distribución Normal'])

shapiro_wilk_df.set_index('Variable', inplace=True)

shapiro_wilk_df

"""Having confirmed the evident abundance of variables with non-normal distributions after conducting three different tests, it will be necessary to use algorithms robust to non-normality and refrain from applying normalization methods in the preprocessing phase that assume the normal distribution of variables.

###<u>Correlation</u>

Measuring correlation is a useful step to explore the relationship between variables and the target variable, but it should not be considered as the sole measure of predictive capability, as correlation does not necessarily imply predictive capability or causality. We print a correlation matrix to see how variables are related to each other.
"""

correlation_matrix = nasdaq.corr()

correlation_matrix

"""We print a heatmap to better visualize the correlation matrix."""

plt.figure(figsize=(50, 35))

# annot=True para que muestre las etiquetas
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", annot_kws={"size": 17})

plt.xticks(fontsize=24)
plt.yticks(fontsize=24)

plt.show()

"""Next, we are going to filter the top 15 most and least correlated pairs of variables. Here are some conclusions we can draw from the lists and the heatmap:

1. In general, as expected, the OHLC variables and moving averages show high correlation among themselves.

2. Except for the Hang Seng Index, other indices are highly correlated with the NASDAQ's OHLC variables.

3. The future of gold shows a strong positive correlation with the NASDAQ, while the future of oil shows a very weak correlation.

4. The 10y-2y yield curve and the federal interest rate have a strong negative correlation because as central bank interest rates rise, the yield on the 2-year bond relative to the 10-year bond increases. This is due to the negative correlation between bond yields and interest rates and because longer-term bonds are generally more sensitive to changes in interest rates.

5. An interesting factor is that both volatility and junk bond risk premiums are measures of market fear at a given time. However, there is not a very strong correlation between these two variables.

6. Both central bank interest rates and the 5-year bond interest rate show a positive correlation with inflation and a negative correlation with unemployment. This is because central banks often raise interest rates to counteract inflationary pressures, while they tend to lower rates in response to negative labor market data.

7. Consumer expectations are negatively correlated with gold, volume, NASDAQ, S&P500, and inflation. The negative correlation with inflation may be because an inflationary environment is perceived as harmful to the economy. The rest can be explained due to the leading indicator nature of consumer expectations, similar to the 10y-2y yield curve indicator.
"""

# Obtener las correlaciones en orden ascendente (menos correlacionadas)
sorted_correlations_asc = correlation_matrix.unstack().sort_values(ascending=True)

# Obtener las correlaciones en orden descendente (más correlacionadas)
sorted_correlations_desc = correlation_matrix.unstack().sort_values(ascending=False)

# Filtrar las correlaciones de variables con sí mismas (diagonales)
sorted_correlations_asc = sorted_correlations_asc[sorted_correlations_asc.index.get_level_values(0) != sorted_correlations_asc.index.get_level_values(1)]
sorted_correlations_desc = sorted_correlations_desc[sorted_correlations_desc.index.get_level_values(0) != sorted_correlations_desc.index.get_level_values(1)]

# Obtener los 30 pares más y menos correlacionados
top_30_least_correlated = sorted_correlations_asc.head(30)
top_30_most_correlated = sorted_correlations_desc.head(30)

# Crear DataFrames para los pares de variables
df_least_correlated = pd.DataFrame(top_30_least_correlated, columns=['Correlation'])
df_most_correlated = pd.DataFrame(top_30_most_correlated, columns=['Correlation'])

# Eliminar duplicados solo en función del campo 'Correlation' en los DataFrames
df_least_correlated = df_least_correlated.drop_duplicates(subset='Correlation')
df_most_correlated = df_most_correlated.drop_duplicates(subset='Correlation')

print("15 pares de variables menos correlacionados:")
df_least_correlated

print("15 pares de variables más correlacionados:")
df_most_correlated

"""Using scatter plots, we can see how the relationships between variables have evolved over time. From this scatter plot showing the correlation between NASDAQ and S&P500, we draw two important conclusions:

1. The closing prices of NASDAQ and S&P500 show an almost perfect positive correlation.
2. Prices have been increasing from 2013 to 2023, as most dark points are located in the upper right and light points are in the lower left.
"""

plt.style.use('ggplot')

def temporal_scatterplot(x_var, y_var):
  # Crear un gráfico de dispersión con la fecha como variable de color
  sns.scatterplot(x=x_var, y=y_var, hue=nasdaq.index.year, data=nasdaq)

  plt.title(f'Correlación de {x_var} vs. {y_var}')

  plt.xlabel(x_var)
  plt.ylabel(y_var)

  return plt.show()

temporal_scatterplot('Close', 'S&P500')

"""Consumer expectations tend to anticipate changes in NASDAQ trends. When this indicator is declining, trading volume continues to rise because it has not yet reached its peak. When prices start to fall and trading volume decreases, consumer expectations rise, suggesting that the end of the recession will occur in the near future.

As can be seen, trading volume has been increasing over the years, while consumer expectations have been declining from 2018 to 2022.
"""

temporal_scatterplot('consumer expectation', 'Volume')

"""Another negative correlation between a leading economic indicator and a representative index of the American economy.

The DJIA has been increasing from 2014 to 2022, while the difference between the yield on the 10-year and 2-year bonds has been negative throughout 2022, suggesting the possibility of an upcoming economic recession.
"""

temporal_scatterplot('10y-2y yield curve', 'Dow Jones Industrial Average')

"""The London stock market does not show as clear a growth trend as the American ones. On the other hand, the spread of junk bonds reached highs in 2020, signaling the peak of panic during the pandemic.

From the negative correlation, it can be inferred that the London stock market does not respond well to financial stress situations.
"""

temporal_scatterplot('junk bonds spread', 'FTSE 100')

"""Lastly, we are going to analyze with a bar chart the correlations of all variables with respect to 3 variables: close, return, and target_return.

The first one is close, which shows a high correlation with many variables.
"""

def correlation_sorted(var):
  # Calcular las correlaciones
  correlations = nasdaq.corr()[var]

  # Eliminar la correlación de la variable consigo misma
  correlations = correlations.drop(var)

  # Crear un DataFrame para facilitar el ordenamiento
  correlation_df = pd.DataFrame({'Variable': correlations.index, 'Correlación': correlations.values})

  # Ordenar las correlaciones en orden descendente
  correlation_df = correlation_df.sort_values(by='Correlación', ascending=False)

  plt.figure(figsize=(12, 10))
  # Asegurar que los nombres de las variables están separadas del gráfico
  sns.set(rc={'figure.autolayout': True})
  sns.barplot(x='Correlación', y='Variable', data=correlation_df, palette='Blues_d', orient='h', errorbar=None, dodge=False, saturation=1.0)
  plt.title(f'Correlación con {var}')
  plt.xlabel('Correlación')
  plt.ylabel('Variables')

  plt.show()

correlation_sorted('Close')

"""Regarding daily return, it can be observed that the intensity of the correlation is much lower across all variables."""

correlation_sorted('return')

"""If we look at the target variable, the other variables do not seem to have a significant correlation. However, as we have mentioned before, correlation does not necessarily indicate predictive capability, so we will not discard any variable for now."""

correlation_sorted('target_return')

"""The conclusion we can draw from this last graph is the difficulty of the problem we are trying to solve, as there is no high correlation between the target variable and any of the variables.

###<u>Heteroscedasticity </u>

As a complementary analysis, we are going to test the variance of errors in a regression model. The goal of this project is to solve a classification problem, but as a demonstration for future work in this area, we will check the complexity and difficulty of performing a regression model instead of classification on this data.

The Breusch-Pagan test is a statistical test used to assess the presence of heteroscedasticity in a regression model. Heteroscedasticity refers to the non-constant variability of errors in a regression model, meaning that the variance of errors is not constant across the values of the independent variables. This can be problematic as it violates one of the key assumptions of many linear regression models, which assumes constant variance of errors (homoscedasticity).

If heteroscedasticity is detected, it would be necessary to use more sophisticated regression algorithms capable of capturing nonlinear relationships and more complex patterns in the data.
"""

from statsmodels.stats.diagnostic import het_breuschpagan

# Ajustamos el modelo de regresión lineal
X = nasdaq.drop(columns=['return'])
X = sm.add_constant(X)  # Añadimos una constante (intercepto) al modelo
y = nasdaq['return']

# Entrenamos el modelo
model = sm.OLS(y, X).fit()

# Calculamos los residuos del modelo
residuals = model.resid

# Realizamos la prueba de Breusch-Pagan
bp_test = het_breuschpagan(residuals, X)

# Obtienemos los valores de estadísticas de la prueba
lm_statistic = bp_test[0]
lm_p_value = bp_test[1]
f_statistic = bp_test[2]
f_p_value = bp_test[3]

print(f"Estadística LM: {lm_statistic}")
print(f"Valor p LM: {lm_p_value}")
print(f"Estadística F: {f_statistic}")
print(f"Valor p F: {f_p_value}")

# Comprobamos la significación (5%) estadística del resultado
alpha = 0.05
if lm_p_value < alpha or f_p_value < alpha:
    print("Se rechaza la hipótesis nula de homocedasticidad: Hay evidencia de heterocedasticidad.")
else:
    print("No se rechaza la hipótesis nula: No hay evidencia de heterocedasticidad.")

"""Both statistics are below the 5% significance level, so we reject homoscedasticity as the null hypothesis. This is another indication of the complex nature of the data we are dealing with and the difficulty of predicting it, both in classification algorithms (as demonstrated in the rest of the sections) and in regression algorithms.

###<u>Temporal Interdependence</u>

To measure the temporal interdependence of the variables, we will use graphs to show the autocorrelation from lag 0 to lag 30. Autocorrelation is a statistical measure that evaluates the linear relationship between a dataset and a delayed version of itself. Conceptually, its goal is to check if the present and future depend on the past. First, we display the autocorrelation of the closing price, and subsequently, we will show that of the daily return, and finally that of the sign of this return (target variable).

Autocorrelation of the closing price:
"""

plt.style.use('ggplot')

def autocorr(var, lags):
  # transformada rápida de Fourier activada
  acf_result = sm.tsa.acf(nasdaq[var], nlags=lags, fft=True)

  plt.bar(range(len(acf_result)), acf_result, width=0.2)

  plt.title(f'Gráfico de Autocorrelación para {var}')
  plt.xlabel('Lag')
  plt.ylabel('Coeficiente de Autocorrelación')
  plt.grid(True)

  plt.show()

autocorr('Close', 30)

"""Autocorrelation of the daily return:"""

autocorr('return', 30)

"""Autocorrelation of the daily return sign:"""

autocorr('target_return', 30)

"""The autocorrelation is very pronounced in the 'Close' variable, but it seems to be not significant for the other two variables.

To analyze the autocorrelation of the target variable more clearly, let's perform the Ljung-Box Test. This test is used to evaluate if there is significant autocorrelation at different lags of a time series. The null hypothesis states that the data are independently distributed (i.e., the correlations in the population from which the sample is drawn are 0, so any correlation observed in the data is the result of randomness in the sampling process). The interpretation relies on the p-value. If the p-value is less than a significance threshold (in this case 0.05), the null hypothesis can be rejected, thus concluding that there is evidence of autocorrelation in the time series.
"""

from statsmodels.stats.diagnostic import acorr_ljungbox

y = nasdaq['target_return']

# Realizamos la prueba de Ljung-Box para 30 lags
ljungbox = acorr_ljungbox(y, lags=30)

ljungbox['Autocorrelación'] = ['Sí' if p < 0.05 else 'No' for p in ljungbox['lb_pvalue']]

ljungbox

"""As observed, according to the Ljung-Box Test, there is evidence of autocorrelation only at the first lag. This proves that there is first-order autocorrelation (lag 1) and therefore, it would be interesting to train a model capable of capturing intertemporal patterns in the data.

###<u>Linear Separability</u>

Given that we want to solve a classification problem, we will check if the data is linearly separable by evaluating a Hard Margin Support Vector Machine (SVM).

The term "Hard Margin" refers to the strict nature of this SVM, which aims to find a separation hyperplane that perfectly separates the samples of two classes in a feature space. In other words, it seeks a hyperplane with a maximum margin (the distance between the hyperplane and the nearest data point) and does not allow any data points within the margin.

The SVM's ability to find a perfect or nearly perfect separation hyperplane can indicate if the classes (in this case, the sign of the next day's return) are easily distinguishable in the data. The Hard Margin SVM assumes that the data is linearly separable, so poor results in the evaluation would indicate that the data is not linearly separable and that a Soft Margin SVM or more flexible machine learning methods, such as neural networks, may be more appropriate for solving this classification problem.

To train and test the Hard Margin SVM, we specify a value of C close to infinity as a hyperparameter. C is a hyperparameter that represents the penalty for misclassifying a point. By specifying a very high value, the SVM will apply a stronger penalty for classification errors. In other words, the model will strive to classify each point correctly, even if it means that the margin between the classes is narrower.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.svm import LinearSVC
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import accuracy_score
# from sklearn.preprocessing import RobustScaler
# 
# # Dividimos en X e Y
# X = nasdaq.drop(columns=['target_return'])
# y = nasdaq['target_return']
# 
# # Dividimos los datos en conjuntos de entrenamiento (70%) y prueba (30%)
# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
# 
# # Escalamos los datos con RobustScaler de scikit-learn
# scaler = RobustScaler()
# train_scaled = scaler.fit_transform(x_train)
# test_scaled = scaler.transform(x_test)
# 
# # Creamos un modelo SVM lineal con margen duro
# svm_classifier = LinearSVC(C=1e10, max_iter=1000000)
# 
# # Entrenamos el modelo con los datos de entrenamiento
# svm_classifier.fit(train_scaled, y_train)
# 
# # Realizamos predicciones en el conjunto de prueba
# y_pred = svm_classifier.predict(test_scaled)
# 
# # Calculamos y mostramos la precisión del modelo
# accuracy = accuracy_score(y_test, y_pred)
# 
# print(f'Precisión del modelo SVM de margen duro: {accuracy:.2f}')

"""As observed, the accuracy reported by the model is not only 49%, but also the model fails to converge even with a million iterations. From these results, we can conclude that the linear Hard Margin SVM does not provide predictive value and that the data is not linearly separable (the classes are not perfectly separable), indicating a more complex classification problem that will require more sophisticated algorithms to be solved.

###<u>Stationarity</u>

To analyze the presence of seasonality in the variables, we will plot the time series to visualize if there are patterns in the data that repeat. Stationarity is an important characteristic in time series analysis, as many statistical techniques and models assume that the data are stationary.

The following candlestick chart shows the OHLC variables. There don't seem to be any recurring patterns in the long term, and the complexity of the chart makes it difficult to determine if there are cyclical patterns in the short term.

mplfinance Documentation: https://pypi.org/project/mplfinance/
"""

!pip install mplfinance > /dev/null
import mplfinance as mpf

# Nos saldrá un aviso de que hay demasiados datos. Para evitarlo le indicamos que este aviso solo salga a partir de las 3000 filas, ya que nosotros contamos con 2503 sesiones.

stock_fig, axes = mpf.plot(nasdaq, type='candle', style='charles', title=f'Gráfico de velas japonesas: NASDAQ', ylabel='Price',
                           figsize=(15, 6), returnfig=True, warn_too_much_data=3000)

"""We'll smooth out the above graph by showing the moving averages to better observe the trends."""

moving_av = ['SMA_50', 'SMA_200', 'EMA_9', 'EMA_50', 'EMA_200', 'WMA_9', 'WMA_50', 'WMA_200']
plt.style.use('ggplot')

def plot_time_series(variables, title):
    plt.figure(figsize=(12, 6))
    for variable in variables:
        plt.plot(nasdaq.index, nasdaq[variable], label=variable)

    plt.title(title)
    plt.legend(loc='upper left')
    plt.grid(True)
    plt.show()

plot_time_series(moving_av, 'Medias Móviles')

"""We do the same with the Bollinger Bands because they are also variables that visually accompany the price."""

bollinger = ['BBL_20_2.0', 'BBM_20_2.0', 'BBU_20_2.0']

plot_time_series(bollinger, 'Bandas de Bollinger')

"""Now we visualize the remaining float variables in a matrix of plots."""

# Utilizamos el estilo 'ggplot' para las gráficas
plt.style.use('ggplot')

graph_vars = ['Volume', 'RSI_14', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9',
              'STOCHk_14_3_3', 'STOCHd_14_3_3', 'MOM_10', 'BBB_20_2.0', 'BBP_20_2.0',
              '10y-2y yield curve', 'volatility', '5-year interest rate',
              'federal interest rate', 'junk bonds spread', 'S&P500', 'Wilshire 5000',
              'Dow Jones Industrial Average', 'Nikkei 225', 'Euro STOXX 50',
              'Hang Seng Index', 'Shanghai Stock Exchange', 'FTSE 100',
              'Gold futures', 'Crude oil futures', 'unemployment', 'inflation',
              'consumer expectation', 'industrial production', 'return']

def graphs(var_list, n_rows, n_cols):
  # Calcular filas y columnas necesarias
  total_plots = len(var_list)
  rows = n_rows
  cols = n_cols

  # Crear una figura con una matriz de gráficos
  fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(15, 2.5 * rows))

  # Recorrer el diccionario y crear los gráficos en la matriz
  for i, variable in enumerate(var_list):
      row_idx = i // cols
      col_idx = i % cols
      ax = axes[row_idx, col_idx]
      nasdaq[variable].plot(ax=ax, title=variable, lw=2)
      ax.set_ylabel(variable)

  # Ajustar el espaciado entre los gráficos y mostrar la figura

  plt.tight_layout()
  return plt.show()

graphs(graph_vars, 10, 3)

"""Due to the complexity and the amount of noise and patterns at short, medium, and long term, it is difficult to visually determine the presence of seasonality in the data.

We will use the Augmented Dickey-Fuller (ADF) Test to test for seasonality in the variables. The ADF Test is a statistical hypothesis test used to determine whether a time series is stationary or not. It is based on the idea that a non-stationary time series can be transformed into a stationary series through differencing. Differencing involves calculating the difference between successive values in the time series. The ADF test compares an original series with its differenced version to assess whether the difference is statistically significant. The null hypothesis (H0) of the test is that the time series has unit roots, indicating non-stationarity. The alternative hypothesis (H1) is that the time series is stationary. If the p-value is less than or equal to a predefined significance level (0.05 in this case), the null hypothesis is rejected, and it is concluded that the series is stationary.

"""

from statsmodels.tsa.stattools import adfuller

# Crear una lista para almacenar los resultados de la prueba de estacionalidad
resultados_estacionalidad = []

for column in nasdaq.columns:
  if column in ['target_return', 'dummy_support', 'dummy_resistance']:
    continue
  # Realizar la Prueba ADF en cada columna
  data = nasdaq[column]
  result = adfuller(data)

  # Agregar los resultados a la lista
  resultados_estacionalidad.append({'Variable': column, 'ADF Statistic': result[0], 'p-value': result[1]})

resultados_adf = pd.DataFrame(resultados_estacionalidad)

resultados_adf['Estacionalidad'] = ['Sí' if p < 0.05 else 'No' for p in resultados_adf['p-value']]

resultados_adf.set_index('Variable', inplace=True)

resultados_adf

"""As can be observed, both stationary and non-stationary series are abundant. The target variable is not analyzed in this test due to its nature as a binary variable, but using the return variable as a proxy, we can determine that the daily returns of the NASDAQ are stationary.

Below, we print a monthly box plot to take a closer look at this variable. Monthly box plots are useful for visualizing the distribution of data across different months, which can help detect seasonal patterns. The number of outliers is similar in all months. There are slight changes in the median and the distribution of the quartiles. Visually, seasonality is not very pronounced, but with the ADF test, we can determine that the proxy variable for the target is stationary.
"""

data = nasdaq['return']

# Extraer el mes de la fecha
nasdaq['Month'] = nasdaq.index.month

plt.figure(figsize=(10, 6))
sns.boxplot(x='Month', y='return', data=nasdaq)
plt.title('Boxplot Mensual de Rentabilidad Diaria')
plt.xlabel('Mes')
plt.ylabel('Return')
plt.show()

nasdaq = nasdaq.drop('Month', axis=1)

"""###<u>Risk and Return Metrics</u>

Finally, we will conclude the EDA with some risk and return metrics on the data. These metrics assume that the asset is purchased on the first day of the records and held in the portfolio without selling it throughout the entire period (Buy & Hold strategy):

**Value at Risk (VaR):** Value at Risk is a statistical measure used to estimate the potential losses in an investment or investment portfolio over a period of time and at a certain level of confidence. It represents the maximum expected loss amount within a specific period. At a confidence level of 95%, the VaR is -2.14%. This means that there is a 95% probability that the losses will not exceed -2.14% of the investment in one day.
"""

returns = nasdaq['Close'].pct_change().dropna()
alpha = 0.05  # Nivel de confianza del 95%
var = returns.quantile(alpha)
print(f'VaR al {alpha * 100}%: {var:.2%}')

"""**Maximum Drawdown:** This is a measure that quantifies the largest loss from the highest peak of a value to the lowest point before starting to recover. It indicates the maximum loss experienced before a recovery occurs. In this case, the loss is -36.40%."""

cumulative_returns = (1 + returns).cumprod()
peak = cumulative_returns.cummax()
drawdown = (cumulative_returns - peak) / peak
max_drawdown = drawdown.min()
print(f'Maximum Drawdown: {max_drawdown:.2%}')

"""**Annual Return:** It is the percentage gain or loss experienced by an investment or portfolio in a year. The annual return of the NASDAQ from 2013 to 2023 has been 12.87%."""

annual_return = ((1 + returns).prod() ** (1 / len(returns.index.year.unique())) - 1)
print(f'Annual Return: {annual_return:.2%}')

"""**Sharpe Ratio:** It is a measure that evaluates the performance of an investment relative to its risk. It compares the additional return obtained beyond a risk-free investment with the volatility of the investment. The risk-free rate chosen as the asset's risk-free return is the yield of the 10-year US Treasury bond (US10YT) on October 13, 2023: 4.62%. As a result, the Sharpe Ratio is 6.25.

Source: https://markets.ft.com/data/bonds/tearsheet/summary?s=US10YT
"""

risk_free_rate = 0.0462  # Tasa libre de riesgo
sharpe_ratio = (annual_return - risk_free_rate) / returns.std()
print(f'Sharpe Ratio: {sharpe_ratio:.2f}')

"""**Sortino Ratio:** It is a metric similar to the Sharpe Ratio, but it focuses on negative volatility rather than total volatility. It measures the performance of an investment relative to its downside risk, making it especially relevant for investments where minimizing downside risk is critical. Based on the previously calculated data, the Sortino Ratio is 7.80."""

downside_returns = returns[returns < 0]
sortino_ratio = (annual_return - risk_free_rate) / downside_returns.std()
print(f'Sortino Ratio: {sortino_ratio:.2f}')

"""To continue with the risk analysis, let's print the 10 largest daily losses experienced by the NASDAQ.

The largest drop occurred on March 16, 2020, during the pandemic, with a decrease of 6.6%. The second largest, also in March 2020, was 5.08%. The rest of the losses are somewhat lower and are distributed heterogeneously across several years.
"""

outliers_low = nasdaq['return'].sort_values()

outliers_low.head(10)

"""Regarding the largest daily gain, it occurred on February 24, 2022, with an increase of 7.03%. The rest of the gains are also distributed heterogeneously and are somewhat lower."""

outliers_high = nasdaq['return'].sort_values(ascending=False)

outliers_high.head(10)

"""###<u>Conclusion</u>

The Exploratory Data Analysis reveals several key characteristics about the dataset that require careful consideration. Below are the main conclusions drawn from the EDA:

1. **Outliers and Noise:** The analysis of the data shows the presence of outliers in several variables. These outliers can introduce noise into the analysis, so it will be necessary to use outlier-robust normalization methods and noise reduction techniques in the data preprocessing.

2. **Non-Normality of Variables:** Most of the analyzed variables do not follow a normal distribution. This is evidenced through normality tests such as the Shapiro-Wilk test. Lack of normality can affect the application of some statistical techniques that assume a normal distribution in the data.

3. **Low Correlation with the Dependent Variable:** There is low correlation between the independent variables and the dependent variable. This suggests that the independent variables are not strongly related to the target variable and may pose a challenge in building an accurate predictive model.

4. **Presence of Heteroscedasticity:** The analysis of residuals suggests the presence of heteroscedasticity in the data. The variance of errors is not constant across the range of values of the dependent variable, which poses problems for regression models.

5. **Autocorrelation in the Target Variable:** Autocorrelation has been detected in the target variable at first-order differences. This indicates that successive values of the target variable are correlated.

6. **Not Linearly Separable:** The data is not linearly separable, meaning that a clear linear decision boundary cannot be drawn to classify the data. This implies that more sophisticated classification algorithms that are robust to non-linearly separable data will be necessary.

7. **Seasonality:** Evidence of both non-seasonality and seasonality has been found in the data. The presence of seasonality in the 'return' variable as a proxy for the target variable suggests seasonality in the target variable. The lack of seasonality in other variables indicates that the data pattern may vary over time.

In summary, the data presents a series of challenges and complexities that need to be addressed appropriately. Given the characteristics of the dataset, it is proposed to use the "Robust Scaler" normalization method to handle outliers. As for classification algorithms, here are some proposed methods that have specific attributes that may be useful for addressing the characteristics and limitations of the data:

1. **Random Forest:** Random Forest is an ensemble algorithm that can handle non-linearly separable data and is robust to outliers. Additionally, it can capture non-linear relationships in the data.

2. **XGBoost (Extreme Gradient Boosting) Classifier:** A powerful boosting algorithm that can handle complex relationships between variables and improve predictive performance.

3. **Linear SVM with Soft Margin:** A SVM with a linear kernel and soft margin, more suitable for classifying classes that cannot be perfectly separated.

4. **LSTM (Long Short-Term Memory):** LSTM is a recurrent neural network that is capable of capturing temporal relationships in the data, which is especially useful for data with autocorrelation. It can address the temporal complexity of the data and handle non-linear sequences.

Each algorithm brings specific attributes that may be beneficial for addressing the particular characteristics of the dataset and achieving optimal performance in classification.

###<u>Export to SQL</u>

We perform the checkpoint by saving the dataframe to the SQL database.
"""

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

def crear_tabla_desde_dataframe(df, tabla, connection):
    cursor = connection.cursor()

    create_table_query = f"CREATE TABLE {tabla} (id INT AUTO_INCREMENT PRIMARY KEY"

    for columna, tipo in zip(df.columns, df.dtypes):
        if tipo == "float64":
            create_table_query += f", `{columna}` FLOAT"
        elif tipo == "int64":
            create_table_query += f", `{columna}` INT"
        elif tipo == "datetime64[ns]":
            create_table_query += f", `{columna}` DATE"
        elif tipo == "bool":
            create_table_query += f", `{columna}` BOOLEAN"

    create_table_query += ");"
    cursor.execute(create_table_query)

# Llamada a la función con las listas anteriores
crear_tabla_desde_dataframe(nasdaq, 'nasdaq_eda', connection)

from sqlalchemy import create_engine

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

def cargar_dataframe_en_tabla(df, tabla):
    df.to_sql(tabla, con = engine, if_exists='replace', index=False)

nasdaq.reset_index(inplace=True)

cargar_dataframe_en_tabla(nasdaq, 'nasdaq_eda')

connection.close()
engine.dispose()

"""# **Feature Engineering**

###<u>Import from SQL</u>
"""

!pip install protobuf mysql-connector-python > /dev/null
import mysql.connector

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

import pandas as pd
from sqlalchemy import create_engine, text

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

conn = engine.connect()

query = text(f'SELECT * FROM nasdaq_eda')
nasdaq = pd.read_sql(query, con=conn)

nasdaq = nasdaq.set_index('Date')

conn.close()
connection.close()

"""We visualize the dataframe to verify that it has been imported correctly and that the date index is correct.


"""

nasdaq

"""###<u>Train, Dev y Test division</u>

Before applying any preprocessing techniques, we will divide the data into training (train: 70%), validation (dev: 10%), and test (test: 20%) sets to avoid any kind of data contamination or 'data leakage'.

'Data leakage' is a phenomenon where information from the test set leaks into the training set, which can lead to incorrect evaluation of the model's performance. In other words, it occurs when the training dataset inadvertently or intentionally includes details about the target variable that would not be available in a production environment. This would lead to an overly optimistic evaluation compared to the results the model will obtain in reality, as the model relied on privileged information during training.
"""

total_rows = len(nasdaq)
train_size = int(0.7 * total_rows)
dev_size = int(0.1 * total_rows)

# Dividimos el DataFrame en los conjuntos de train, dev y test
train_df = nasdaq[:train_size]
dev_df = nasdaq[train_size:(train_size + dev_size)]
test_df = nasdaq[(train_size + dev_size):]

print("Train: ", len(train_df), "\nDev: ", len(dev_df), "\nTest: ", len(test_df))

for df_name, df in [("train_df", train_df), ("dev_df", dev_df), ("test_df", test_df)]:
    print(f"Fecha inicial en {df_name}: {df.index[0]}")
    print(f"Fecha final en {df_name}: {df.index[-1]}")

"""Once we have obtained the date splits, we can plot the three periods. The train set will be used to train the models. The dev set will be used to tune the hyperparameters through cross-validation. Finding the hyperparameters on the dev set instead of the train set will be computationally less expensive since the dev set is smaller, and at the same time, it will help us generalize on the train set. Finally, the test set will be used to evaluate the predictions of the trained model. The advantage of the performed split is that the train set includes the COVID-19 crisis of 2020, so it can be trained on periods of high volatility and downturns."""

import matplotlib.pyplot as plt

# Convertimos las fechas de corte a objetos de fecha y hora de Pandas
corte1 = pd.to_datetime('2020-07-02')
corte2 = pd.to_datetime('2021-06-30')

# Graficamos la variable 'Close'
plt.figure(figsize=(12, 6))
plt.plot(train_df.index, train_df['Close'], label='Train')
plt.plot(dev_df.index, dev_df['Close'], label='Dev')
plt.plot(test_df.index, test_df['Close'], label='Test')

# Agregamos líneas verticales discontinuas para señalar las diferencias
plt.axvline(x=corte1, color='gray', linestyle='--', label='2020-07-02')
plt.axvline(x=corte2, color='gray', linestyle='--', label='2021-06-30')

plt.xlabel('Fecha')
plt.ylabel('Precio de Cierre')
plt.title('División de Train, Dev y Test')
plt.legend()
plt.grid()
plt.show()

"""We define a function to separate the target variable from the set of independent variables."""

# Dividimos los conjuntos en y (target_return) y X (variables independientes)

def xy_split(df):
  y = df['target_return']
  x = df.drop(columns=['target_return'])

  return x, y

train_x, train_y = xy_split(train_df)

"""###<u>Wavelet Transform</u>

The wavelet transform is a technique used in signal processing and data analysis that allows decomposing a signal or dataset into components representing different levels of detail. This decomposition is useful for identifying patterns, structures, and changes in the signal at different scales, facilitating analysis and extraction of relevant information.

When the wavelet transform is applied to data, wavelet coefficients representing the contribution of different frequencies or scales in the original signal are obtained. These coefficients can be divided into two parts: approximation (or low-frequency component) and detail (or high-frequency component). The approximation captures the general and smoothed characteristics of the signal, while the details highlight fine variations and rapid changes.

There could be said to be two types of information in the training set: signal and noise. The 'signal' resembles the useful and generalizable information in the data, while the 'noise' represents random fluctuations or incidental patterns that do not significantly contribute to the model's ability to make predictions on new data.

The function used below decomposes the time series into wavelets and removes the 3 highest frequency components to smooth the signal by eliminating noise (high-frequency wavelets). The choice of removing the 3 highest frequency components has been made after a trial and error process on the 'Close' variable in training, which will be generalized to the rest of variables and the test set to be consistent with training. The adjustment process has been carried out only on train to avoid 'data leakage'.

Regarding the choice of the type of wavelet transform, the Daubechies wavelet transform (sym5) has been chosen instead of the Haar wavelet transform, used in other works. The main reason is that daily financial time series tend to be smoother and contain high-frequency fluctuations, and Daubechies wavelets, such as 'sym5', are usually more appropriate due to their ability to efficiently represent and analyze high-frequency fluctuations, reduce noise, and preserve trends in the data. This has also been verified through trial and error.

Library documentation pywt: https://pywavelets.readthedocs.io/en/latest/index.html
"""

import pywt
import numpy as np

def wavelet_transform(data):
  # Creamos una copia del dataframe original
  transformed_data = data.copy()
  for col in data.columns:
    # Evitamos las variables dummies
    if col in ['dummy_support', 'dummy_resistance']:
      continue
    # Descomponemos las variables en ondículas
    coeff = pywt.wavedec(data[col], 'sym5', mode='symmetric')
    # Eliminamos los 3 componentes de mayor frecuencia para suavizar las variables
    coeff[-3:] = [np.zeros_like(c) for c in coeff[-3:]]
    # Reconstruimos las variables con las ondículas restantes
    rec = pywt.waverec(coeff, 'sym5', mode='symmetric')
    if len(rec) > len(transformed_data):
            rec = rec[:-1]
    transformed_data[col] = rec
  return transformed_data

"""The trial and error process has been carried out by visualizing the 'Close' graph. In it, the effectiveness of noise removal in the variables can be observed. One of the great advantages of the wavelet transform over other noise removal methods such as moving averages is that the signal recomposed by the wavelet transform is simultaneous with the original signal, thus avoiding the lag or delay that occurs with moving averages."""

import matplotlib.pyplot as plt

train_wt = wavelet_transform(train_x)

# Tras un proceso de prueba y error, la eliminación óptima de componentes de alta frecuencia es 3
y = train_x['Close']
y_wt = train_wt['Close']
x = train_x.index

plt.figure(figsize=(18,6))
plt.plot(x, y)
plt.plot(x, y_wt)
plt.legend(['Original', 'Transformada Wavelet'])

plt.show()

"""We also visualize how the adjusted transform looks on the test data. The generalization proves to be effective."""

import matplotlib.pyplot as plt

test_x = test_df.drop(columns=['target_return'])
test_wt = wavelet_transform(test_x)

y = test_x['Close']
y_wt = test_wt['Close']
x = test_x.index

plt.figure(figsize=(18,6))
plt.plot(x, y)
plt.plot(x, y_wt)
plt.legend(['Original', 'Transformada Wavelet'])

plt.show()

"""###<u>Normalization</u>

Before training the models, we will proceed with the normalization of the variables for the following reasons:

1. Consistent Scales: Ensuring that all variables are on the same scale, avoiding biases due to differences in units.

2. Faster Convergence: Facilitating fast convergence of algorithms, improving training efficiency.

3. Avoiding Attribute Dominance: Preventing variables with larger magnitudes from dominating the impact on the model, ensuring equitable contribution.

4. Facilitated Interpretation: Making the interpretation of coefficients in linear models easier by making them directly comparable.

5. Improved Model Performance: Enhancing the overall performance of the models, especially in numerical calculations and optimization.

We have chosen Robust Scaler as the method for normalizing the dataset due to its resistance to outliers, as mentioned in the EDA where it was decided not to remove them. These outliers can negatively affect the performance of the models, as they can significantly influence the calculation of central tendency and dispersion measures. Robust Scaler mitigates the impact of outliers by focusing on robust statistics, such as the median and interquartile range (IQR), instead of the mean and standard deviation used in other normalization methods.

Since Robust Scaler uses statistics from the dataset, we will fit it only based on the training dataset and then transform the test set with that fit. The goal is to avoid contamination of the training set that would occur if statistics such as the median or IQR are influenced by the test set.

RobustScaler Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html
"""

from sklearn.preprocessing import RobustScaler

# Crear un objeto RobustScaler
scaler = RobustScaler()

# Ajustar y transformar los conjuntos de entrenamiento
train_norm = scaler.fit_transform(train_wt)

"""###<u>Stacked Autoencoders</u>

As a final preprocessing technique, we will build some Stacked Autoencoders. Stacked Autoencoders are a special type of artificial neural networks used for dimensionality reduction in a model's variables by extracting their deep features.

The network is trained to learn an efficient encoding of the input data. There is an encoding phase where it reduces the dimensionality of the data. Then, it has a decoding phase that tries to reconstruct the original data from the encoded representation. The goal is for the autoencoders to automatically learn relevant features from the data, extracting valuable information and removing noise. This reduces the dimensionality while maintaining essential information of the variables, improving model generalization.

In the code, we create 5 autoencoders with an input layer, a deep layer, and an output layer, each with 150 neurons. Once defined and trained, they are stacked into a Keras sequential model. Since most hyperparameters have been adjusted through a trial-and-error process, we print out the minimum training error to make the autoencoders produce the most faithful representation of the original data. Once adjusted, trained, and stacked, the Stacked Autoencoders receive the original variables as input and produce an output representing their deep features.

The input they receive for fitting is the training data after applying the wavelet transform and normalization. The test set is excluded to avoid data leakage.

TensorFlow Documentation: https://www.tensorflow.org/api_docs/python/tf/all_symbols
"""

import tensorflow as tf

# Creamos una función para construir un AE
def build_single_layer_ae(input_dim, hidden_dim):
    model = tf.keras.models.Sequential([
        tf.keras.layers.InputLayer(input_shape=(input_dim,)),
        tf.keras.layers.Dense(hidden_dim, activation='elu'),
        tf.keras.layers.Dense(input_dim, activation='elu')
    ])

    model.compile(optimizer='adam', loss='mse')

    return model

# Creamos una lista de AEs para apilarlos
single_layer_aes = []
# Número de características de entrada
input_dim = 47
# Tamaño de las capas ocultas para cada uno de los 5 AE
hidden_dims = [150, 150, 150, 150, 150]

for hidden_dim in hidden_dims:
    ae = build_single_layer_ae(input_dim, hidden_dim)
    single_layer_aes.append(ae)

# Entrenamos cada AE en una capa
for ae in single_layer_aes:
    min_loss = float('inf')
    for epoch in range(300):
        history = ae.fit(train_norm, train_norm, epochs=1, batch_size=16, verbose=0)
        loss = history.history['loss'][0]
        min_loss = min(min_loss, loss)

    # Mostramos el valor mínimo de pérdida para cada AE para obtener una referencia
    print(f'Min Loss for AE: {min_loss}')

# Apilamos los AE para obtener un SAE
stacked_ae = tf.keras.models.Sequential()
stacked_ae.add(tf.keras.layers.InputLayer(input_shape=(input_dim,)))
for ae in single_layer_aes:
    stacked_ae.add(ae)

"""
For binary independent variables, no transformation is necessary since they do not contain noise and do not need to be normalized. For this reason, we restore these variables to their original state before the transformations."""

print(nasdaq.columns[25])
print(nasdaq.columns[26])

def dummy_adjust(array, dataframe):
  # Sustituir las columnas 25 y 26 del array por las columnas de dummies
  array[:, 25] = dataframe['dummy_support'].values
  array[:, 26] = dataframe['dummy_resistance'].values

  return array

"""###<u>Processing and Exportation</u>

We define a function that includes the 5 preprocessing steps.
"""

def preprocessing(raw_data):
  # Paso 1: Separar X e Y
  train_x, train_y = xy_split(raw_data)

  # Paso 2: Aplicar Wavelet Transform
  train_wt = wavelet_transform(train_x)

  # Paso 3: Normalización con Robust Scaler (el scaler ya está ajustado a train)
  train_norm = scaler.transform(train_wt)

  # Paso 4: Procesar los datos con SAE para obtener características extraídas
  train_sae = stacked_ae.predict(train_norm)

  # Paso 5: Recuperar las variables dummies originales
  train_sae = dummy_adjust(train_sae, train_wt)

  return train_x, train_y, train_wt, train_norm, train_sae

"""In the models, we will evaluate the different preprocessing techniques, so we need to save the datasets at each step. The transformations are applied in parallel and independently to the train, dev, and test sets to avoid data leakage."""

train_x, train_y, train_wt, train_norm, train_sae = preprocessing(train_df)
dev_x, dev_y, dev_wt, dev_norm, dev_sae = preprocessing(dev_df)
test_x, test_y, test_wt, test_norm, test_sae = preprocessing(test_df)

"""All objects are converted to dataframes to achieve homogeneity."""

# Lista de arrays de NumPy y sus nombres correspondientes
arrays = [('train_x', train_x), ('train_y', train_y), ('train_wt', train_wt), ('train_norm', train_norm), ('train_sae', train_sae),
          ('dev_x', dev_x), ('dev_y', dev_y), ('dev_wt', dev_wt), ('dev_norm', dev_norm), ('dev_sae', dev_sae),
          ('test_x', test_x), ('test_y', test_y), ('test_wt', test_wt), ('test_norm', test_norm), ('test_sae', test_sae)]

def convert_to_dataframe(obj):
    if isinstance(obj[1], (pd.DataFrame, pd.Series)):
        # Si ya es DataFrame, no es necesario convertirlo
        return (obj[0], obj[1])
    else:
        # Si no lo es, lo convertimos a DataFrame
        return (obj[0], pd.DataFrame(obj[1]))

arrays = [convert_to_dataframe(obj) for obj in arrays]

# Verificar los tipos de los objetos después de la conversión
for name, dataframe in arrays:
    print(f"Nombre: {name}, Tipo: {type(dataframe)}")

"""They are saved in the persistent database."""

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

def crear_tabla_desde_arrays(arrays, connection):
    cursor = connection.cursor()

    for nombre, dataframe in arrays:
        create_table_query = f"CREATE TABLE {nombre} (id INT AUTO_INCREMENT PRIMARY KEY"
        columna = "col"

        if len(dataframe.columns) > 1:
            for i, col in enumerate(dataframe.columns):
                create_table_query += f", `{col}` FLOAT"
        else:
            create_table_query += f", `{dataframe.columns[0]}` FLOAT"

        create_table_query += ");"
        cursor.execute(create_table_query)

crear_tabla_desde_arrays(arrays, connection)

from sqlalchemy import create_engine

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

def cargar_dataframe_en_tabla(df_list):
  for tabla, dataframe in df_list:
    dataframe.to_sql(tabla, con = engine, if_exists='replace', index=False)

cargar_dataframe_en_tabla(arrays)

connection.close()
engine.dispose()

"""# **Training and Evaluation of Models**"""

!pip install protobuf mysql-connector-python > /dev/null
import mysql.connector

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

conn = engine.connect()

# Definición de la función para importar una tabla de SQL y convertirla en un DataFrame

def importar_tabla_a_dataframe(df_name):
    query = text(f'SELECT * FROM {df_name}')
    df = pd.read_sql(query, con=conn)
    return df

train_x = importar_tabla_a_dataframe('train_x')
train_y = importar_tabla_a_dataframe('train_y').values.ravel()  # Utilizar values y ravel() para convertir a una matriz 1D
train_wt = importar_tabla_a_dataframe('train_wt')
train_norm = importar_tabla_a_dataframe('train_norm')
train_sae = importar_tabla_a_dataframe('train_sae')

dev_x = importar_tabla_a_dataframe('dev_x')
dev_y = importar_tabla_a_dataframe('dev_y').values.ravel()
dev_wt = importar_tabla_a_dataframe('dev_wt')
dev_norm = importar_tabla_a_dataframe('dev_norm')
dev_sae = importar_tabla_a_dataframe('dev_sae')

test_x = importar_tabla_a_dataframe('test_x')
test_y = importar_tabla_a_dataframe('test_y').values.ravel()
test_wt = importar_tabla_a_dataframe('test_wt')
test_norm = importar_tabla_a_dataframe('test_norm')
test_sae = importar_tabla_a_dataframe('test_sae')

conn.close()
engine.dispose()

"""The goal of importing the datasets from intermediate preprocessing steps is to evaluate the added value that each step brings in order to improve the predictive capability of the models.

To achieve this, first, cross-validation is performed on the dev set using GridSearch to obtain the optimal hyperparameters. These hyperparameters won't be final; they'll serve as a starting point from which we'll try to approach the truly optimal hyperparameters through a trial and error process.

Subsequently, the accuracy of the models is evaluated first on a fully preprocessed dataset (Wavelet Transform + Normalization + Stacked Autoencoders), then only with WT + Normalization, then only with WT, and finally evaluated on an unprocessed dataset.

Finally, the best model (with the best dataset from those mentioned above) is selected, and its performance is evaluated using a confusion matrix.

###<u>Logistic Regression</u>

The first model will be a logistic regression using scikit-learn. We obtain the starting hyperparameters with which we will experiment.

Documentation for LogisticRegression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

Documentation for GridSearchCV: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html

Documentation for accuracy_score: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

# Definir los hiperparámetros a ajustar
param_grid = {
    'penalty': ['l1', 'l2'],
    'C': [10, 1, 0.1, 0.01, 0.001],
    'solver': ['liblinear', 'saga']
}

# Crear un objeto de regresión logística
logistic_reg = LogisticRegression(fit_intercept=True, random_state=42, solver='saga', max_iter=10000)

# Crear un objeto GridSearchCV
grid_search = GridSearchCV(logistic_reg, param_grid, cv=5, scoring='accuracy')

# Realizar la búsqueda en cuadrícula en los datos de entrenamiento
grid_search.fit(dev_sae, dev_y)

# Obtener los mejores hiperparámetros y el modelo óptimo
best_params = grid_search.best_params_
print("Mejores hiperparámetros:", best_params)

"""We fit the model and evaluate it with each dataset. We define random_state=42 to ensure the reproducibility of the results.

PROCESSED DATA (WT + NORM + SAE):
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# logistic_reg = LogisticRegression(penalty='l1', C=1.0, fit_intercept=True, random_state=42, solver='saga', max_iter=50000)
# modelo_lr = logistic_reg.fit(train_sae, train_y)
# y_lr = modelo_lr.predict(test_sae)
# 
# accuracy = accuracy_score(test_y, y_lr)
# print(f'Precisión en datos de validación: {accuracy * 100:.2f}%')

"""PROCESSED DATA (WT + NORM):"""

modelo_lr1 = logistic_reg.fit(train_norm, train_y)
y_lr1 = modelo_lr1.predict(test_norm)

accuracy = accuracy_score(test_y, y_lr1)
print(f'Precisión en datos de validación: {accuracy * 100:.2f}%')

"""PROCESSED DATA  (WT):"""

modelo_lr2 = logistic_reg.fit(train_wt, train_y)
y_lr2 = modelo_lr2.predict(test_wt)

accuracy = accuracy_score(test_y, y_lr2)
print(f'Precisión en datos de validación: {accuracy * 100:.2f}%')

"""RAW DATA:"""

modelo_lr3 = logistic_reg.fit(train_x, train_y)
y_lr3 = modelo_lr3.predict(test_x)

accuracy = accuracy_score(test_y, y_lr3)
print(f'Precisión en datos de validación: {accuracy * 100:.2f}%')

"""The best among the models is the fully preprocessed one, with an accuracy of 59.48%. While it seems that the wavelet transform does not add predictive power to the logistic regression model, normalization and stacked autoencoders do. Below, we print the coefficients and predictions of the best model."""

coeficientes_lr = modelo_lr.coef_[0]

coeficientes_lr

y_lr

"""We evaluate the best model with a confusion matrix in percentages. Out of the times the model predicts an increase the following day, 60.98% of the time the increase actually occurs, with a 39.02% false entry rate. In the case of negative predictions, the accuracy rate is 57.81%.

Documentation for confusion_matrix: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html
"""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Calcular la matriz de confusión
confusion = confusion_matrix(test_y, y_lr)

# Calcular los porcentajes en lugar de contar
confusion_percentage = confusion.astype('float') / confusion.sum(axis=0)[np.newaxis, :]

# Crear una visualización de la matriz de confusión en porcentajes
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_percentage, annot=True, fmt=".2%", cmap="Blues", xticklabels=["Negativo", "Positivo"], yticklabels=["Negativo", "Positivo"])
plt.xlabel('Predicciones')
plt.ylabel('Etiquetas Reales')
plt.title('Matriz de Confusión en Porcentajes')
plt.show()

"""###<u>Random Forest</u>

We perform the same procedure with Random Forest.

Documentation for RandomForestClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import GridSearchCV
# from sklearn.metrics import accuracy_score
# 
# # Creamos un clasificador de Random Forest con semilla para garantizar la reproducibilidad de los resultados
# rf_classifier = RandomForestClassifier(random_state=42)
# 
# # Definimos la cuadrícula de hiperparámetros para búsqueda
# param_grid = {
#     'n_estimators': [500, 700, 900],
#     'max_depth': [None, 5, 10],
#     'min_samples_split': [2, 5, 10],
#     'max_leaf_nodes': [50, 100, 200],
# }
# 
# grid_search = GridSearchCV(rf_classifier, param_grid, cv=5)
# grid_search.fit(dev_sae, dev_y)
# 
# best_params = grid_search.best_params_
# print("Mejores hiperparámetros:", best_params)

"""PROCESSED DATA (WT + NORM + SAE):"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

model_rf = RandomForestClassifier(n_estimators=700, criterion='gini', max_depth=5, min_samples_split=2, min_samples_leaf=42,
                                  max_features=None, max_leaf_nodes=100, random_state=42, max_samples=850)
modelo_rf = model_rf.fit(train_sae, train_y)

y_rf = modelo_rf.predict(test_sae)

accuracy = accuracy_score(test_y, y_rf)
print(f'Precisión del modelo Random Forest: {accuracy * 100:.2f}%')

"""PROCESSED DATA (WT + NORM):"""

modelo_rf1 = model_rf.fit(train_norm, train_y)

y_rf1 = modelo_rf1.predict(test_norm)

accuracy = accuracy_score(test_y, y_rf1)
print(f'Precisión del modelo Random Forest: {accuracy * 100:.2f}%')

"""PROCESSED DATA (WT):"""

modelo_rf2 = model_rf.fit(train_wt, train_y)

y_rf2 = modelo_rf.predict(test_wt)

accuracy = accuracy_score(test_y, y_rf2)
print(f'Precisión del modelo Random Forest: {accuracy * 100:.2f}%')

"""RAW DATA:"""

modelo_rf3 = model_rf.fit(train_x, train_y)

y_rf3 = modelo_rf3.predict(test_x)

accuracy = accuracy_score(test_y, y_rf3)
print(f'Precisión del modelo Random Forest: {accuracy * 100:.2f}%')

"""In Random Forest, the preprocessing step that adds the most value is the wavelet transform, achieving a precision of 60.28%. In this case, the Stacked Autoencoders reduce the model's accuracy. We print the coefficients."""

importances = modelo_rf1.feature_importances_
print(importances)

"""The accuracy percentage for positive entries is 59.17%, while for non-entries, it is 62.58%."""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

confusion = confusion_matrix(test_y, y_rf1)

confusion_percentage = confusion.astype('float') / confusion.sum(axis=0)[np.newaxis, :]

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_percentage, annot=True, fmt=".2%", cmap="Blues", xticklabels=["Negativo", "Positivo"], yticklabels=["Negativo", "Positivo"])
plt.xlabel('Predicciones')
plt.ylabel('Etiquetas Reales')
plt.title('Matriz de Confusión en Porcentajes')
plt.show()

"""We plot the importance of the model's features to see the weight that each of them has had in the model's prediction."""

import seaborn as sns
import matplotlib.pyplot as plt

# Obtener la importancia de las características
feature_importance = modelo_rf1.feature_importances_

# Obtener los nombres de las características
feature_names = ['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_50', 'SMA_200', 'EMA_9',
       'EMA_50', 'EMA_200', 'WMA_9', 'WMA_50', 'WMA_200', 'RSI_14',
       'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'STOCHk_14_3_3',
       'STOCHd_14_3_3', 'MOM_10', 'BBL_20_2.0', 'BBM_20_2.0', 'BBU_20_2.0',
       'BBB_20_2.0', 'BBP_20_2.0', 'dummy_support', 'dummy_resistance',
       '10y-2y yield curve', 'volatility', '5-year interest rate',
       'federal interest rate', 'junk bonds spread', 'S&P500', 'Wilshire 5000',
       'Dow Jones Industrial Average', 'Nikkei 225', 'Euro STOXX 50',
       'Hang Heng Index', 'Shanghai Stock Exchange', 'FTSE 100',
       'Gold futures', 'Crude oil futures', 'unemployment', 'inflation',
       'consumer expectation', 'industrial production', 'return']

# Configurar el estilo de seaborn ("whitegrid")
sns.set(style="whitegrid")

# Elegir un color específico para todas las barras
bar_color = 'steelblue'

# Crear un gráfico de barras horizontal con barras del mismo color
plt.figure(figsize=(10, 12))
ax = sns.barplot(x=feature_importance, y=feature_names, orient="h", color=bar_color)

plt.xlabel('Importancia de las características')
plt.ylabel('Características')
plt.title('Importancia de las características en el modelo Random Forest')

# Ajustar el espaciado entre las barras
for p in ax.patches:
    width = p.get_width()
    ax.annotate(f'{width:.4f}', (width, p.get_y() + p.get_height() / 2), ha="left", va="center")

plt.show()

"""###<u>XGBoost Classifier</u>

We do the same with the XGBoost Classifier algorithm, first cross-validation and then model evaluation.

Documentation for XGBoost: https://xgboost.readthedocs.io/en/stable/
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# from sklearn.model_selection import GridSearchCV
# 
# # Definir la cuadrícula de hiperparámetros
# param_grid = {
#     'max_depth': [2, 3, 4],
#     'learning_rate': [0.1, 0.01, 0.001],
#     'reg_alpha': [0.0001, 0.001, 0.01],
#     'reg_lambda': [0.1, 1, 10]
# }
# 
# # Crear un modelo XGBoost
# xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42)
# 
# # Realizar la búsqueda en cuadrícula
# grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy')
# grid_search.fit(dev_sae, dev_y)
# 
# best_params = grid_search.best_params_
# print("Mejores hiperparámetros:", best_params)

"""PROCESSED DATA (WT + NORM + SAE):"""

import xgboost as xgb
from sklearn.metrics import accuracy_score

xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42, n_estimators=110, learning_rate=0.1, max_depth=2,
                              subsample=0.8, colsample_bytree=0.8, reg_alpha=0.001, reg_lambda=1)

modelo_xgb = xgb_model.fit(train_sae, train_y)

y_xgb = modelo_xgb.predict(test_sae)

accuracy = accuracy_score(test_y, y_xgb)
print(f'Precisión del modelo XGBoost Classifier: {accuracy * 100:.2f}%')

"""PROCESSED DATA (WT + NORM):"""

modelo_xgb1 = xgb_model.fit(train_norm, train_y)

y_xgb1 = modelo_xgb1.predict(test_norm)

accuracy = accuracy_score(test_y, y_xgb1)
print(f'Precisión del modelo XGBoost Classifier: {accuracy * 100:.2f}%')

"""PROCESSED DATA (WT):"""

modelo_xgb2 = xgb_model.fit(train_wt, train_y)

y_xgb2 = modelo_xgb2.predict(test_wt)

accuracy = accuracy_score(test_y, y_xgb2)
print(f'Precisión del modelo XGBoost Classifier: {accuracy * 100:.2f}%')

"""RAW DATA:"""

modelo_xgb3 = xgb_model.fit(train_x, train_y)

y_xgb3 = modelo_xgb3.predict(test_x)

accuracy = accuracy_score(test_y, y_xgb3)
print(f'Precisión del modelo XGBoost Classifier: {accuracy * 100:.2f}%')

"""In this case, the combination of wavelet transform with stacked autoencoders enhances the predictive capability of the model to the point of being the best-performing model so far. In the confusion matrix, the accuracy rate for positive predictions reaches 63.02%, while for negative predictions, it is 60.17%."""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

confusion = confusion_matrix(test_y, y_xgb)

confusion_percentage = confusion.astype('float') / confusion.sum(axis=0)[np.newaxis, :]

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_percentage, annot=True, fmt=".2%", cmap="Blues", xticklabels=["Negativo", "Positivo"], yticklabels=["Negativo", "Positivo"])
plt.xlabel('Predicciones')
plt.ylabel('Etiquetas Reales')
plt.title('Matriz de Confusión en Porcentajes')
plt.show()

"""Below are the feature importances assigned by the model to the characteristics. It can be observed that certain characteristics have an importance value of zero due to the regularization parameter penalizing uninformative features. Note that 'dummy_resistance' has a relatively high importance within the model, indicating that the creation of this variable in the data extraction phase has been relevant for the construction of this model."""

import seaborn as sns
import matplotlib.pyplot as plt

# Obtener la importancia de las características
feature_importance = xgb_model.feature_importances_

# Obtener los nombres de las características
feature_names = ['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_50', 'SMA_200', 'EMA_9',
       'EMA_50', 'EMA_200', 'WMA_9', 'WMA_50', 'WMA_200', 'RSI_14',
       'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'STOCHk_14_3_3',
       'STOCHd_14_3_3', 'MOM_10', 'BBL_20_2.0', 'BBM_20_2.0', 'BBU_20_2.0',
       'BBB_20_2.0', 'BBP_20_2.0', 'dummy_support', 'dummy_resistance',
       '10y-2y yield curve', 'volatility', '5-year interest rate',
       'federal interest rate', 'junk bonds spread', 'S&P500', 'Wilshire 5000',
       'Dow Jones Industrial Average', 'Nikkei 225', 'Euro STOXX 50',
       'Hang Heng Index', 'Shanghai Stock Exchange', 'FTSE 100',
       'Gold futures', 'Crude oil futures', 'unemployment', 'inflation',
       'consumer expectation', 'industrial production', 'return']

# Configurar el estilo de seaborn ("whitegrid")
sns.set(style="whitegrid")

# Elegir un color específico para todas las barras
bar_color = 'steelblue'


plt.figure(figsize=(10, 12))
ax = sns.barplot(x=feature_importance, y=feature_names, orient="h", color=bar_color)

plt.xlabel('Importancia de las características')
plt.ylabel('Características')
plt.title('Importancia de las características en el modelo XGBoost')

# Ajustar el espaciado entre las barras
for p in ax.patches:
    width = p.get_width()
    ax.annotate(f'{width:.4f}', (width, p.get_y() + p.get_height() / 2), ha="left", va="center")

plt.show()

"""###<u>Support Vector Machine</u>

Due to the demanding computational requirements of training this particular model, and also with the intention of training neural networks on TPU in the next section, we have proceeded to purchase 100 computing units on Google Colab to train the models on GPU and TPU. The goal is also to demonstrate the scalability of the project for more demanding algorithms or larger datasets.

The first step is to change the runtime environment. We switch from CPU with 13GB of RAM to GPU V100 with 54.8GB. To verify that it is correctly connected, we run the following code:
"""

# Nos aseguramos de que el entorno de ejecución esté correctamente conectado a la GPU

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

"""We check the capacity of the RAM."""

# Comprobamos la capacidad de la RAM

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

"""Scikit-learn is a machine learning library in Python that includes implementations of SVM, but it is not specifically designed to accelerate model training on GPU. To maximize GPU performance and significantly speed up training of linear SVMs, it has been considered to use cuML (Rapids.ai) as a specific GPU-accelerated SVM library. To do this, the following installations need to be performed.

cuML Documentation: https://docs.rapids.ai/api/cuml/stable/
"""

!git clone https://github.com/rapidsai/rapidsai-csp-utils.git > /dev/null
!python rapidsai-csp-utils/colab/pip-install.py > /dev/null

import cudf
import cuml
from cuml.svm import LinearSVC
from cuml.metrics import accuracy_score

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# from sklearn.model_selection import GridSearchCV
# 
# #Crear los objetos GPU
# X_train_gpu = cudf.DataFrame(dev_sae).values
# y_train_gpu = cudf.Series(dev_y).values
# 
# # Define los valores de hiperparámetros que deseas explorar en la búsqueda en cuadrícula
# param_grid = {
#     'C': [0.01, 0.1, 1.0, 10.0],
#     'penalty': ['l1', 'l2'],
#     'loss': ['hinge', 'squared_hinge'],
# }
# 
# # Crea el modelo LinearSVC
# svm_classifier = LinearSVC(max_iter=10000)
# 
# # Crea un objeto GridSearchCV para la búsqueda en cuadrícula
# grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid, cv=5)
# 
# # Ajusta el modelo a los datos de entrenamiento en GPU
# grid_search.fit(X_train_gpu.get(), y_train_gpu.get())
# 
# best_params = grid_search.best_params_
# print("Mejores hiperparámetros:", best_params)

"""PROCESSED DATA (WT + NORM + SAE):

Note that the execution times are very short thanks to the GPU.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# X_train_gpu = cudf.DataFrame(train_sae).values
# y_train_gpu = cudf.Series(train_y).values
# X_test_gpu = cudf.DataFrame(test_sae).values
# 
# # Crear un modelo de LinearSVC
# svm_classifier = LinearSVC(C=0.15, penalty='l1', loss='squared_hinge', max_iter=10000)
# 
# # Entrenar el modelo con los datos de entrenamiento
# modelo_svm = svm_classifier.fit(X_train_gpu, y_train_gpu)
# 
# # Realizar predicciones en los datos de prueba
# y_svm = modelo_svm.predict(X_test_gpu)
# 
# accuracy = accuracy_score(test_y, y_svm)
# print(f'Precisión del modelo LinearSVC en GPU: {accuracy * 100:.2f}%')

"""PROCESSED DATA (WT + NORM):"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# X_train_gpu = cudf.DataFrame(train_norm).values
# X_test_gpu = cudf.DataFrame(test_norm).values
# 
# modelo_svm1 = svm_classifier.fit(X_train_gpu, y_train_gpu)
# 
# y_svm1 = modelo_svm1.predict(X_test_gpu)
# 
# accuracy = accuracy_score(test_y, y_svm1)
# print(f'Precisión del modelo SVM Classifier en GPU: {accuracy * 100:.2f}%')

"""PROCESSED DATA (WT):"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# X_train_gpu = cudf.DataFrame(train_wt).values
# X_test_gpu = cudf.DataFrame(test_wt).values
# 
# modelo_svm2 = svm_classifier.fit(X_train_gpu, y_train_gpu)
# 
# y_svm2 = modelo_svm2.predict(X_test_gpu)
# 
# accuracy = accuracy_score(test_y, y_svm2)
# print(f'Precisión del modelo SVM Classifier en GPU: {accuracy * 100:.2f}%')

"""RAW DATA:"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# X_train_gpu = cudf.DataFrame(train_x).values
# X_test_gpu = cudf.DataFrame(test_x).values
# 
# modelo_svm2 = svm_classifier.fit(X_train_gpu, y_train_gpu)
# 
# y_svm2 = modelo_svm2.predict(X_test_gpu)
# 
# accuracy = accuracy_score(test_y, y_svm2)
# print(f'Precisión del modelo SVM Classifier en GPU: {accuracy * 100:.2f}%')

"""In this model, both normalization and wavelet transform add predictive capability to the model. However, similar to Random Forest, the predictability decreases when adding stacked autoencoders to the preprocessing. Below, the best of the models (WT + NORM) shows an accuracy rate on buy signals of 63.80%."""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

confusion = confusion_matrix(test_y, y_svm1.get())

confusion_percentage = confusion.astype('float') / confusion.sum(axis=0)[np.newaxis, :]

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_percentage, annot=True, fmt=".2%", cmap="Blues", xticklabels=["Negativo", "Positivo"], yticklabels=["Negativo", "Positivo"])
plt.xlabel('Predicciones')
plt.ylabel('Etiquetas Reales')
plt.title('Matriz de Confusión en Porcentajes')
plt.show()

"""We check the predictions of all the models so far.





"""

y_lr

y_rf1

y_xgb

y_svm1

test_y

"""We merge them into a single dataframe.





"""

results_pred = pd.DataFrame({
    'LR_pred': y_lr,
    'RF_pred': y_rf1,
    'XGB_pred': y_xgb,
    'SVM_pred': y_svm1.get(),
    'Y_real': test_y
})

results_pred['XGB_pred'] = results_pred['XGB_pred'].astype(float)

results_pred

"""
We export them to the SQL database because we are going to change the execution environment to TPU to train the neural networks next, and we don't want to lose the local variables."""

def crear_tabla_desde_dataframe(df, tabla, connection):
    cursor = connection.cursor()

    create_table_query = f"CREATE TABLE {tabla} (id INT AUTO_INCREMENT PRIMARY KEY"

    for columna in df.columns:
        create_table_query += f", `{columna}` FLOAT"

    create_table_query += ");"
    cursor.execute(create_table_query)

crear_tabla_desde_dataframe(results_pred, 'results_pred', connection)

from sqlalchemy import create_engine

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

def cargar_dataframe_en_tabla(df, tabla):
    df.to_sql(tabla, con = engine, if_exists='replace', index=False)

cargar_dataframe_en_tabla(results_pred, 'results_pred')

connection.close()
engine.dispose()

"""###<u>Long Short-Term Memory</u>

To train the LSTM recurrent neural networks, we first verify that the execution environment is correctly connected to the Google TPU.
"""

import tensorflow as tf

# Detectar si estamos usando una TPU
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    print('Connected to a TPU.')
except ValueError:
    tpu = None
    print('Not connected to a TPU.')

"""We ensure that the RAM capacity is 37.8 GB."""

# Comprobamos la capacidad de la RAM

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

"""
Given that LSTM is a recurrent neural network, which has memory capacity, it needs to be fed with three-dimensional arrays instead of two-dimensional ones. To achieve this, we define a function that splits the data into groups of 30 consecutive sessions (this number has been determined through trial and error)."""

import numpy as np

def data_delay(x_data, y_data, delay):
    x_data = np.array(x_data)
    y_data = np.array(y_data)
    X = np.array([x_data[i - delay:i, :].copy() for i in range(delay, len(x_data))])
    y = np.array(y_data[delay:])

    print('Dimensiones de X (número de secuencias, longitud de la secuencia, número de características): ', X.shape)
    print('Dimensiones de y: ', y.shape)

    return X, y

X_train, Y_train = data_delay(train_sae, train_y, 30)

"""Apply the transformation function to the dev data, which will be used to evaluate the model during training, and to the test data to evaluate the model overall."""

X_val, Y_val = data_delay(dev_sae, dev_y, 30)

X_test, Y_test = data_delay(test_sae, test_y, 30)

"""We define the model with TensorFlow. It consists of an input layer with the specified dimensions (30 days, 47 variables), 3 hidden LSTM layers with 200 neurons each, and a final output layer with the 'sigmoid' activation function to output either a 1 or a 0. A dropout rate of 30% is introduced between the hidden layers to prevent overfitting. We set 'unroll=True' to optimize training performance by unrolling time iterations during compilation.

The specified loss function is binary cross-entropy, and the metric is accuracy. The 'early_stopping' object saves computational resources and prevents overfitting by stopping training when there is no improvement of at least 0.001 in accuracy on the validation set for 50 epochs. At this point, the weights of the model with the best accuracy are restored. Once all these parameters are defined through trial and error, we train the model for a maximum of 1000 epochs with a batch size of 130.

TensorFlow documentation: https://www.tensorflow.org/api_docs

LSTM documentation: https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM

PROCESSED DATA (WT + NORM + SAE):
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, InputLayer, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping

# Creamos un modelo LSTM
model = Sequential()

drop_rate = 0.3

# Agregamos las capas de neuronas
model.add(InputLayer(input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(LSTM(200, return_sequences=True, unroll=True))
model.add(Dropout(drop_rate))
model.add(LSTM(200, return_sequences=True, unroll=True))
model.add(Dropout(drop_rate))
model.add(LSTM(200, unroll=True))
model.add(Dense(1, activation='sigmoid'))  # Capa de salida binaria

# Compilamos el modelo
model.compile(optimizer=tf.keras.optimizers.Adam(),
              loss='binary_crossentropy',
              metrics=['accuracy'])

early_stopping = EarlyStopping(
    monitor='val_accuracy',  # Monitorizar la precisión de validación
    min_delta=0.001,         # Cambio mínimo para considerar una mejora
    patience=50,             # Número de épocas a esperar antes de detenerse
    restore_best_weights=True
)

# Entrenamos el modelo
history = model.fit(X_train, Y_train, epochs=1000, batch_size=130, callbacks=[early_stopping], validation_data=(X_val, Y_val))

"""Evaluamos el modelo entrenado en test."""

# Evaluar el modelo en el conjunto de prueba
loss, accuracy = model.evaluate(X_test, Y_test)
print(f'Precisión en el conjunto de prueba: {accuracy * 100:.2f}%')

"""Obtenemos las predicciones de este modelo."""

# Obtener las predicciones para el conjunto de prueba
y_lstm = model.predict(X_test)

# Convertir las probabilidades en etiquetas de clase (0 o 1)
y_lstm = (y_pred > 0.5).astype(int)

print(len(y_lstm))

"""PROCESSED DATA (WT + NORM):

Preprocesamos los datos con la función e indicamos 'verbose=0' para que no se imprima el entrenamiento en epochs y salga directamente el resultado.
"""

X_train, Y_train = data_delay(train_norm, train_y, 30)
X_val, Y_val = data_delay(dev_norm, dev_y, 30)
X_test, Y_test = data_delay(test_norm, test_y, 30)

lstm1 = model.fit(X_train, Y_train, epochs=1000, batch_size=130, callbacks=[early_stopping], validation_data=(X_val, Y_val), verbose=0)

loss, accuracy = model.evaluate(X_test, Y_test)
print(f'Precisión en el conjunto de prueba: {accuracy * 100:.2f}%')

"""PROCESSED DATA (WT):"""

X_train, Y_train = data_delay(train_wt, train_y, 30)
X_val, Y_val = data_delay(dev_wt, dev_y, 30)
X_test, Y_test = data_delay(test_wt, test_y, 30)

lstm2 = model.fit(X_train, Y_train, epochs=1000, batch_size=130, callbacks=[early_stopping], validation_data=(X_val, Y_val), verbose=0)

loss, accuracy = model.evaluate(X_test, Y_test)
print(f'Precisión en el conjunto de prueba: {accuracy * 100:.2f}%')

"""RAW DATA:"""

X_train, Y_train = data_delay(train_x, train_y, 30)
X_val, Y_val = data_delay(dev_x, dev_y, 30)
X_test, Y_test = data_delay(test_x, test_y, 30)

lstm3 = model.fit(X_train, Y_train, epochs=1000, batch_size=130, callbacks=[early_stopping], validation_data=(X_val, Y_val), verbose=0)

loss, accuracy = model.evaluate(X_test, Y_test)
print(f'Precisión en el conjunto de prueba: {accuracy * 100:.2f}%')

"""In this case, it seems that the only preprocessing step that has contributed predictive value is normalization. However, the predictive capacity of the neural networks is poor, and the small difference between the model with stacked autoencoders and without stacked autoencoders may be due to randomness. In the confusion matrix, we observe an accuracy rate of 55.10%."""

from keras.layers import LSTM
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

y_lstm = model.predict(X_test)

# Aplicar un umbral de 0.5 para convertir las probabilidades en 1 o 0
y_lstm_binary = (y_lstm >= 0.5).astype(int).ravel()

test_y_lstm = test_y[30:]

confusion = confusion_matrix(test_y_lstm, y_lstm_binary)

confusion_percentage = confusion.astype('float') / confusion.sum(axis=0)[np.newaxis, :]

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_percentage, annot=True, fmt=".2%", cmap="Blues", xticklabels=["Negativo", "Positivo"], yticklabels=["Negativo", "Positivo"])
plt.xlabel('Predicciones')
plt.ylabel('Etiquetas Reales')
plt.title('Matriz de Confusión en Porcentajes')
plt.show()

"""Here is the plot showing the loss function and accuracy of the training and validation sets during training (the x-axis represents the epochs). It is clear that the model tends to overfit, but the methods introduced to reduce overfitting showed an even worse predictive capacity on the test set, so it has been decided to leave it as it is."""

history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot();
print("Minimum validation loss: {}".format(history_df['val_loss'].min()))

history_df = pd.DataFrame(history.history)
history_df.loc[:, ['accuracy', 'val_accuracy']].plot();
print("Maximum validation accuracy: {}".format(history_df['val_accuracy'].max()))

"""###<u>KNN + XGBoost</u>

Now we are going to use a combination of the best performing algorithms so far and K-nearest neighbors (KNN). The idea is to preprocess the training data with an unsupervised classification model. This way, sessions will be divided into classes based on their similarity with other sessions. Subsequently, an XGBoost algorithm will be trained for each of these classes. The test sessions will be classified based on the training groups, and either a trained XGBoost algorithm or another one will be employed based on the class to which that test session belongs.

KMeans Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
"""

knn_x = np.array(train_sae)

"""We perform the elbow method to obtain the optimal number of clusters (K).





"""

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Rango de valores de k que deseas probar
k_values = range(1, 20)

# Lista para almacenar las inercias (suma de distancias al cuadrado)
inertias = []

# Realizar K-Means clustering para diferentes valores de k
for k in k_values:
    kmeans = KMeans(n_clusters=k, n_init=10)
    kmeans.fit(knn_x)
    inertias.append(kmeans.inertia_)

# Graficar la inercia en función de k
plt.figure(figsize=(8, 5))
plt.plot(k_values, inertias, marker='o')
plt.title('Método del Codo')
plt.xlabel('Número de Clusters (k)')
plt.ylabel('Inercia')
plt.grid(True)
plt.show()

"""It seems that from 3 or 4 clusters onwards, the slope of the function decreases, but as it is not very clear, we will also perform the silhouette score test, in which we finally clear up the doubts and determine 3 as the optimal number of clusters."""

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Rango de valores de k que deseas probar
k_values = range(2, 11)

# Lista para almacenar los valores de silhouette para cada k
silhouette_scores = []

for k in k_values:
    kmeans = KMeans(n_clusters=k, n_init=10)
    cluster_labels = kmeans.fit_predict(knn_x)

    # Calcular el valor de silhouette para este k
    silhouette_avg = silhouette_score(knn_x, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Graficar el valor de silhouette en función de k
plt.figure(figsize=(8, 5))
plt.plot(k_values, silhouette_scores, marker='o')
plt.title('Método Silhouette')
plt.xlabel('Número de Clusters (k)')
plt.ylabel('Valor de Silhouette')
plt.grid(True)
plt.show()

"""We fit the model and observe the number of sessions for each of the 3 classes.





"""

from sklearn.cluster import KMeans

# Creamos un objeto KMeans con el número deseado de clústeres (k)
k = 3
kmeans = KMeans(n_clusters=k, n_init='auto', random_state = 42)

# Ajustamos el modelo K-Means a los datos
kmeans.fit(knn_x)

# Obtenemos las etiquetas de clúster asignadas a cada muestra
train_labels = kmeans.labels_

# Declaramos una variable con los centros de los clusters
cluster_centers = kmeans.cluster_centers_

cluster_counts = np.bincount(train_labels)

# cluster_counts[i] contiene el número de elementos en el clúster i
for i, count in enumerate(cluster_counts):
    print(f'Número de elementos en el clúster {i}: {count}')

"""As we are now moving to a supervised classification problem, we will need labels for the data. To do this, we define the following function, which will group sessions by their classes and add the labels (target variable)."""

def data_transform(x, labels, y):
    knn_x_label = np.column_stack((x[:, :47], labels, y))

    cluster_1 = []
    cluster_2 = []
    cluster_3 = []

    for array in knn_x_label:
        if array[-2] == 1:  # Usar el índice -2 para acceder a la columna 47
            cluster_1.append(array)
        elif array[-2] == 2:
            cluster_2.append(array)
        else:
            cluster_3.append(array)

    cluster_1 = np.array(cluster_1)
    cluster_2 = np.array(cluster_2)
    cluster_3 = np.array(cluster_3)

    cluster_1 = np.delete(cluster_1, -2, axis=1)
    cluster_2 = np.delete(cluster_2, -2, axis=1)
    cluster_3 = np.delete(cluster_3, -2, axis=1)

    cluster_1_y = cluster_1[:, -1]
    cluster_2_y = cluster_2[:, -1]
    cluster_3_y = cluster_3[:, -1]

    cluster_1_x = cluster_1[:, :-1]
    cluster_2_x = cluster_2[:, :-1]
    cluster_3_x = cluster_3[:, :-1]

    return cluster_1_x, cluster_2_x, cluster_3_x, cluster_1_y, cluster_2_y, cluster_3_y

train_1_x, train_2_x, train_3_x, train_1_y, train_2_y, train_3_y = data_transform(knn_x, train_labels, train_y)

"""We check that the function has processed the data correctly."""

print('Dimensiones de train_1_x:', train_1_x.shape)
print('Dimensiones de train_2_x:', train_2_x.shape)
print('Dimensiones de train_3_x:', train_3_x.shape)
print('Dimensiones de train_1_y:', train_1_y.shape)
print('Dimensiones de train_2_y:', train_2_y.shape)
print('Dimensiones de train_3_y:', train_3_y.shape)

"""When classifying the training set based on the clusters adjusted in the training set, it is observed that all sessions belong to the same cluster. This will prevent us from having to train the model three times, as we only need to train it with the training data from cluster 0."""

knn_test = np.array(test_sae)
test_labels = kmeans.predict(test_sae)

cluster_counts = np.bincount(test_labels)

for i, count in enumerate(cluster_counts):
    print(f'Número de elementos en el clúster {i}: {count}')

"""We check the train and test sets."""

print(train_3_x.shape)
train_3_x[0]

print(train_3_y.shape)
train_3_y

print(knn_test.shape)
knn_test[0]

print(test_y.shape)
test_y

"""By training and tuning the hyperparameters of the XGBoost model, we realize that we can achieve the same accuracy as the model with 1752 records, but this time with only the 680 records most similar to the test data. This can be very useful when training datasets with thousands of records, as computational resources can be saved by training models only with the most similar records."""

import xgboost as xgb
from sklearn.metrics import accuracy_score

xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42, n_estimators=71, learning_rate=0.1, max_depth=1,
                              subsample=0.6, colsample_bytree=0.6, reg_alpha=0.001, reg_lambda=1)

modelo_xgb = xgb_model.fit(train_3_x, train_3_y)

y_xgb3 = modelo_xgb.predict(knn_test)

accuracy = accuracy_score(test_y, y_xgb3)
print(f'Precisión del modelo XGBoost Classifier: {accuracy * 100:.2f}%')

"""The accuracy is much lower with the groups that have fewer similarities."""

modelo_xgb = xgb_model.fit(train_1_x, train_1_y)

y_xgb1 = modelo_xgb.predict(knn_test)

accuracy = accuracy_score(test_y, y_xgb1)
print(f'Precisión del modelo XGBoost Classifier: {accuracy * 100:.2f}%')

modelo_xgb = xgb_model.fit(train_2_x, train_2_y)

y_xgb2 = modelo_xgb.predict(knn_test)

accuracy = accuracy_score(test_y, y_xgb2)
print(f'Precisión del modelo XGBoost Classifier: {accuracy * 100:.2f}%')

"""We print the confusion matrix to see the accuracy of positive and negative predictions.





"""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

confusion = confusion_matrix(test_y, y_xgb3)

confusion_percentage = confusion.astype('float') / confusion.sum(axis=0)[np.newaxis, :]

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_percentage, annot=True, fmt=".2%", cmap="Blues", xticklabels=["Negativo", "Positivo"], yticklabels=["Negativo", "Positivo"])
plt.xlabel('Predicciones')
plt.ylabel('Etiquetas Reales')
plt.title('Matriz de Confusión en Porcentajes')
plt.show()

"""# Results

We import the dataframe used before LSTM.
"""

!pip install protobuf mysql-connector-python > /dev/null
import mysql.connector

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

from sqlalchemy import create_engine, text
import pandas as pd

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

conn = engine.connect()

query = text(f'SELECT * FROM results_pred')
results_pred = pd.read_sql(query, con=conn)

conn.close()
connection.close()

results_pred

"""We add the predictions from KNN+XGBoost and LSTM to the predictions dataframe.





"""

y_lstm_padded = np.pad(y_lstm_binary, (0, len(results_pred) - len(y_lstm_binary)), mode='constant', constant_values=-1)

results_pred['LSTM_pred'] = y_lstm_padded

results_pred['KNNXGB_pred'] = pd.Series(y_xgb3, index=results_pred.index)

results_pred.replace(-1, np.nan, inplace=True)

results_pred

"""The null values in the LSTM predictions (produced by the 30-day length of the matrix) are replaced with -1 instead of NaN to avoid compatibility issues. The matrix is then converted to integers."""

import numpy as np

results_pred['LSTM_pred'] = results_pred['LSTM_pred'].shift(30)

results_pred['LSTM_pred'].fillna(-1, inplace=True)

for col in results_pred.columns:
    results_pred[col] = pd.to_numeric(results_pred[col], errors='coerce', downcast='integer')


results_pred

"""
To evaluate combinations of multiple algorithms, they will be joined starting with those that have the best predictive ability. The idea is that trading only occurs the next day if all algorithms give a positive entry signal."""

results_pred['KNNXGB_XGB'] = np.where((results_pred['XGB_pred'] == 1) & (results_pred['KNNXGB_pred'] == 1), 1, 0)
results_pred['KNNXGB_XGB_RF'] = np.where((results_pred['RF_pred'] == 1) & (results_pred['KNNXGB_XGB'] == 1), 1, 0)
results_pred['KNNXGB_XGB_RF_SVM'] = np.where((results_pred['SVM_pred'] == 1) & (results_pred['KNNXGB_XGB_RF'] == 1), 1, 0)
results_pred['KNNXGB_XGB_RF_SVM_LR'] = np.where((results_pred['LR_pred'] == 1) & (results_pred['KNNXGB_XGB_RF_SVM'] == 1), 1, 0)
results_pred['NO_pred'] = 1

results_pred

results_arrays = [results_pred[col].values for col in results_pred.columns]

"""###<u>Accuracy</u>

We evaluate the accuracy by adding a strategy that would consist of trading every day (NO_pred).
"""

from sklearn.metrics import accuracy_score

for col in results_pred.columns:
    if col == 'Y_real':
      continue
    elif col == 'LSTM_pred':
      lstm_y = results_pred[col][30:]
      real_y = results_pred['Y_real'][30:] # los 30 primeros son los nulos, no los últimos
      accuracy = accuracy_score(lstm_y, real_y)
      print(f'Precisión de {col}: {accuracy * 100:.2f}%')
    else:
      accuracy = accuracy_score(results_pred[col], results_pred['Y_real'])
      print(f'Precisión de {col}: {accuracy * 100:.2f}%')

"""We calculate the total increase in the candle of the next day captured by each algorithm."""

for column in results_pred.columns:
    if column != 'Y_real':
        correct_predictions = ((results_pred[column] == 1) & (results_pred['Y_real'] == 1)).sum()
        total_positive_actual = (results_pred['Y_real'] == 1).sum()

        accuracy = correct_predictions / total_positive_actual * 100 if total_positive_actual > 0 else 0

        print(f'Porcentaje de veces que Y_real es 1 y {column} predice 1: {accuracy:.2f}%')

"""
We calculate the reliability of each algorithm: the percentage of times it predicts that the candle of the next day will rise and it actually does."""

for column in results_pred.columns:
    if column != 'Y_real':
        correct_predictions = ((results_pred[column] == 1) & (results_pred['Y_real'] == 1)).sum()
        total_positive_predictions = (results_pred[column] == 1).sum()

        accuracy = correct_predictions / total_positive_predictions * 100 if total_positive_predictions > 0 else 0

        print(f'Porcentaje de veces que {column} predice 1 e Y_real es 1: {accuracy:.2f}%')

"""We import the Nasdaq dataframe to calculate profitability.




"""

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

conn = engine.connect()

query = text(f'SELECT * FROM nasdaq_eda')
nasdaq = pd.read_sql(query, con=conn)

conn.close()
connection.close()

nasdaq

nasdaq_test = nasdaq[-501:]

nasdaq_test = nasdaq_test.reset_index(drop=True)


nasdaq_test

"""We merge the dataframe with predicitions."""

nasdaq_pred = nasdaq_test.merge(results_pred, left_index=True, right_index=True)
nasdaq_pred.set_index('Date', inplace=True)

nasdaq_pred

"""###<u>Return</u>

First, we calculate the total compounded interest profitability obtained from the beginning of the period (July 1, 2021) until the end of each year with a Buy&Hold strategy (buying the NASDAQ and holding it in the portfolio without selling it).
"""

# Calcular los rendimientos diarios
returns = nasdaq_pred['Close'].pct_change().dropna()

# Crear un DataFrame para almacenar las rentabilidades anuales acumuladas
annual_returns = pd.DataFrame(index=returns.index.year.unique(), columns=['Annual Return'])

# Inicializar una variable para almacenar la rentabilidad acumulada
cumulative_return = 0

# Calcular y acumular la rentabilidad acumulada por año
for year in annual_returns.index:
    # Filtrar los rendimientos hasta el final del año actual
    year_returns = returns[returns.index.year <= year]

    # Calcular la rentabilidad acumulada utilizando la fórmula del interés compuesto
    cumulative_return = (1 + year_returns).prod() - 1

    # Asignar el valor al DataFrame de rentabilidades anuales acumuladas
    annual_returns.loc[year, 'Annual Return'] = cumulative_return

# Formatear la columna 'Annual Return' como porcentaje con dos decimales
annual_returns['Annual Return'] = annual_returns['Annual Return'].map('{:.2%}'.format)

annual_returns

"""
We print the compounded interest profitability each year, meaning from the beginning of each year until the end of each year with the Buy&Hold strategy."""

returns = nasdaq_pred['Close'].pct_change().dropna()

annual_returns = pd.DataFrame(index=returns.index.year.unique(), columns=['Annual Return'])

# Calcular la rentabilidad del último día del año respecto al primer día de ese año
for year in annual_returns.index:
    year_returns = returns[returns.index.year == year]
    annual_return = (1 + year_returns).prod() - 1
    annual_returns.loc[year, 'Annual Return'] = annual_return

annual_returns['Annual Return'] = annual_returns['Annual Return'].map('{:.2%}'.format)


annual_returns

"""Now we calculate the cumulative compounded interest profitability from the beginning of the period until the end of each year with the strategy followed by each of our algorithms."""

# Crear un diccionario para almacenar las rentabilidades anuales
rentabilidades_anuales = {}

nasdaq_pred['Rendimiento Diario'] = nasdaq_pred['Close'] / nasdaq_pred['Open'] - 1

# Calcular las rentabilidades anuales para cada 'pred'
for pred in results_pred.columns:
    # Filtrar los días en que se compra y vende
    dias_compra_venta = nasdaq_pred[nasdaq_pred[pred].shift(1) == 1].copy()

    # Calcular el rendimiento acumulado diario para los días de compra y venta
    dias_compra_venta['Rendimiento Acumulado Diario'] = (1 + dias_compra_venta['Rendimiento Diario']).cumprod() - 1

    # Agrupar por año y calcular el rendimiento anual
    rendimiento_anual = dias_compra_venta.groupby(dias_compra_venta.index.year)['Rendimiento Acumulado Diario'].last()

    # Almacenar el rendimiento anual en el diccionario
    rentabilidades_anuales[pred] = rendimiento_anual

# Crear un DataFrame a partir del diccionario
rentabilidades_acumuladas_totales = pd.DataFrame(rentabilidades_anuales)

rentabilidades_acumuladas_totales = rentabilidades_acumuladas_totales.applymap('{:.2%}'.format)

rentabilidades_acumuladas_totales

"""We do the same but dividing by years: from the beginning to the end of each year.






"""

rentabilidades_anuales = {}

nasdaq_pred['Rendimiento Diario'] = nasdaq_pred['Close'] / nasdaq_pred['Open'] - 1

for pred in results_pred.columns:
    dias_compra_venta = nasdaq_pred[nasdaq_pred[pred].shift(1) == 1].copy()

    dias_compra_venta['Rendimiento Diario'] = dias_compra_venta['Close'] / dias_compra_venta['Open'] - 1

    dias_compra_venta['Rendimiento Acumulado Diario'] = (1 + dias_compra_venta['Rendimiento Diario']).cumprod() - 1

    agupamiento_anual = dias_compra_venta.groupby(dias_compra_venta.index.year)

    rentabilidades_pred = []

    for year, group in agupamiento_anual:
        rendimiento_anual = ((1 + group['Rendimiento Diario']).prod() - 1)

        rentabilidades_pred.append(rendimiento_anual)

    rentabilidades_anuales[pred] = rentabilidades_pred

rentabilidades_acumuladas_anuales = pd.DataFrame(rentabilidades_anuales, index=dias_compra_venta.index.year.unique())

rentabilidades_acumuladas_anuales = rentabilidades_acumuladas_anuales.applymap('{:.2%}'.format)

rentabilidades_acumuladas_anuales

"""###<u>Risk</u>

We calculate the Value at Risk (VaR) for the Buy&Hold strategy and for the algorithms.

VaR
"""

returns = nasdaq_pred['Close'].pct_change().dropna()
alpha = 0.05  # Nivel de confianza del 95%
var = returns.quantile(alpha)
print(f'VaR al {alpha * 100}%: {var:.2%}')

nasdaq_pred['Rendimiento Diario'] = nasdaq_pred['Close'] / nasdaq_pred['Open'] - 1

alpha = 0.05

for pred in results_pred.columns:
    dias_compra_venta = nasdaq_pred[nasdaq_pred[pred].shift(1) == 1].copy()

    # Calcular el VaR para el rendimiento diario
    var = dias_compra_venta['Rendimiento Diario'].quantile(alpha)
    print(f'VaR al {alpha * 100}% para {pred}: {var:.2%}')

"""Maximum Drawdown

We calculate the maximum loss in each of the strategies.
"""

cumulative_returns = (1 + returns).cumprod()
peak = cumulative_returns.cummax()
drawdown = (cumulative_returns - peak) / peak
max_drawdown = drawdown.min()
print(f'Maximum Drawdown: {max_drawdown:.2%}')

for pred in results_pred.columns:
    dias_compra_venta = nasdaq_pred[nasdaq_pred[pred].shift(1) == 1].copy()

    dias_compra_venta['Rendimiento Acumulado Diario'] = dias_compra_venta['Rendimiento Diario'].cumsum()

    # Calcular el Máximo Drawdown para cada 'pred'
    cumulative_returns = (1 + dias_compra_venta['Rendimiento Diario']).cumprod()
    peak = cumulative_returns.cummax()
    drawdown = (cumulative_returns - peak) / peak
    max_drawdown = drawdown.min()
    print(f'Maximum Drawdown para {pred}: {max_drawdown:.2%}')

"""Sharpe Ratio

We calculate the Sharpe Ratio to assess the risk-adjusted return. The risk-free rate chosen is the yield on the 10-year US Treasury bond (US10YT) on October 13, 2023: 4.62%.
"""

returns = nasdaq_pred['Close'].pct_change().dropna()
cumulative_returns = (1 + returns).cumprod() - 1

risk_free_rate = 0.0462  # Tasa libre de riesgo

sharpe_ratio = (cumulative_returns[-1] - risk_free_rate) / cumulative_returns.std()

print(f'Sharpe Ratio: {sharpe_ratio:.2f}')

for pred in results_pred.columns:
    dias_compra_venta = nasdaq_pred[nasdaq_pred[pred].shift(1) == 1].copy()

    dias_compra_venta['Rendimiento Diario'] = dias_compra_venta['Close'] / dias_compra_venta['Open'] - 1
    dias_compra_venta['Rendimiento Acumulado Diario'] = (1 + dias_compra_venta['Rendimiento Diario']).cumprod() - 1

    # Calcular el Sharpe Ratio
    cumulative_returns = dias_compra_venta['Rendimiento Acumulado Diario']
    sharpe_ratio = (cumulative_returns.iloc[-1] - risk_free_rate) / cumulative_returns.std()

    print(f'Sharpe Ratio para {pred}: {sharpe_ratio:.2f}')

"""We save the dataframe that combines the predictions with the NASDAQ in the SQL database.





"""

nasdaq_pred.reset_index(inplace=True)
nasdaq_pred.drop('Rendimiento Diario', axis=1, inplace=True)

nasdaq_pred

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

def crear_tabla_desde_dataframe(df, tabla, connection):
    cursor = connection.cursor()

    create_table_query = f"CREATE TABLE {tabla} (id INT AUTO_INCREMENT PRIMARY KEY"

    for columna in df.columns:
        create_table_query += f", `{columna}` FLOAT"

    create_table_query += ");"
    cursor.execute(create_table_query)

# Llamada a la función con las listas anteriores
crear_tabla_desde_dataframe(nasdaq_pred, 'nasdaq_pred', connection)

from sqlalchemy import create_engine

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

def cargar_dataframe_en_tabla(df, tabla):
    df.to_sql(tabla, con = engine, if_exists='replace', index=False)


cargar_dataframe_en_tabla(nasdaq_pred, 'nasdaq_pred')

connection.close()
engine.dispose()

"""###<u>Conclusions</u>

1. The model with the highest accuracy is the combination of KNN+XGB, XGB, and Random Forest with an accuracy of 63.07%.
2. The model that captures the highest number of true positives is Random Forest with 76.63%.
3. The model most reliable at giving a positive signal is the combination of KNN+XGB, XGB, Random Forest, SVM, and Logistic Regression, with a 68.52% accuracy rate when predicting a positive signal.
4. The most profitable model by the end of 2021, 2022, and 2023 turns out to be the combination of KNN+XGB, XGB, and Random Forest, with a 155.60% return.
5. The models with the best 5% Value at Risk (VaR) are KNN+XGB, XGB, and Random Forest with or without SVM, with a -1.34% VaR.
6. The lowest maximum drawdown was experienced by all models containing the combination of KNN+XGB and XGB: -3.60%.
7. The best risk-adjusted return was achieved by the logistic regression model, with a Sharpe Ratio of 3.06.
"""