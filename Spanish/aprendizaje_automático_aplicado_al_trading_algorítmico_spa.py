# -*- coding: utf-8 -*-
"""Aprendizaje_Automático_Aplicado_al_Trading_Algorítmico_SPA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kmEVqWxD2wAANAsWSHOQvmd4AfVif9uJ

# **Extracción de Datos**

###<u>Datos de cotización</u>

En esta primera parte del código se procede a la extracción de las variables conocidas como OHLC (Open, High, Low y Close) de los datos de cotización del NASDAQ. Esto se hace mediante la librería "yfinance", que extrae los datos directamente desde la popular web Yahoo Finance.

Como se puede observar en la salida del código, tenemos las variables Volume, Dividends y Stock Splits aparte de las OHLC. El índice de los datos es la fecha de la sesión en la que se dieron esos datos. El rango de fechas para el estudio es de 10 años, pero al principio se ha extraído algo más ya que las primeras fechas serán eliminadas posteriormente por contener muchos valores nulos en los indicadores técnicos.

Documentación: https://pypi.org/project/yfinance/
"""

import yfinance as yf

# El ticker del NASDAQ Composite en Yahoo Finance es ^IXIC

nasdaq = yf.Ticker('^IXIC')

# Definir las fechas de inicio y fin en el formato "YYYY-MM-DD"

from_date = "2012-09-30"
to_date = "2023-06-30"

# Obtener el historial de precios en el rango de fechas

quote_data = nasdaq.history(start=from_date, end=to_date)


quote_data.head()

"""###<u>Indicadores Técnicos</u>

A continuación se define una función llamada "technical_indicators" que calcula hasta 14 indicadores técnicos a partir de los datos anteriormente extraídos. Los cálculos se realizan a través de fórmulas matemáticas empleadas por la librería "pandas_ta".

Documentación: https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#momentum-indicators
"""

# "/dev/null" evita que el desarrollo de la instalación se imprima por pantalla

!pip install pandas_ta > /dev/null
import pandas_ta as ta

def technical_indicators(stock_data, MAshort=9, MAmedium=50, MAlong=200, RSIlength=14, MOMlength=10, BBlength=20, BBstd=2):

  #Media móvil simple (solo medio y largo plazo porque el corto plazo ya viene dentro del RSI y Bandas de Bollinger)
  sma50 = stock_data.ta.sma(length = MAmedium, append = True) #el append=True hace que los datos se añadan al data frame stock_data
  sma200 = stock_data.ta.sma(length = MAlong, append = True)

  #Media móvil exponencial
  ema9 = stock_data.ta.ema(length = MAshort, append = True)
  ema50 = stock_data.ta.ema(length = MAmedium, append = True)
  ema200 = stock_data.ta.ema(length = MAlong, append = True)

  #Media móvil ponderada
  wma9 = stock_data.ta.wma(length = MAshort, append = True)
  wma50 = stock_data.ta.wma(length = MAmedium, append = True)
  wma200 = stock_data.ta.wma(length = MAlong, append = True)

  #Relative Strength Index (RSI)
  rsi = stock_data.ta.rsi(length = RSIlength, append = True)
  rsi_df = rsi.to_frame(name = 'close') #tiene que llamarse "close" para que la detecte bien
  rsi_ma = rsi_df.ta.sma(length = RSIlength, append = True) #media movil de 14 del RSI

  #Moving Average Convergence Divergence (MACD)
  macd = stock_data.ta.macd(append = True)

  #Stochastic Oscillator
  stoch = stock_data.ta.stoch(append = True)

  #Momentum
  momentum = stock_data.ta.mom(length = MOMlength, append = True)

  #Bandas de Bollinger
  bbands = stock_data.ta.bbands(length = BBlength, bb_std = BBstd, append = True) #la banda central (BBM) es la SMA de 20 periodos en este caso

  return stock_data


quote_tech = technical_indicators(quote_data)

quote_tech.head()

"""###<u>Soportes y Resistencias</u>

Los soportes y resistencias serán incluidos en el dataframe a través de dos variables binarias que toman valor 1 cuando el precio de cierre está en niveles de soporte o resistencia, y 0 en otro caso. Estos niveles los podemos obtener mediante dos funciones que establecen un nivel de soporte o resistencia en base a un número de velas con mínimos o máximos, decrecientes o ascendientes y posteriores o anteriores.

En este caso, para declarar un nivel de soporte el requisito es que las 4 velas consecutivas inmediatamente anteriores tengan mínimos descendientes, y las 4 velas posteriores tengan mínimos ascendientes. Lo mismo pasa con las resistencias pero al revés.

La salida del código es una lista de tuplas formadas por el número de fila en la que se declara el soporte o resistencia y el nivel de precio de este.
"""

def support(stock_data, l, n1, n2):
  for i in range(l-n1+1, l+1):
    if(stock_data.Low[i]>stock_data.Low[i-1]):
      return 0
  for i in range(l+1, l+n2+1):
    if(stock_data.Low[i]<stock_data.Low[i-1]):
      return 0
  return 1


def resistance(stock_data, l, n1, n2):
  for i in range(l-n1+1, l+1):
    if(stock_data.High[i]<stock_data.High[i-1]):
      return 0
  for i in range(l+1, l+n2+1):
    if(stock_data.High[i]>stock_data.High[i-1]):
      return 0
  return 1

supports = []
resistances = []

n1, n2 = 4, 4

for row in range(n1, len(quote_data)):
  if support(quote_data, row, n1, n2):
    supports.append((row, quote_data.Low[row]))
  if resistance(quote_data, row, n1, n2):
    resistances.append((row, quote_data.High[row]))

print("Soportes:", supports)
print("Resistencias:",resistances)

"""Dado que esta parte consiste en un proceso iterativo de prueba y error, vamos a ir imprimiendo un gráfico por pantalla con los resultados mediante la librería "mplfinance", la cual imprime gráficos de velas japonesas.

Documentación: https://pypi.org/project/mplfinance/
"""

!pip install mplfinance > /dev/null
import mplfinance as mpf

# Crear el gráfico de velas japonesas

stock_fig, axes = mpf.plot(quote_data, type='candle', style='charles', title=f'Gráfico de velas japonesas: NASDAQ Composite', ylabel='Price',
                           figsize=(15, 6), returnfig=True, warn_too_much_data=3000)

# Pintar las resitencias en verde

c = 0
while c < len(supports):
    axes[0].axhline(y=supports[c][1], color='green', linestyle='--', linewidth=1)
    c += 1

# Pintar las resitencias en naranja

c = 0
while c < len(resistances):
    axes[0].axhline(y=resistances[c][1], color='orange', linestyle='--', linewidth=1)
    c += 1

mpf.show()

"""Como se ha mencionado anteriormente, los soportes y resistencias, más que líneas, se tratan de zonas o áreas. En estas zonas los compradores y vendedores entran en conflicto hasta que uno de los dos grupos cede ante el otro. Con el objetivo de que las variables binarias capten todos los soportes y resistencias sin perder ninguno, vamos a convertir las líneas en áreas añadiendo un pequeño márgen a ambos lados del equivalente al 1% del precio a cada lado."""

stock_fig, axes = mpf.plot(quote_data, type='candle', style='charles', title=f'Gráfico de velas japonesas: NASDAQ Composite',
                           ylabel='Price', figsize=(15, 6), returnfig=True, warn_too_much_data=3000)

bound = 0.01

# Date tiene que ser columna y no índice porque si no da error

quote_data.reset_index(inplace=True)

# Dibujar áreas para soportes
for support in supports:
    value = support[1]
    lower_bound = value * (1-bound)
    upper_bound = value * (1+bound)
    axes[0].fill_between(quote_data['Date'].index, lower_bound, upper_bound, color='green', alpha=0.2)

# Dibujar áreas para resistencias
for resistance in resistances:
    value = resistance[1]
    lower_bound = value * (1-bound)
    upper_bound = value * (1+bound)
    axes[0].fill_between(quote_data['Date'].index, lower_bound, upper_bound, color='orange', alpha=0.2)

# Volvemos a poner las fechas como índice
quote_data = quote_data.set_index('Date')


mpf.show()

"""Dado que la validez de los soportes y resistencias no es infinita y cuanto mayor sea su proximidad, mayor validez tienen, vamos a limitar estos en el eje X.

La idea es que queremos crear variables que estén relacionadas con el precio y tengan un impacto significativo en este. Un soporte formado en 2013 va a tener poca o ninguna validez en 2023, por lo que vamos a limitar el número de sesiones para que un soporte sea válido a 250 sesiones, el equivalente a un año de cotización.

Este límite de 250 sesiones será desde el día en el que se forma el soporte o resistencia hasta las 250 sesiones siguientes. Dada la limitación de este algoritmo para detectar algunos soportes y resistencias en los que no concurre la condición de 4 máximos y mínimos anteriores y posteriores descendientes o ascendientes pero existen, cabría preguntarse si es buena idea hacer que este período de duración de las zonas de soporte y resistencia fuera también retroactivo. Es decir, en vez de ***día de soporte < zona soporte < día de soporte+250*** , utilizar la fórmula ***día de soporte - 125 < zona soporte < día de soporte+125***. Se ha comprobado visualmente que de esta manera se recuperan muchos soportes y resistencias perdidos, enriqueciendo así la información de la que dispone el modelo para entrenar.

El problema de este enfoque es que estaríamos dando una ventaja al modelo que no va a existir en la realidad. Si utilizamos este algoritmo de detección de soportes y resistencias para ponerlo en producción en la realidad, nunca vamos a contar con información sobre futuros soportes y resistencias que aún no se han formado, porque es imposible. Es por ellos que la variable dummy solo tomará 1 cuando esté en una zona de soporte y resistencia que, acorde al algoritmo de detección, ya se ha creado y existe, para igualar las condiciones en las que se entrena el modelo a las de la realidad.
"""

stock_fig, axes = mpf.plot(quote_data, type='candle', style='charles', title=f'Gráfico de velas japonesas: NASDAQ Composite',
                           ylabel='Price', figsize=(15, 6), returnfig=True, warn_too_much_data=3000)

sessions = 250

# Dibujar áreas para soportes limitadas a +- 125 velas
for support in supports:
    x_axis = support[0]
    y_axis = support[1]
    lower_x = x_axis
    upper_x = x_axis + sessions
    lower_y = y_axis * (1-bound)
    upper_y = y_axis * (1+bound)
    axes[0].fill_betweenx([lower_y, upper_y], lower_x, upper_x, color='green', alpha=0.2)

# Dibujar áreas para resistencias limitadas a +- 125 velas
for resistance in resistances:
    x_axis = resistance[0]
    y_axis = resistance[1]
    lower_x = x_axis
    upper_x = x_axis + sessions
    lower_y = y_axis * (1-bound)
    upper_y = y_axis * (1+bound)
    axes[0].fill_betweenx([lower_y, upper_y], lower_x, upper_x, color='orange', alpha=0.2)


mpf.show()

"""Por último, la función "dummy_sr" crea un diccionario en el que la clave es el número de fila y el valor es 1 si el precio de cierre está en un área de soporte/resistencia o 0 si no es así."""

def dummy_sr(sr_list):
  dummy_supres = {}
  for index, closing_price in enumerate(quote_data['Close']):
    dummy_supres[index] = 0  # Inicializa con 0 por defecto
    if any(sr[1]*(1-bound) <= closing_price <= sr[1]*(1+bound) for sr in sr_list): #Márgenes en el precio (eje y)
      if any(sr[0] <= index <= sr[0] + sessions for sr in sr_list): #Márgenes en las fechas (eje x)
        dummy_supres[index] = 1
    else:
        dummy_supres[index] = 0
  return(dummy_supres)

dummy_supports = dummy_sr(supports)
dummy_resistances = dummy_sr(resistances)

print(dummy_supports)
print(dummy_resistances)

"""Se incluyen ambas variables en el dataframe quote_tech"""

import pandas as pd

# Crear un nuevo DataFrame para dummy_supports
dummy_supports_df = pd.DataFrame.from_dict(dummy_supports, orient='index', columns=['dummy_support'])

# Crear un nuevo DataFrame para dummy_resistances
dummy_resistances_df = pd.DataFrame.from_dict(dummy_resistances, orient='index', columns=['dummy_resistance'])

# Resetear el índice para poder hacer el join
quote_tech.reset_index(inplace=True)

# Combinar los DataFrames usando el índice como clave
quote_tech = quote_tech.join(dummy_supports_df, how='outer')
quote_tech = quote_tech.join(dummy_resistances_df, how='outer')

# Volver a poner la fecha como índice
quote_tech = quote_tech.set_index('Date')

quote_tech.drop('index', axis=1, inplace=True)

quote_tech.head()

"""###<u>Indicadores Macroeconómicos</u>

Para la obtención de las variables macroeconómicas se va a utilizar la API de la FRED. Una vez importada la librería e introducida la clave de la API, para la cual nos hemos registrado con nuestro correo electrónico, se realiza una primera búsqueda del indicador de empleo ordenando por popularidad para que salgan los indicadores más utilizados primero.

Documentación API: https://fred.stlouisfed.org/docs/api/fred/

Documentación librería: https://frb.readthedocs.io/en/latest/
"""

!pip install fredapi > /dev/null
from fredapi import Fred

fred = Fred(api_key = '9868db94d0ab2e936e5a2812121fef5d')

sp_search = fred.search('Unemployment', order_by='popularity')

sp_search.head()

"""En este caso el indicador que buscamos tiene el identificador "UNRATE" (columna "series id"), pero la frecuencia es mensual y nuestros datos son diarios, así que vamos a intentar encontrar un indicador de empleo filtrando por frecuencia diaria."""

unemp_search = fred.search('unemployment', filter=('frequency', 'Daily'))

unemp_search

"""Al no haber diario, tendremos que seguir adelante con los datos mensuales. La extracción de los datos macroeconómicos se hace mediante un diccionario con los nombres e identificadores de cada indicador y con una función. La salida de la función serán dos dataframes, uno para los indicadores diarios y otro para los indicadores mensuales.

Todos los indicadores mensuales tienen 130 filas para las fechas seleccionadas. En cuanto a los indicadores diarios, la mayoría tienen 2805 filas pero hay algunos que tienen más, probablemente debido a festivos o a la propia idiosincrasia de los indicadores. En estos casos, el código elimina el excedente de los datos, que son aquellos para los cuales no hay una coincidencia en el índice de fechas.
"""

#diccionario de índices con sus nombres y claves para la API

indicators = {'unemployment':'UNRATE',
              '10y-2y yield curve':'T10Y2Y',
              'volatility':'VIXCLS',
              'inflation':'CORESTICKM159SFRBATL',
              'consumer expectation':'CSCICP03USM665S',
              'industrial production':'INDPRO',
              '5-year interest rate': 'DGS5',
              'federal interest rate': 'DFF',
              'junk bonds spread': 'BAMLH0A0HYM2'}

def fred_import(indicator_dict):
  indicators_monthly = pd.DataFrame()
  indicators_daily = pd.DataFrame()
  for key, value in indicator_dict.items():
    indicator = fred.get_series(series_id = value, observation_start = from_date, observation_end = to_date)
    info = fred.get_series_info(series_id=value) #información sobre el indicador, incluida su frecuencia
    if info['frequency'] == 'Monthly':
      indicators_monthly[key] = indicator
    else:
      indicators_daily[key] = indicator

  return indicators_monthly, indicators_daily

macro_monthly, macro_daily = fred_import(indicators)

# Dataframe de variables macroeconómicas mensuales

macro_monthly

# Dataframe de variables macroeconómicas diarias

macro_daily

"""###<u>Cotizaciones de otros activos</u>

Para finalizar con la extracción de datos, se van a importar datos de la cotización de otros activos que pueden estar correlacionados con el NASDAQ, tales como índices bursátiles de EEUU y otros países, futuros del oro y el petróleo.

Los datos que se obtienen son los precios de cierre diarios. Debido a la diferencia horaria de las sesiones de negociación de los índices mundiales, es necesario eliminar las horas del índice y dejar solo la fecha para que todos los activos puedan estar en el mismo dataframe.
"""

from datetime import datetime

indices_world = {'S&P500':'^GSPC',
                'Wilshire 5000':'^FTW5000',
                'Dow Jones Industrial Average':'^DJI',
                'Nikkei 225':'^N225',
                'Euro STOXX 50':'^STOXX50E',
                'Hang Seng Index':'^HSI',
                'Shanghai Stock Exchange':'000001.SS',
                'FTSE 100':'^FTSE',
                'Gold futures':'GC=F',
                'Crude oil futures':'CL=F',
                }

def yf_import(indicator_dict):
  other_assets = pd.DataFrame()
  for key, value in indicator_dict.items():
    asset_ticker = yf.Ticker(value)
    hist = asset_ticker.history(start=from_date, end=to_date)
    # Debido a la diferencia horaria entre índices, eliminamos las horas con la siguiente línea
    hist.index = hist.index.strftime('%Y-%m-%d')
    other_assets[key] = hist['Close']

  return other_assets

assets_quote = yf_import(indices_world)

# Dataframe con precios de cierre diarios de diferentes índices y materias primas

assets_quote

"""###<u>Conexión a la base de datos SQL</u>

Una vez extraídos los datos en bruto, se procede a su carga en una base de datos SQL. El objetivo es almacenar los datos en una base de datos persistente para realizar check points a lo largo del código. No será necesario el uso de bases de datos no relacionales dado que los datos tienen una estructura relacional.

Por motivos de compatibilidad y pragmatismo se ha escogido una base de datos SQL gratuita en la nube, cuyas credenciales están en el diccionario config. Esta base de datos gratuita ha sido obtenida del sitio web db4free.net (https://www.db4free.net/index.php?language=es). Se ha escogido MySQL como sistema de gestión de bases de datos relacional de código abierto. Para la gestión y supervisión de la base de datos se utiliza phpMyAdmin (https://www.phpmyadmin.co), una herramienta de administración de bases de datos basada en web que se utiliza para gestionar bases de datos MySQL. No es una base de datos en sí misma, sino una interfaz web que permite realizar tareas administrativas en bases de datos MySQL de manera más sencilla y visual.
"""

# Instalación e importación de la librería myql.connnector

!pip install protobuf==3.20.3 mysql-connector-python > /dev/null
import mysql.connector

"""Documentación mysql.connector: https://dev.mysql.com/doc/connector-python/en/"""

# Se configura la conexión con las credenciales de la base de datos
config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

# Se establecer la conexión con mensajes de confirmación
try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

# Se lanza un mensaje de error en caso de que se produzca un error

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

"""Primero se va a proceder a la creación de  tablas dentro de la base de datos "sql7644199", una para cada uno de los  dataframes que tenemos que subir. Dado que en este caso los índices contienen información muy valiosa que no queremos que se pierda, las fechas, se van a convertir los 4 índices a columnas para evitar pérdidas de información debido a la incompatibilidad."""

# Definición de dataframes y tablas SQL

dataframes = [quote_tech, macro_monthly, macro_daily, assets_quote]
table_names = ['quote_tech', 'macro_monthly', 'macro_daily', 'assets_quote']

for df in dataframes:
  df.reset_index(inplace=True)

"""Definimos una función que crea las tablas en la base de datos SQL a partir de SQL queries que recorren los dataframes y sus columnas, clasificándolas según el tipo de dato que contienen."""

def crear_tabla_desde_dataframe(df, tabla, connection):
    cursor = connection.cursor()
    create_table_query = f"CREATE TABLE {tabla} (id INT AUTO_INCREMENT PRIMARY KEY"

    for columna, tipo in zip(df.columns, df.dtypes):
        if tipo == "float64":
            create_table_query += f", `{columna}` FLOAT"
        elif tipo == "int64":
            create_table_query += f", `{columna}` INT"
        elif tipo == "datetime64[ns]":
            create_table_query += f", `{columna}` DATE"
        elif tipo == "bool":
            create_table_query += f", `{columna}` BOOLEAN"

    create_table_query += ");"
    cursor.execute(create_table_query)

# Llamada a la función con las listas anteriores

for df, table_name in zip(dataframes, table_names):
    crear_tabla_desde_dataframe(df, table_name, connection)

"""Por último, empleamos la librería sqlalchemy para rellenar las tablas creadas con los datos del dataframe. La conexión se cierra al terminar la carga para ahorrar recursos.

Documentación librería sqlalchemy: https://docs.sqlalchemy.org/en/20/intro.html#documentation-overview
"""

from sqlalchemy import create_engine

# Configuración de la conexión a la base de datos MySQL con las credenciales y el host

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

# if_exists='replace' hace que, si existe la base de datos, se reemplace con los nuevos datos

def cargar_dataframe_en_tabla(df, tabla):
    df.to_sql(tabla, con = engine, if_exists='replace', index=False)

# index=False hace que no se tenga en cuenta el índice, ya que este figura como una columna más

cargar_dataframe_en_tabla(quote_tech, 'quote_tech')
cargar_dataframe_en_tabla(macro_monthly, 'macro_monthly')
cargar_dataframe_en_tabla(macro_daily, 'macro_daily')
cargar_dataframe_en_tabla(assets_quote, 'assets_quote')

# Se cierra la conexión

connection.close()
engine.dispose()

"""# **Transformación e Integración de Datos**

En esta segunda fase del proyecto se van a limpiar y transformar los datos en bruto y se van a integrar los cuatro conjuntos de datos en un único conjunto.

###<u>Conexión a la base de datos SQL y recuperación de dataframes</u>

Para recuperar los datos sin tener que ejecutar el código anterior primero se establece la conexión de nuevo, tal y como se ha hecho anteriormente.
"""

!pip install protobuf==3.20.3 mysql-connector-python > /dev/null
import mysql.connector

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

"""Mediante la query SELECT * FROM se van recuperando uno a uno los datos de las tablas. Una vez extraídos todos los datos y creados los dataframes, se cierra la conexión."""

import pandas as pd
from sqlalchemy import create_engine, text

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

conn = engine.connect()

# Definición de la función para importar una tabla de SQL y convertirla en un DataFrame

def importar_tabla_a_dataframe(tabla, engine):
    query = text(f'SELECT * FROM {tabla}')
    df = pd.read_sql(query, con=conn)
    return df

quote_tech_sql = importar_tabla_a_dataframe('quote_tech', engine)
macro_monthly_sql = importar_tabla_a_dataframe('macro_monthly', engine)
macro_daily_sql = importar_tabla_a_dataframe('macro_daily', engine)
assets_quote_sql = importar_tabla_a_dataframe('assets_quote', engine)

# Cerrar la conexión correctamente

conn.close()
connection.close()

"""###<u>Revisión y Transformación</u>

Ahora se va a proceder a la revisión y transformación de los dataframes uno a uno, empezando por el dataframe con los datos de cotización e indicadores técnicos.
"""

quote_tech_sql

print(quote_tech_sql.columns)

# Calcular la frecuencia de cada valor en las columnas 'Dividends' y 'Stock Splits'

frecuencia_div = quote_tech_sql['Dividends'].value_counts()
frecuencia_split = quote_tech_sql['Stock Splits'].value_counts()

print('Valores de Dividends:', frecuencia_div, '\nValores de Stock Split:', frecuencia_split)

"""Las columnas "Dividends" y "Stock Splits" son constantes sin ningún valor explicativo para la variación diaria en la variable objetivo, por lo que las vamos a eliminar junto con la columna "index"."""

drop_columns = ['Dividends', 'Stock Splits']

quote_tech_sql = quote_tech_sql.drop(drop_columns, axis=1)

print(quote_tech_sql.columns)

"""Fijamos la columna date como índice, pero primero hay que comprobar que todas las columnas de fechas estén en formato fecha para que no haya problemas en la integración. Si alguna resulta ser de otro tipo, habrá que modificarla."""

print(quote_tech_sql['Date'].dtypes)
print(macro_monthly_sql['index'].dtypes)
print(macro_daily_sql['index'].dtypes)
print(assets_quote_sql['Date'].dtypes)

quote_tech_sql = quote_tech_sql.set_index('Date')

quote_tech_sql.head()

"""Pasamos al siguiente dataframe"""

macro_monthly_sql

"""Como para el siguiente dataframe hay que realizar las mismas modificaciones, definimos una función para no tener que repetir el código."""

def transform_df(df):
  df = df.rename(columns={'index': 'Date'})

  df = df.set_index('Date')

  return df

macro_monthly_sql = transform_df(macro_monthly_sql)

macro_monthly_sql.head()

macro_daily_sql

macro_daily_sql = transform_df(macro_daily_sql)

macro_daily_sql.head()

assets_quote_sql

assets_quote_sql['Date'] = pd.to_datetime(assets_quote_sql['Date'])

# Cambiamos el formato de la columna 'Date' a fecha para que sea compatible con los demas en el merge

assets_quote_sql = assets_quote_sql.set_index('Date')

assets_quote_sql.head()

# Comprobamos que efectivamente el índice es tipo fecha

print(type(assets_quote_sql.index))

"""###<u>Integración</u>

Antes de realizar la integración, dado que esta va a ser horizontal y no vertical, vamos a visualizar el número de filas por cada uno de los 4 dataframes.
"""

print("Número de filas en quote_tech_sql:", quote_tech_sql.shape[0])
print("Número de filas en macro_monthly_sql:", macro_monthly_sql.shape[0])
print("Número de filas en macro_daily_sql:", macro_daily_sql.shape[0])
print("Número de filas en assets_quote_sql:", assets_quote_sql.shape[0])

"""Para hacer la integración horizontal vamos a unir primero los 3 dataframes con más filas, ya que al emplear la función merge sin el parámetro 'how='outer'' solo se van a unir las filas cuyos índices coincidan con todos los índices de los dataframes a unir. Si una fila tiene un índice que no coincide con almenos uno de los dataframes, esta será eliminada.

Como se puede observar abajo, el dataframe resultante de la unión tiene 2703 filas, las mismas que los datos de cotización del NASDAQ, mientras que el excedente en macro_daily_sql ha sido eliminado.
"""

# Utilizamos left_index=True y right_index=True para que se unan por los índices

nasdaq_df = pd.merge(quote_tech_sql, macro_daily_sql, left_index=True, right_index=True)
nasdaq_df = pd.merge(nasdaq_df, assets_quote_sql, left_index=True, right_index=True)

nasdaq_df

"""Para la unión del dataframe con macro_monthly_sql tenemos que usar el parámetro 'how='outer'' para que asigne un valor nulo a todas aquellas filas cuyo índice no coincida con el otro índice. Si no utilizaramos este parámetro, nos quedaría un dataframe de como máximo 130 filas.

El resultado es un dataframe de 2749 filas. Este aumento se debe a que en los índices mensuales hay valores en días (probablemente festivos o fines de semana) en los cuales el NASDAQ no está cotizando.
"""

# Utilizamos how='outer' para que los índices no coincidentes se rellenen con valores nulos

nasdaq_df = pd.merge(nasdaq_df, macro_monthly_sql, left_index=True, right_index=True, how='outer')

nasdaq_df

"""###<u>Gestión de Valores Nulos</u>

Primero hacemos un recuento del número total de valores nulos por cada columna.
"""

nasdaq_df.isnull().sum()

"""Las variables macroeconómicas mensuales son las que tienen más nulos. Esto se debe a que solo tienen un valor por mes. Vamos a rellenar el resto de días del mes poniendo el mismo valor que el primer día del mes, ya que el dato que sale el primer día es el que tienen en cuenta los inversores durante todo el mes."""

monthly = ['unemployment', 'inflation', 'consumer expectation', 'industrial production']

for col in monthly:

    # Rellenar los valores nulos utilizando el método "forward fill" (ffill)

    nasdaq_df[col] = nasdaq_df[col].fillna(method='ffill')

    print('Nulos en', col, ':', nasdaq_df[col].isna().sum())

"""En las variables de indicadores técnicos la mayoría de nulos están en las primeras filas. Esto se debe a que en los primeros días no hay suficientes datos para carlcularlas. Por ejemplo, la media móvil de 200 no estará disponible hasta después de las primeras 200 sesiones. Por ello, vamos a eliminar las primeras filas."""

nasdaq_df = nasdaq_df.iloc[203:]

nasdaq_df.isnull().sum()

"""La mayoría de variables tienen ahora 42 nulos. Esto se debe probablemente a la fusión con el dataframe macro_monthly. Vamos a eliminar las filas en las que haya nulos en la variable 'Close'."""

nasdaq_df = nasdaq_df.dropna(subset=['Close'])

nasdaq_df.isnull().sum()

nasdaq_df

"""Tras estas modificaciones el set de datos ha perdido más de 200 registros. Para evitar perder aun más información vamos a optar por rellenar los datos faltantes en vez de eliminarlos, ya que esto supondría perder cientos de filas que serán necesarias a la hora de entrenar los modelos.

El método escogido para ello es interpolate de la librería pandas. La función interpolate con el método 'time' en pandas se utiliza para realizar una interpolación basada en el tiempo en una serie temporal o un DataFrame que contiene datos temporales, como el nuestro que contiene series temporales financieras. Esta técnica es especialmente útil cuando se trata de rellenar valores nulos en series temporales en las que los datos están relacionados con el tiempo.

El método 'time' funciona de la siguiente manera:

1. Examina los datos en la serie temporal en busca de valores nulos.

2. Cuando encuentra un valor nulo, busca los puntos de datos más cercanos en el tiempo que tengan valores no nulos antes y después del valor nulo.

3. Utiliza una interpolación lineal para estimar el valor faltante en función de los valores no nulos circundantes y su ubicación temporal relativa.

4. Rellena el valor nulo con la estimación calculada utilizando la interpolación lineal.

5. Repite este proceso para todos los valores nulos en la serie temporal.

Documentación Pandas(interpolate): https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html
"""

import pandas as pd

# Utilizamos la función interpolate con el método 'time' para rellenar los valores nulos basados en el tiempo

nasdaq_inter = nasdaq_df.interpolate(method='time')

print(nasdaq_inter.isnull().sum())

"""Finalmente, hemos conseguido conservar el número de filas a la vez que eliminar todos los valores nulos."""

nasdaq_inter.shape

"""Mostramos el resultado final por pantalla reseteando el índice para no perder las fechas al subir el dataframe a la base de datos SQL"""

nasdaq_inter.reset_index(inplace=True)

nasdaq_inter

"""Por último, creamos una tabla en la base de datos y subimos el dataframe para hacer un check point. Podemos reciclar el código usado en la extracción de datos para ello."""

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

def crear_tabla_desde_dataframe(df, tabla, connection):
    cursor = connection.cursor()

    create_table_query = f"CREATE TABLE {tabla} (id INT AUTO_INCREMENT PRIMARY KEY"

    for columna, tipo in zip(df.columns, df.dtypes):
        if tipo == "float64":
            create_table_query += f", `{columna}` FLOAT"
        elif tipo == "int64":
            create_table_query += f", `{columna}` INT"
        elif tipo == "datetime64[ns]":
            create_table_query += f", `{columna}` DATE"
        elif tipo == "bool":
            create_table_query += f", `{columna}` BOOLEAN"

    create_table_query += ");"
    cursor.execute(create_table_query)

# Llamada a la función con las listas anteriores
crear_tabla_desde_dataframe(nasdaq_inter, 'nasdaq', connection)

from sqlalchemy import create_engine

# Configuración de la conexión a la base de datos MySQL con las credenciales y el host

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

# if_exists='replace' hace que, si existe la base de datos, se reemplace con los nuevos datos

def cargar_dataframe_en_tabla(df, tabla):
    df.to_sql(tabla, con = engine, if_exists='replace', index=False)

# index=False hace que no se tenga en cuenta el índice, ya que este figura como una columna más

cargar_dataframe_en_tabla(nasdaq_inter, 'nasdaq')

# Se cierra la conexión

connection.close()
engine.dispose()

"""# **Análisis Exploratorio de Datos**

Antes de proceder con la ingeniería de características, vamos a realizar un análisis exploratorio de datos (EDA) para poder conocer más en profundidad las variables y realizar algunas preguntas a los datos.

Para no tener que ejecutar de nuevo todo el código anterior, vamos a importar la tabla 'nasdaq' de nuestra base de datos persistente.

###<u>Importación de datos SQL</u>
"""

!pip install protobuf==3.20.3 mysql-connector-python > /dev/null
import mysql.connector

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

import pandas as pd
from sqlalchemy import create_engine, text

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

conn = engine.connect()

# Importamos la tabla de SQL y la convertimos en un DataFrame

query = text(f'SELECT * FROM nasdaq')
nasdaq_sql = pd.read_sql(query, con=conn)

# Cerrar la conexión correctamente

conn.close()
connection.close()

# Volvemos a fijar la fecha como índice

nasdaq = nasdaq_sql.set_index('Date')

"""###<u>Resumen Estadístico Inicial</u>

Con el atributo shape recuperamos el número de filas y columnas del dataframe, 2504 y 46 respectivamente.
"""

nasdaq.shape

"""Visualizamos todas las variables del dataframe."""

nasdaq.columns

"""Todas estas variables son de tipo float, incluyendo las variables binarias de soportes y resistencias. Podríamos cambiar el tipo de estas columnas a boolean ya que esto reflejaría mejor la naturaleza binaria de las variables y ocuparía menos espacio en memoria. Sin embargo, cambiarlas a boolean dificultará el análisis de estas variables en el EDA, por lo que por ahora las vamos a dejar como float."""

nasdaq.dtypes

"""Cuando imprimimos un dataframe con muchas columnas, muchas salen ocultas para poder abreviarlo. Vamos a subir el límite para la abreviación a 50 columnas para poder visualizar todas las columnas de nuestro dataframe y las primeras 10 filas."""

# Con esta linea podemos hacer que salgan todas las columnas por pantalla, hasta un máximo de 50 columnas
pd.set_option('display.max_columns', 50)

nasdaq.head(10)

"""Comprobamos si hay algún valor de la variable 'Close' que esté repetido varias verces. Esto podria darnos indicios de que hay un error."""

# Número de veces que se repite cada valor

nasdaq['Close'].value_counts()

"""El mayor número de valores repetidos es 2, por lo que parece que no hay ninguna fila duplicada. Podemos terminar de asegurarnos de esto con la siguiente línea de código."""

# Comprobar si hay filas duplicadas

nasdaq.loc[nasdaq.duplicated()]

"""Este estudio se centra en el signo de la diferencia porcentual entre el precio de cierre y precio de apertura del día siguiente. Creamos en este punto la variable objetivo para poder analizarla en el EDA.

El motivo por el cual no se utiliza la diferencia entre el precio de cierre actual y el precio de cierre del día anterior es que queremos comprobar la viabilidad de una estrategia que consistiera que comprar todos los días cuando abre la sesión y vender cuando cierra. Comprar al precio de cierre no es factible en la realidad porque cuando este se sabe el mercado ya está cerrado y solo es posible comprar al día siguiente al precio de apertura. Podría utilizarse una estrategia consistente en comprar al precio de apertura solo si este es igual o inferior al precio de cierre del día anterior, pero esto limitaría mucho los días de operativa.

La variable return mide la diferencia en términos porcentuales entre el precio de apertura y el precio de cierre. No se usan términos absolutos ya que estás diferencias van en aumento junto con el aumento del valor del NASDAQ, por lo que si queremos analizar esta variable en un período de 10 años de manera equitativa, esta es una solución.

La variable target_return toma el valor 1 si return es positivo y 0 si es negativo. Se rueda una posición abajo para indicar 1 si la rentabilidad del día siguiente es positiva y 0 si es negativa.
"""

# Creamos la variable objetivo rodándola una posición.

nasdaq['return'] = (nasdaq['Close'] - nasdaq['Open'])/nasdaq['Open']

nasdaq['target_return'] = nasdaq['return'].apply(lambda x: 1 if x > 0 else 0)

nasdaq['target_return'] = nasdaq['target_return'].shift(-1)

nasdaq.dropna(inplace=True)

nasdaq

"""Imprimimos un dataframe con las estadísticas básicas de cada variable con el método describe. Entre ellas encontramos el número total de valores, la media, la desviación estándar, los mínimos y máximos y los 3 cuartiles.

1. Se puede observar que ciertos indicadores técnicos, como el RSI o el estocástico, están acotados entre 0 y 100.

2. De la media de las variables dummy_support y dummy_resistance obtenemos que un 17.30% de las veces el precio del NASDAQ cierra en una zona de soporte, mientras que un 17.62% lo hace en una zona de resistencia.

3. El precio del futuro del barril de crudo ha llegado a estar en negativo. Esto probablemente haya ocurrido en marzo de 2020, durante la pandemia.

4. El cuartil del 75% de la volatilidad está en 21.30, mientras que el máximo que ha llegado a alcanzar es 82.69, indicándonos que probablemente esta variable haya experimentado un gran valor atípico.

5. La media del desempleo y la inflación durante los últimos 10 años en Estados Unidos ha sido del 5.06% y 2.81% respectivamente. Estás variables también han experimentado picos, dado que sus cuartiles 75% son 5.80% y 2.64% pero sus máximos son 14.70% y 6.62%.

6. La rentabilidad diaria media del NASDAQ es de un 0.03%. La mayoría de las veces ha estado entre un -0.44% y un 0.57%, aunque ha alcanzado un máximo de 7.04% y un mínimo de -6.60%. Un 54.29% de las veces el precio ha cerrado por encima de su apertura diaria.

En general, en la mayoría de las variables la distancia entre máximos y mínimos y los cuartiles sugieren la existencia de valores atípicos. En la siguiente sección se hará un análisis en mayor profundidad de estos.
"""

nasdaq.describe()

"""Ahora visualizamos el conteo de la variable target_return con las librerías matplotlib y seaborn.

Documentación de la librería Matplotlib: https://matplotlib.org/stable/index.html

Documentación de la librería Seaborn: https://seaborn.pydata.org/
"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style="whitegrid")
plt.figure(figsize=(8, 6))

sns.countplot(x='target_return', data=nasdaq, palette='Set2')

plt.title('Total de velas positivas y negativas')
plt.ylabel('Conteo')

plt.show()

"""Para ver la proporción la mejor opción es utilizar un pie chart."""

# Calculamos el recuento de cada valor en 'target_return'
counts = nasdaq['target_return'].value_counts()

# Creamos un pie chart en verde para las velas positivas y rojo para las negativas
plt.figure(figsize=(6, 6))
plt.pie(counts, labels=['Velas Positivas', 'Velas Negativas'],
        autopct='%1.1f%%', startangle=140, colors=['#58FA82', '#FF9999'])

plt.title('Porcentaje de velas positivas y negativas')

plt.show()

"""###<u>Valores Atípicos</u>

Para corroborar la existencia de los posibles valores atípicos que hemos observado en el resumen estadístico vamos a obtener un Box Plot de cada una de las variables float del dataframe.
"""

# Crear una matriz con múltiples subplots de 9x5
fig, axes = plt.subplots(nrows=9, ncols=5, figsize=(18, 25))

# Aplanar la matriz de subplots para recorrerla fácilmente
axes = axes.flatten()

# Eliminamos las variables dummies
del_cols = ['target_return', 'dummy_resistance', 'dummy_support']

nasdaq_boxplot = nasdaq.drop(del_cols, axis=1)

# Bucle for para generar un boxplot para cada variable
for i, col in enumerate(nasdaq_boxplot.columns):
    ax = axes[i]
    nasdaq_boxplot.boxplot(column=col, ax=ax)
    ax.set_xlabel('')
    ax.set_ylabel('')

# Ajustar el espacio entre los subplots
plt.tight_layout()

plt.show()

"""Usando la definición de valor atípico basada en el rango intercuartílico, podemos confirmar la abundancia de estos valores en varias variables, tal y como se muestra en la tabla de abajo. Se ha tomado la decisión de no eliminar los valores atípicos por los siguientes motivos:

1. La eliminación de estos valores atípicos supondría una pérdida de información demasiado grande, ya que habría que eliminar cientos de registros.

2. Estos valores atípicos no parecen deberse a errores de medición.

3. Los valores atípicos pueden contener información valiosa sobre eventos inusuales o eventos extremos en los datos financieros. Estos eventos pueden ser de interés, ya que podrían indicar cambios significativos en el mercado, noticias importantes o eventos inesperados que pueden tener un impacto en nuestra estrategia de inversión.

4. Los valores atípicos son parte de la naturaleza de los datos financieros. Los mercados financieros son volátiles y están sujetos a cambios rápidos e inesperados. Eliminar todos los valores atípicos podría conducir a una representación irrealista de los datos y ocultar la verdadera naturaleza del mercado.

5. En algunos casos, los modelos financieros pueden beneficiarse de la inclusión de valores atípicos. Los modelos robustos pueden ser capaces de manejar valores atípicos sin que esto afecte significativamente sus predicciones. Además, los valores atípicos pueden ayudar a detectar cambios estructurales en los datos.

6. La eliminación de valores atípicos puede distorsionar las métricas y los indicadores que usaremos para los modelos.
"""

# Crear una función para contar outliers basados en el rango intercuartil (IQR)
def count_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = (series < lower_bound) | (series > upper_bound)
    return outliers.sum()

# Calcular el número de outliers por cada variable en el DataFrame
outliers_count = nasdaq_boxplot.apply(count_outliers)

outliers_matrix = pd.DataFrame({'Número de Outliers': outliers_count})


print(outliers_matrix)

"""Habiendo decidido no eliminar los valores atípicos directamente, será necesario emplear un método de normalización que sea robusto frente a valores atípicos. También se tratará de mitigar parcialmente los efectos de los valores atípicos, sin proceder a su eliminación directa, mediante técnicas de reducción de ruido en ingeniería de características.

###<u>Normalidad</u>

Para analizar la normalidad de las variables lo primero que vamos a hacer es graficar un histograma de cada una de las variables indicando su curtosis y asimetría en la leyenda.

A primera vista, parece que abundan las variables que no siguen una distribución normal. Entre las que sí aparentan una distribución normal podemos encontrar las 3 variables del indicador técnico MACD, el indicador técnico Momentum y la rentabilidad diaria del NASDAQ. Cabe destacar que esta última es la que más se asemeja a una Campana de Gauss, con (aproximadamente) media en 0 y desviación estandar en 0.1. La curtosis y asimetría están próximas al 3 y 0 respectivamente, indicando una campana mesocúrtica y simétrica respecto a su media típica de una distribución normal estándar.
"""

# Crear una figura con múltiples subplots
fig, axes = plt.subplots(nrows=16, ncols=3, figsize=(18, 60))

# Aplanar la matriz de subplots para recorrerla fácilmente
axes = axes.flatten()

# Bucle para generar histogramas con kurtosis y skewness para cada variable
for i, col in enumerate(nasdaq.columns):
    ax = axes[i]
    data = nasdaq[col]
    kurtosis = data.kurtosis()
    skewness = data.skew()

    sns.histplot(data, kde=True, color='blue', ax=ax)
    ax.legend([f'Kurtosis: {kurtosis:.2f}\nSkewness: {skewness:.2f}'],
               fontsize=16, facecolor='lightgray', edgecolor='black', framealpha=1)
    ax.set_title(col)
    ax.set_xlabel('')
    ax.set_ylabel('')

# Ajustar el espacio entre los subplots
plt.tight_layout()


plt.show()

"""Para visualizar la rentabilidad diaria del NASDAQ (la variable return) más de cerca vamos a graficar un histograma añadiendo la media y las desviaciones estándar para analizar su posible normalidad.

Asumiendo que esta frecuecia sigue una distribución normal, un 68% de los valores deberían encontrarse dentro de una desviación estándar, y un 95% debería estar dentro de dos desviaciones estándar. A simple vista esta condición parece cumplirse, pero habrá que realizar más pruebas antes de confirmar la normalidad de su distribución.

Documentación sobre norm de scipy.stats: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html

Documentación de NumPy: https://numpy.org/doc/
"""

from scipy.stats import norm
import numpy as np
plt.style.use('ggplot')

plt.figure(figsize=(8, 6))

# Plotear el histograma de 'return'
sns.histplot(nasdaq['return'], kde=True, color='blue', element='step', lw=1)

# Calcular la media y la desviación estándar de 'return'
media = nasdaq['return'].mean()
desviacion_estandar = nasdaq['return'].std()

# Crear un array de valores X para la campana de Gauss
x = np.linspace(nasdaq['return'].min(), nasdaq['return'].max(), 1000)
# Calcular los valores Y de la campana de Gauss usando la media y la desviación estándar
y = norm.pdf(x, media, desviacion_estandar)

# Plotear la campana de Gauss en el mismo gráfico
plt.plot(x, y)
plt.axvline(media, color='red', linestyle='dashed', linewidth=2, label='Media')
plt.axvline(media + desviacion_estandar, color='green', linestyle='dashed', linewidth=2, label='1 Desviación Estándar')
plt.axvline(media - desviacion_estandar, color='green', linestyle='dashed', linewidth=2)
plt.axvline(media + 2 * desviacion_estandar, color='purple', linestyle='dashed', linewidth=2, label='2 Desviaciones Estándar')
plt.axvline(media - 2 * desviacion_estandar, color='purple', linestyle='dashed', linewidth=2)
plt.axvline(media + 3 * desviacion_estandar, color='orange', linestyle='dashed', linewidth=2, label='3 Desviaciones Estándar')
plt.axvline(media - 3 * desviacion_estandar, color='orange', linestyle='dashed', linewidth=2)

plt.title('Distribución normal del rendimiento diario del NASDAQ')
plt.xlabel('return')
plt.legend()

plt.show()

"""Para continuar con el análisis de la normalidad, vamos a visualizar un Q-Q plot. En este gráfico se vuelve a confirmar la no-normalidad de la distribución de la muchas de las variables. En muchas variables se observan desviaciones sustanciales de la recta diagonal, rechanzando la hipótesis nula de similitud con una distribución normal.

Documentación de la librería Statsmodels: https://www.statsmodels.org/stable/index.html
"""

import statsmodels.api as sm
plt.style.use('default')

fig, axes = plt.subplots(nrows=12, ncols=4, figsize=(18, 30))

axes = axes.flatten()

# Bucle para generar QQ plots para cada variable
for i, col in enumerate(nasdaq.columns):
    ax = axes[i]
    data = nasdaq[col]
    sm.qqplot(data, line='s', ax=ax)
    ax.set_title(col)

plt.tight_layout()

plt.show()

"""Finalmente, realizamos la prueba de Shapiro-Wilk para determinar la normalidad de las variables.

La prueba de Shapiro-Wilk es una de las pruebas más poderosas para verificar la normalidad de una muestra. Es ampliamente recomendada debido a su mayor poder para detectar desviaciones de la normalidad en comparación con otros tests de normalidad.

La hipótesis nula (H0) es que los datos se distribuyen normalmente. La hipótesis alternativa (H1) es que los datos no se distribuyen normalmente. La prueba calcula una estadística de prueba (W) y un valor p.

Si el valor p es menor que un nivel de significancia elegido (en este caso hemos escogido 0.05, pero los resultados son los mismos para 0.01), se rechaza la hipótesis nula, lo que indica que los datos no siguen una distribución normal.

Como se puede observar en el siguiente dataframe, ninguno de los valores p supera el umbral de significancia establecido, por lo que en todas las variables se ha de rechazar la hipótesis nula de distribución normal.

Documentación de la librería Scipy: https://docs.scipy.org/doc/scipy/
"""

from scipy import stats

shapiro_wilk_results = []

# Bucle para realizar la prueba de Shapiro-Wilk para cada variable
for col in nasdaq.columns:
    data = nasdaq[col]
    statistic, p_value = stats.shapiro(data)
    is_normal = 'Si' if p_value > 0.05 else 'No'
    shapiro_wilk_results.append((col, statistic, p_value, is_normal))

shapiro_wilk_df = pd.DataFrame(shapiro_wilk_results, columns=['Variable', 'Statistic', 'p-value', 'Distribución Normal'])

shapiro_wilk_df.set_index('Variable', inplace=True)

shapiro_wilk_df

"""Habiendo confirmado la evidente abundancia de variables con distribuciones no-normales tras la realización de tres pruebas distintas, será necesario emplear algoritmos robustos a la no-normalidad y no aplicar métodos de normalización en la fase de preprocesamiento que asuman la distribución normal de las variables.

###<u>Correlación</u>

Medir la correlación es un paso útil para explorar la relación entre las variables y la variable objetivo, pero no debe considerarse como la única medida de capacidad predictiva, ya que correlación no necesariamente implica capacidad predictiva ni causalidad. Imprimimos una matriz de correlación para ver como se relacionan unas variables con otras.
"""

correlation_matrix = nasdaq.corr()

correlation_matrix

"""Para visualizar mejor la matriz de correlación imprimimos un mapa de calor."""

plt.figure(figsize=(50, 35))

# annot=True para que muestre las etiquetas
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", annot_kws={"size": 17})

plt.xticks(fontsize=24)
plt.yticks(fontsize=24)

plt.show()

"""A continuación vamos a filtrar los 15 pares de variables más y menos correlacionados. Estas son algunas de las conclusiones que podemos sacar de las listas y del mapa de calor:

1. En general, y como era de esperar, las variables OHCL y las medias móviles guardan una alta correlación entre sí.

2. A excepción del Hang Seng Index, los demás índices están altamente correlacionados con las variables OHCL del NASDAQ.

3. El futuro del oro muestra una fuerte correlación positiva con el NASDAQ, mientras que el futuro del barril de petróleo muestra una correlación muy tenue.

3. 10y-2y yield curve y federal interest rate tiene una fuerte correlación negativa ya que cuanto mas suben los tipos de interés del banco central, mayor es la rentabilidad del bono a 2 años frente al bono a 10 años. Esto se debe a que hay una correlación negativa entre la rentabilidad de los bonos y los tipos de interés y a que, por lo general, los bonos de mayor duración son más sensibles a las variaciones en los tipos de interés.

4. Un factor llamativo es que tanto la volatilidad como la prima de riesgo del bono basura son medidas del miedo que tiene el mercado en un determinado momento. Sin embargo, no hay una correlación muy fuerte entre ambas variables.

5. Tanto los tipos de interés del banco central como el tipo de interés del bono a 5 años muestran una correlación positiva con la inflación, y negativa con el desempleo. Esto se debe a que los bancos centrales suelen subir los tipos de interés para contrarrestar las subidas de la inflación, mientras que los suelen bajar como respuesta a datos negativos en el mercado de trabajo.

6. La producción industrial y el desempleo muestran la correlación más negativa de toda la matriz. Esto es obvio, ya que a menor producción del tejido industrial, mayor será el desempleo.

7. Las expectativas del consumidor están negativamente correlacionadas con el oro, volumen, NASDAQ, S&P500 e inflación. Esta última puede deberse a que un entorno inflacionario es percibido como perjuicial para la economía. El resto pueden explicarse debido a la naturaleza de indicador adelantado de la expectativas del consumidor, al igual que ocurre con el indicador 10y-2y yield curve.
"""

# Obtener las correlaciones en orden ascendente (menos correlacionadas)
sorted_correlations_asc = correlation_matrix.unstack().sort_values(ascending=True)

# Obtener las correlaciones en orden descendente (más correlacionadas)
sorted_correlations_desc = correlation_matrix.unstack().sort_values(ascending=False)

# Filtrar las correlaciones de variables con sí mismas (diagonales)
sorted_correlations_asc = sorted_correlations_asc[sorted_correlations_asc.index.get_level_values(0) != sorted_correlations_asc.index.get_level_values(1)]
sorted_correlations_desc = sorted_correlations_desc[sorted_correlations_desc.index.get_level_values(0) != sorted_correlations_desc.index.get_level_values(1)]

# Obtener los 30 pares más y menos correlacionados
top_30_least_correlated = sorted_correlations_asc.head(30)
top_30_most_correlated = sorted_correlations_desc.head(30)

# Crear DataFrames para los pares de variables
df_least_correlated = pd.DataFrame(top_30_least_correlated, columns=['Correlation'])
df_most_correlated = pd.DataFrame(top_30_most_correlated, columns=['Correlation'])

# Eliminar duplicados solo en función del campo 'Correlation' en los DataFrames
df_least_correlated = df_least_correlated.drop_duplicates(subset='Correlation')
df_most_correlated = df_most_correlated.drop_duplicates(subset='Correlation')

print("15 pares de variables menos correlacionados:")
df_least_correlated

print("15 pares de variables más correlacionados:")
df_most_correlated

"""Utilizando diagramas de disperión podemos ver como han ido evolucionando las relaciones entre variables con el tiempo. De este diagrama de dispersión que muestra la correlación entre el NASDAQ y el S&P500 sacamos dos importantes conclusiones:

1. Los precios de cierre del NASDAQ y del S&P500 muestran una correlación positiva casi perfecta.
2. Los precios han ido en aumento desde 2013 hasta 2023, ya que la mayoría de puntos oscuros están arriba a la derecha y los claros están abajo a la izquierda.
"""

plt.style.use('ggplot')

def temporal_scatterplot(x_var, y_var):
  # Crear un gráfico de dispersión con la fecha como variable de color
  sns.scatterplot(x=x_var, y=y_var, hue=nasdaq.index.year, data=nasdaq)

  plt.title(f'Correlación de {x_var} vs. {y_var}')

  plt.xlabel(x_var)
  plt.ylabel(y_var)

  return plt.show()

temporal_scatterplot('Close', 'S&P500')

"""Las expectativas del consumidor suelen adelantarse a cambios de tendencia en el NASDAQ. Cuando este indicador está bajando el volumen de negociación sigue en aumento debido a que aún no ha llegado al techo. Cuando el precio empieza a caer y el volumen de negociación baja las expectativas del consumidor suben augurando que el final de la recesión tendrá lugar en el futuro cercano.

Como se puede observar, el volumen de negociación se ha ido incrementando con los años, mientras que las expectativas del consumidor han ido decreciendo desde 2018 hasta 2022.
"""

temporal_scatterplot('consumer expectation', 'Volume')

"""Otra correlación negativa entre un indicador económico adelantado al ciclo y un índice representativo de la economía americana.

El DJIA ha ido en aumento desde 2014 a 2022, mientras que la diferencia entre la rentabilidad del bono a 10 y 2 años ha estado en negativo durante todo el 2022, augurando la proximidad de una posible recesión económica.
"""

temporal_scatterplot('10y-2y yield curve', 'Dow Jones Industrial Average')

"""La bolsa londinense no muestra un crecimiento tan claro como las americanas. Por otro lado, el spread de los bonos basura alcanzaró máximos en 2020, señalando el pico de pánico alcanzado durante la pandemia.

De la correlación negativa se puede deducir que la bolsa londinense no responde bien en situaciones de estrés financiero.
"""

temporal_scatterplot('junk bonds spread', 'FTSE 100')

"""Por último vamos a analizar con un gráfico de barras las correlaciones de todas las variables frente a 3 variables: close, return y target_return.

La primera es el close, que muestra una alta correlación con muchas variables.
"""

def correlation_sorted(var):
  # Calcular las correlaciones
  correlations = nasdaq.corr()[var]

  # Eliminar la correlación de la variable consigo misma
  correlations = correlations.drop(var)

  # Crear un DataFrame para facilitar el ordenamiento
  correlation_df = pd.DataFrame({'Variable': correlations.index, 'Correlación': correlations.values})

  # Ordenar las correlaciones en orden descendente
  correlation_df = correlation_df.sort_values(by='Correlación', ascending=False)

  plt.figure(figsize=(12, 10))
  # Asegurar que los nombres de las variables están separadas del gráfico
  sns.set(rc={'figure.autolayout': True})
  sns.barplot(x='Correlación', y='Variable', data=correlation_df, palette='Blues_d', orient='h', errorbar=None, dodge=False, saturation=1.0)
  plt.title(f'Correlación con {var}')
  plt.xlabel('Correlación')
  plt.ylabel('Variables')

  plt.show()

correlation_sorted('Close')

"""En cuanto a la rentabilidad diaria, se puede observar que la intensidad de la correlación es mucho menor en todas las variables."""

correlation_sorted('return')

"""Si nos vamos a la variable objetivo, el resto de variables no parecen guardar una correlación sigfnificativa, pero como ya hemos dicho, correlación no necesariamente indica capacidad predictiva, por lo que no vamos a descartar ninguna variable por ahora."""

correlation_sorted('target_return')

"""La conclusión que podemos extraer de este último gráfico es la dificultad del problema que estamos tratando de resolver, ya que no existe una alta correlación entre la variable objetivo y ninguna de las variables.

###<u>Heterocedasticidad</u>

Como análisis complementario vamos a testear la la varianza de los errores de un modelo de regresión. El objetivo de este proyecto es resolver un problema de clasificación, pero como muestra para futuros trabajos sobre la materia vamos a comprobar la complejidad y dificultad que supondría realizar un modelo de regresión en vez de clasificación sobre estos datos.

La prueba de Breusch-Pagan, es una prueba estadística utilizada para evaluar la presencia de heterocedasticidad en un modelo de regresión. La heterocedasticidad se refiere a la variabilidad no constante de los errores en un modelo de regresión, lo que significa que la varianza de los errores no es constante a lo largo de los valores de las variables independientes. Esto puede ser problemático ya que viola una de las suposiciones clave de muchos modelos de regresión lineal, que asume una varianza constante de los errores (homoscedasticidad).

En caso de detectar la presencia de heterocedasticidad sería necesario utilizar algoritmos de regresión más sofisticados y capaces de capturar relaciones no lineales y patrones más complejos en los datos.
"""

from statsmodels.stats.diagnostic import het_breuschpagan

# Ajustamos el modelo de regresión lineal
X = nasdaq.drop(columns=['return'])
X = sm.add_constant(X)  # Añadimos una constante (intercepto) al modelo
y = nasdaq['return']

# Entrenamos el modelo
model = sm.OLS(y, X).fit()

# Calculamos los residuos del modelo
residuals = model.resid

# Realizamos la prueba de Breusch-Pagan
bp_test = het_breuschpagan(residuals, X)

# Obtienemos los valores de estadísticas de la prueba
lm_statistic = bp_test[0]
lm_p_value = bp_test[1]
f_statistic = bp_test[2]
f_p_value = bp_test[3]

print(f"Estadística LM: {lm_statistic}")
print(f"Valor p LM: {lm_p_value}")
print(f"Estadística F: {f_statistic}")
print(f"Valor p F: {f_p_value}")

# Comprobamos la significación (5%) estadística del resultado
alpha = 0.05
if lm_p_value < alpha or f_p_value < alpha:
    print("Se rechaza la hipótesis nula de homocedasticidad: Hay evidencia de heterocedasticidad.")
else:
    print("No se rechaza la hipótesis nula: No hay evidencia de heterocedasticidad.")

"""Ambos estadíticos son inferiores al nivel de significación de 5%, por lo que se rechaza la homocedasticidad como hipótesis nula. Esto es una muestra más de la compleja naturaleza de los datos que estamos tratando y la dificultad de su predicción tanto en algoritmos de clasificación (demostrado en el resto de apartados) como en algoritmos de regresión.

###<u>Interdependencia Temporal</u>

Para medir la interdependencia temporal de las variables vamos a utilizar gráficos para mostrar la autocorrelación del lag 0 al lag 30. La autocorrelación es una medida estadística que evalúa la relación lineal entre una serie de datos y una versión retrasada de sí misma. Conceptualmente su objetivo es comprobar si el presente y futuro dependen del pasado. Primero mostramos la autocorrelación del precio de cierre, para posteriormente mostrar la de la rentabilidad diaria y finalmente la del signo de esta rentabilidad (variable objetivo).

Autocorrelación del precio de cierre:
"""

plt.style.use('ggplot')

def autocorr(var, lags):
  # transformada rápida de Fourier activada
  acf_result = sm.tsa.acf(nasdaq[var], nlags=lags, fft=True)

  plt.bar(range(len(acf_result)), acf_result, width=0.2)

  plt.title(f'Gráfico de Autocorrelación para {var}')
  plt.xlabel('Lag')
  plt.ylabel('Coeficiente de Autocorrelación')
  plt.grid(True)

  plt.show()

autocorr('Close', 30)

"""Autocorrelación de la rentabilidad diaria:"""

autocorr('return', 30)

"""Autocorrelación del signo de la rentabilidad diaria:"""

autocorr('target_return', 30)

"""La autocorrelación es muy pronunciada en la variable 'Close', pero parece no ser significativa para las otras dos variables.

Para poder analizar la autocorrelación de la variable objetivo con mayor claridad vamos a realizar la Prueba de Ljung-Box. Esta prueba se utiliza para evaluar si hay autocorrelación significativa en diferentes rezagos (lags) de una serie temporal. La hipótesis nula establece que los datos se distribuyen de forma independiente (es decir, las correlaciones en la población de la que se toma la muestra son 0, de modo que cualquier correlación observada en los datos es el resultado de la aleatoriedad del proceso de muestreo). La interpretación se basa en el p-valor (P-Value). Si el p-valor es menor que un umbral de significación (en este caso 0.05), se puede rechazar la hipótesis nula y por lo tanto concluir que hay evidencia de autocorrelación en la serie temporal.
"""

from statsmodels.stats.diagnostic import acorr_ljungbox

y = nasdaq['target_return']

# Realizamos la prueba de Ljung-Box para 30 lags
ljungbox = acorr_ljungbox(y, lags=30)

ljungbox['Autocorrelación'] = ['Sí' if p < 0.05 else 'No' for p in ljungbox['lb_pvalue']]

ljungbox

"""Como se puede observar, de acuerdo con la Prueba de Ljung-Box solo hay evidencia de autocorrelación en el primer lag. Esto prueba que existe autocorrelación de primer orden (lag 1) y que por tanto sería interesante entrenar un modelo capaz de captar patrones intertemporales en los datos.

###<u>Separabilidad Lineal</u>

Dado que queremos resolver un problema de clasificación, vamos a comprobar si los datos son linealmente separables mediante la evaluación de una Support Vector Machine de márgen duro.

El término "márgen duro" o "Hard Margin" se refiere a la naturaleza estricta de esta SVM, que busca encontrar un hiperplano de separación que separe perfectamente las muestras de dos clases en un espacio de características. En otras palabras, busca un hiperplano que tenga un margen máximo (la distancia entre el hiperplano y el punto de datos más cercano) y que no permita ningún punto de datos en el margen.

La capacidad de la SVM para encontrar un hiperplano de separación perfecta o casi perfecta puede indicar si las clases (en este caso el signo de la rentabilidad del día siguiente) son fácilmente distinguibles en los datos. La Hard Margin SVM asume que los datos son linealmente separables, por lo que unos resultados pobres en la evaluación indicarían que los datos no son linealmente separables y que una Soft Margin SVM o métodos más flexibles de aprendizaje automático, como las redes neuronales, pueden ser más apropiados para resolver este problema de clasificación.

Para entrenar y testear el Hard Margin SVM, le indicamos como hiperparámetro un valor de C próximo al infinito. C es un hiperparámetro que representa la penalización por clasificar incorrectamente un punto. Al indicarle un valor muy alto, el SVM aplicará una penalización más fuerte por los errores de clasificación. En otras palabras, el modelo se esforzará por clasificar correctamente cada punto, incluso si eso significa que el margen entre las clases es más estrecho.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.svm import LinearSVC
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import accuracy_score
# from sklearn.preprocessing import RobustScaler
# 
# # Dividimos en X e Y
# X = nasdaq.drop(columns=['target_return'])
# y = nasdaq['target_return']
# 
# # Dividimos los datos en conjuntos de entrenamiento (70%) y prueba (30%)
# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
# 
# # Escalamos los datos con RobustScaler de scikit-learn
# scaler = RobustScaler()
# train_scaled = scaler.fit_transform(x_train)
# test_scaled = scaler.transform(x_test)
# 
# # Creamos un modelo SVM lineal con margen duro
# svm_classifier = LinearSVC(C=1e10, max_iter=1000000)
# 
# # Entrenamos el modelo con los datos de entrenamiento
# svm_classifier.fit(train_scaled, y_train)
# 
# # Realizamos predicciones en el conjunto de prueba
# y_pred = svm_classifier.predict(test_scaled)
# 
# # Calculamos y mostramos la precisión del modelo
# accuracy = accuracy_score(y_test, y_pred)
# 
# print(f'Precisión del modelo SVM de margen duro: {accuracy:.2f}')

"""Como se puede observar, la precisión arrojada por el modelo no es solo de un 49%, sino que además el modelo no llega a converger con un millón de iteraciones. De estos resultados podemos concluir que el SVM lineal de márgen duro no aporta valor predictivo y que los datos no son linealmente separables (las clases no son perfectamente separables), siendo este un problema de clasificación más complejo y que requerirá de algoritmos más sofisticados para ser resuelto.

###<u>Estacionalidad</u>

Para analizar la presencia de estacionalidad en las variables vamos a graficar las series temporales para visualizar si hay patrones en los datos que se repiten. La estacionariedad es una característica importante en el análisis de series temporales, ya que muchas técnicas y modelos estadísticos asumen que los datos son estacionarios.

El siguiente gráfico de velas muestra las variables OHLC. No parecen haber patrones recurrentes a largo plazo, y la complejidad del gráfico dificulta determinar si existen patrones cíclicos a corto plazo.

Documentación mplfinance: https://pypi.org/project/mplfinance/
"""

!pip install mplfinance > /dev/null
import mplfinance as mpf

# Nos saldrá un aviso de que hay demasiados datos. Para evitarlo le indicamos que este aviso solo salga a partir de las 3000 filas, ya que nosotros contamos con 2503 sesiones.

stock_fig, axes = mpf.plot(nasdaq, type='candle', style='charles', title=f'Gráfico de velas japonesas: NASDAQ', ylabel='Price',
                           figsize=(15, 6), returnfig=True, warn_too_much_data=3000)

"""Suavizamos el gráfico de arriba mostrando las medias móviles para observar mejor las tendencias."""

moving_av = ['SMA_50', 'SMA_200', 'EMA_9', 'EMA_50', 'EMA_200', 'WMA_9', 'WMA_50', 'WMA_200']
plt.style.use('ggplot')

def plot_time_series(variables, title):
    plt.figure(figsize=(12, 6))
    for variable in variables:
        plt.plot(nasdaq.index, nasdaq[variable], label=variable)

    plt.title(title)
    plt.legend(loc='upper left')
    plt.grid(True)
    plt.show()

plot_time_series(moving_av, 'Medias Móviles')

"""Hacemos lo mismo con las Bandas de Bollinger porque también son variables que acompañan al precio visualmente."""

bollinger = ['BBL_20_2.0', 'BBM_20_2.0', 'BBU_20_2.0']

plot_time_series(bollinger, 'Bandas de Bollinger')

"""Ahora visualizamos el resto de variables float en una matriz de gráficos."""

# Utilizamos el estilo 'ggplot' para las gráficas
plt.style.use('ggplot')

graph_vars = ['Volume', 'RSI_14', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9',
              'STOCHk_14_3_3', 'STOCHd_14_3_3', 'MOM_10', 'BBB_20_2.0', 'BBP_20_2.0',
              '10y-2y yield curve', 'volatility', '5-year interest rate',
              'federal interest rate', 'junk bonds spread', 'S&P500', 'Wilshire 5000',
              'Dow Jones Industrial Average', 'Nikkei 225', 'Euro STOXX 50',
              'Hang Seng Index', 'Shanghai Stock Exchange', 'FTSE 100',
              'Gold futures', 'Crude oil futures', 'unemployment', 'inflation',
              'consumer expectation', 'industrial production', 'return']

def graphs(var_list, n_rows, n_cols):
  # Calcular filas y columnas necesarias
  total_plots = len(var_list)
  rows = n_rows
  cols = n_cols

  # Crear una figura con una matriz de gráficos
  fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(15, 2.5 * rows))

  # Recorrer el diccionario y crear los gráficos en la matriz
  for i, variable in enumerate(var_list):
      row_idx = i // cols
      col_idx = i % cols
      ax = axes[row_idx, col_idx]
      nasdaq[variable].plot(ax=ax, title=variable, lw=2)
      ax.set_ylabel(variable)

  # Ajustar el espaciado entre los gráficos y mostrar la figura

  plt.tight_layout()
  return plt.show()

graphs(graph_vars, 10, 3)

"""Debido a la complejidad y a la cantidad ruido y patrones a corto, medio y largo plazo es díficil de determinar visualmente la presencia de estacionalidad en los datos.

Vamos a emplear la Prueba de Dickey-Fuller Aumentada (ADF) para testear la estacionalidad en las variables. La Prueba ADF es una prueba de hipótesis estadística utilizada para determinar si una serie temporal es estacionaria o no. Se basa en la idea de que una serie de tiempo no estacionaria se puede transformar en una serie estacionaria a través de diferenciación. La diferenciación implica calcular la diferencia entre valores sucesivos en la serie de tiempo. La prueba ADF compara una serie original con su versión diferenciada para evaluar si la diferencia es estadísticamente significativa. La hipótesis nula (H0) de la prueba es que la serie de tiempo tiene raíces unitarias, lo que indica no estacionariedad. La hipótesis alternativa (H1) es que la serie de tiempo es estacionaria. Si el p-value es menor o igual a un nivel de significación predefinido (0.05 en este caso), se rechaza la hipótesis nula y se concluye que la serie es estacionaria.


"""

from statsmodels.tsa.stattools import adfuller

# Crear una lista para almacenar los resultados de la prueba de estacionalidad
resultados_estacionalidad = []

for column in nasdaq.columns:
  if column in ['target_return', 'dummy_support', 'dummy_resistance']:
    continue
  # Realizar la Prueba ADF en cada columna
  data = nasdaq[column]
  result = adfuller(data)

  # Agregar los resultados a la lista
  resultados_estacionalidad.append({'Variable': column, 'ADF Statistic': result[0], 'p-value': result[1]})

resultados_adf = pd.DataFrame(resultados_estacionalidad)

resultados_adf['Estacionalidad'] = ['Sí' if p < 0.05 else 'No' for p in resultados_adf['p-value']]

resultados_adf.set_index('Variable', inplace=True)

resultados_adf

"""Como se puede observar, abundan tanto las series estacionarias como no estacionarias. La variable objetivo no es analizada en esta pruebla debido a su naturaleza de variable binaria, pero utilizando la variable return como proxy podemos determinar que las rentabilidades diarias del NASDAQ son estacionarias.

Abajo imprimimos un box plot mensual para observar más de cerca esta variable. Los boxplots mensuales son útiles para visualizar la distribución de los datos en diferentes meses, lo que puede ayudar a detectar patrones estacionales. El número de outliers es similar en todos los meses. Existen ligeros cambios en la mediana y en la distribución de los cuartiles. Visualmente la estacionalidad no es muy pronunciada, pero con la prueba ADF podemos determinar que la variable proxy de la objetivo es estacionaria.
"""

data = nasdaq['return']

# Extraer el mes de la fecha
nasdaq['Month'] = nasdaq.index.month

plt.figure(figsize=(10, 6))
sns.boxplot(x='Month', y='return', data=nasdaq)
plt.title('Boxplot Mensual de Rentabilidad Diaria')
plt.xlabel('Mes')
plt.ylabel('Return')
plt.show()

nasdaq = nasdaq.drop('Month', axis=1)

"""###<u>Métricas de Riesgo y Rentabilidad</u>

Por último, vamos a finalizar el EDA con algunas medidas de riesgo y rentabilidad en los datos. Estas métricas asumen que se compra el activo el día del primer registro y se mantiene en cartera sin venderse durante todo el período de tiempo (estrategia Buy & Hold):

**Value at Risk (VaR):** El Valor en Riesgo es una medida estadística utilizada para estimar las pérdidas potenciales en una inversión o cartera de inversiones en un período de tiempo y con un cierto nivel de confianza. Representa la cantidad máxima de pérdida esperada en un período específico. A un nivel de confianza del 95%, el VaR es de -2.14%. Esto significa que existe un 95% de probabilidad de que las pérdidas no superen el -2.14% de la inversión en un día.
"""

returns = nasdaq['Close'].pct_change().dropna()
alpha = 0.05  # Nivel de confianza del 95%
var = returns.quantile(alpha)
print(f'VaR al {alpha * 100}%: {var:.2%}')

"""**Maximum Drawdown:** Es una medida que cuantifica la mayor pérdida desde el pico más alto de un valor hasta el punto más bajo antes de comenzar a recuperarse. Indica la máxima pérdida experimentada antes de que se produzca una recuperación. En este caso la pérdida es de un -36.40%."""

cumulative_returns = (1 + returns).cumprod()
peak = cumulative_returns.cummax()
drawdown = (cumulative_returns - peak) / peak
max_drawdown = drawdown.min()
print(f'Maximum Drawdown: {max_drawdown:.2%}')

"""**Rendimiento Anual:** Es el porcentaje de ganancia o pérdida experimentado por una inversión o cartera en un año. La rentabilidad anual del NASDAQ desde 2013 hasta 2023 ha sido del 12.87%."""

annual_return = ((1 + returns).prod() ** (1 / len(returns.index.year.unique())) - 1)
print(f'Annual Return: {annual_return:.2%}')

"""**Sharpe Ratio:** Es una medida que evalúa el rendimiento de una inversión en relación con su riesgo. Compara el rendimiento adicional obtenido más allá de una inversión sin riesgo con la volatilidad de la inversión. Como rentabilidad del activo libre de riesgo se ha escogido la rentabilidad del bono americano a 10 años (US10YT) el 13 de octubre de 2023: 4.62%. Como resultado el Sharpe Ratio es 6.25.

Fuente: https://markets.ft.com/data/bonds/tearsheet/summary?s=US10YT
"""

risk_free_rate = 0.0462  # Tasa libre de riesgo
sharpe_ratio = (annual_return - risk_free_rate) / returns.std()
print(f'Sharpe Ratio: {sharpe_ratio:.2f}')

"""**Sortino Ratio:** Es una métrica similar al Índice de Sharpe, pero se enfoca en la volatilidad negativa en lugar de la volatilidad total. Mide el rendimiento de una inversión en relación con su riesgo negativo, lo que lo hace especialmente relevante para inversiones donde la reducción del riesgo a la baja es crítica. Tomando los datos calculados anteriormente, el Sortino Ratio es 7.80."""

downside_returns = returns[returns < 0]
sortino_ratio = (annual_return - risk_free_rate) / downside_returns.std()
print(f'Sortino Ratio: {sortino_ratio:.2f}')

"""Para continuar con el análisis de riesgo, vamos a imprimir las 10 mayores pérdidas diarias que ha experimentado el NASDAQ.

La mayor bajada se produjo el 16 de marzo de 2020, en plena pandemia, con una caída del 6.6%. La sengunda, también en marzo de 2020, fue del 5.08%. El resto de pérdidas son algo inferiores y están repartidos de manera heterogénea en varios años.
"""

outliers_low = nasdaq['return'].sort_values()

outliers_low.head(10)

"""En cuanto a la mayor ganancia diaria, esta se produjo el 24 de febrero de 2022, con una subida del 7.03%. El resto de ganancias también están repartidas de manera heterogénea, y son algo inferiores."""

outliers_high = nasdaq['return'].sort_values(ascending=False)

outliers_high.head(10)

"""###<u>Conclusión</u>

El Análisis Exploratorio de Datos revela varias características clave sobre el conjunto de datos que requieren una consideración cuidadosa. A continuación, se resumen las principales conclusiones extraídas del EDA:

1. **Valores Atípicos y Ruido:** El análisis de los datos muestra la presencia de valores atípicos en varias de las variables. Estos valores atípicos pueden introducir ruido en el análisis, por lo que será necesario usar métodos de normalización robustos a los valores atípicos y técnicas de reducción de ruido en el preprocesamiento de datos.

2. **No-Normalidad de las Variables:** La mayoría de las variables analizadas no siguen una distribución normal. Esto se evidencia a través de pruebas de normalidad como el test de Shapiro-Wilk. La falta de normalidad puede afectar la aplicación de algunas técnicas estadísticas que asumen una distribución normal en los datos.

3. **Baja Correlación con la Variable Dependiente:** Se observa una correlación baja entre las variables independientes y la variable dependiente. Esto sugiere que las variables independientes no están fuertemente relacionadas con la variable objetivo y pueden suponer un desafío a la hora de construir un modelo predictivo preciso.

4. **Presencia de Heterocedasticidad:** El análisis de las residuales sugiere la presencia de heterocedasticidad en los datos. La varianza de los errores no es constante en todo el rango de valores de la variable dependiente, lo que plantea problemas para los modelos de regresión.

5. **Autocorrelación en la Variable Objetivo:** Se ha detectado la presencia de autocorrelación en la variable objetivo en la diferencia de primer orden. Esto indica que los valores sucesivos de la variable objetivo están correlacionados.

6. **No Linealmente Separable:** Los datos no son linealmente separables, lo que significa que no se puede trazar un límite de decisión lineal claro para clasificar los datos. Esto implica que será necesario emplear algoritmos de clasificación más sofisticados que sean robustos a datos no linealmente separables.

7. **Estacionalidad:** Se ha encontrado evidencia tanto de no estacionalidad como de estacionalidad en los datos. La presencia de estacionalidad en la variable 'return' como proxy de la variable objetivo sugiere la estacionalidad de la variable objetivo. La falta de estacionalidad en otras variables indica que el patrón de los datos puede variar con el tiempo.

En resumen, los datos presentan una serie de desafíos y complejidades que deben abordarse de manera adecuada. Dadas las características del conjunto de datos, se propone utilizar el método de normalización "Robust Scaler" para manejar los valores atípicos. En cuanto a los algoritmos de clasificación, estas son algunas de las propuestas que se van a utilizar, ya que cuentan con atributos específicos que pueden ser útiles para abordar las características y limitaciones de los datos:

1. **Random Forest:** Random Forest es un algoritmo de conjunto que puede manejar datos no linealmente separables y es robusto frente a valores atípicos. Además, es capaz de captar relaciones no lineales en los datos.

2. **XGBoost (Extreme Gradient Boosting) Classifier:** Un potente algoritmo de boosting que puede manejar relaciones complejas entre variables y mejorar el rendimiento predictivo.

3. **Linear SVM con Soft Margin:** Un SVM con kernel lineal y márgen blando, más adecuado para clasificar clases que no se pueden separar perfectamente.

4. **LSTM (Long Short-Term Memory):** LSTM es una red neuronal recurrente que es capaz de capturar relaciones temporales en los datos, lo que es especialmente útil para datos con autocorrelación. Puede abordar la complejidad temporal de los datos y manejar secuencias no lineales.

Cada algoritmo aporta atributos específicos que pueden ser beneficiosos para enfrentar las características particulares del conjunto de datos y lograr un rendimiento óptimo en la clasificación.

###<u>Exportación a SQL</u>

Realizamos el checkpoint guardando el dataframe en la base de datos SQL.
"""

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

def crear_tabla_desde_dataframe(df, tabla, connection):
    cursor = connection.cursor()

    create_table_query = f"CREATE TABLE {tabla} (id INT AUTO_INCREMENT PRIMARY KEY"

    for columna, tipo in zip(df.columns, df.dtypes):
        if tipo == "float64":
            create_table_query += f", `{columna}` FLOAT"
        elif tipo == "int64":
            create_table_query += f", `{columna}` INT"
        elif tipo == "datetime64[ns]":
            create_table_query += f", `{columna}` DATE"
        elif tipo == "bool":
            create_table_query += f", `{columna}` BOOLEAN"

    create_table_query += ");"
    cursor.execute(create_table_query)

# Llamada a la función con las listas anteriores
crear_tabla_desde_dataframe(nasdaq, 'nasdaq_eda', connection)

from sqlalchemy import create_engine

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

def cargar_dataframe_en_tabla(df, tabla):
    df.to_sql(tabla, con = engine, if_exists='replace', index=False)

nasdaq.reset_index(inplace=True)

cargar_dataframe_en_tabla(nasdaq, 'nasdaq_eda')

connection.close()
engine.dispose()

"""# **Ingeniería de Características**

###<u>Importación de SQL</u>
"""

!pip install protobuf mysql-connector-python > /dev/null
import mysql.connector

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

import pandas as pd
from sqlalchemy import create_engine, text

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

conn = engine.connect()

query = text(f'SELECT * FROM nasdaq_eda')
nasdaq = pd.read_sql(query, con=conn)

nasdaq = nasdaq.set_index('Date')

conn.close()
connection.close()

"""Visualizamos el dataframe para comprobar que se ha importado correctamente y que el índice de fechas es correcto.


"""

nasdaq

"""###<u>División en Train, Dev y Test</u>

Antes de aplicar ninguna técnica de preprocesamiento vamos dividir los datos en entrenamiento (train: 70%), validación (dev: 10%) y prueba (test: 20%) para evitar cualquier tipo de contaminación en los datos o 'data leakage'.

El 'data leakage' es un fenómeno en el que se produce una fuga información del conjunto de prueba en el conjunto de entrenamiento, lo que puede conducir a una evaluación incorrecta del rendimiento del modelo. En otras palabras, se produce cuando la información del conjunto de datos de entrenamiento incluye accidental o intencionalmente detalles sobre la variable objetivo que no estarían disponibles en un entorno de producción. Esto llevaría a una evaluación demasiado optimista en comparación a los resultados que el modelo obtendrá en la realidad, ya que el modelo contó con información privilegiada a la hora de entrenarse.
"""

total_rows = len(nasdaq)
train_size = int(0.7 * total_rows)
dev_size = int(0.1 * total_rows)

# Dividimos el DataFrame en los conjuntos de train, dev y test
train_df = nasdaq[:train_size]
dev_df = nasdaq[train_size:(train_size + dev_size)]
test_df = nasdaq[(train_size + dev_size):]

print("Train: ", len(train_df), "\nDev: ", len(dev_df), "\nTest: ", len(test_df))

for df_name, df in [("train_df", train_df), ("dev_df", dev_df), ("test_df", test_df)]:
    print(f"Fecha inicial en {df_name}: {df.index[0]}")
    print(f"Fecha final en {df_name}: {df.index[-1]}")

"""Una vez obtenidas las fechas de división podemos graficar los tres períodos. El conjunto train servirá para entrenar los modelos. El conjunto dev servirá para ajustar los hiperparámetros en validación cruzada. Encontrar los hiperparámetros en el conjunto dev en vez de en el conjunto train será computacionalmente menos costoso ya que el conjunto dev es más pequeño y al mismo tiempo nos servirá para generalizar en el conjunto train. Por último, el conjunto test servirá para evaluar las predicciones del modelo entrenado. La ventaja de la división realizada es que train recoge la crisis de covid de 2020, por lo que podrá entrenarse en periódos de alta volatilidad y caídas."""

import matplotlib.pyplot as plt

# Convertimos las fechas de corte a objetos de fecha y hora de Pandas
corte1 = pd.to_datetime('2020-07-02')
corte2 = pd.to_datetime('2021-06-30')

# Graficamos la variable 'Close'
plt.figure(figsize=(12, 6))
plt.plot(train_df.index, train_df['Close'], label='Train')
plt.plot(dev_df.index, dev_df['Close'], label='Dev')
plt.plot(test_df.index, test_df['Close'], label='Test')

# Agregamos líneas verticales discontinuas para señalar las diferencias
plt.axvline(x=corte1, color='gray', linestyle='--', label='2020-07-02')
plt.axvline(x=corte2, color='gray', linestyle='--', label='2021-06-30')

plt.xlabel('Fecha')
plt.ylabel('Precio de Cierre')
plt.title('División de Train, Dev y Test')
plt.legend()
plt.grid()
plt.show()

"""Definimos una función para separa la variable objetivo del conjunto de variables independientes."""

# Dividimos los conjuntos en y (target_return) y X (variables independientes)

def xy_split(df):
  y = df['target_return']
  x = df.drop(columns=['target_return'])

  return x, y

train_x, train_y = xy_split(train_df)

"""###<u>Wavelet Transform</u>

La transformada wavelet es una técnica utilizada en procesamiento de señales y análisis de datos que permite descomponer una señal o conjunto de datos en componentes que representan diferentes niveles de detalle. Esta descomposición es útil para identificar patrones, estructuras y cambios en la señal en distintas escalas, lo que facilita el análisis y la extracción de información relevante.

Cuando se aplica la transformada wavelet a datos, se obtienen coeficientes wavelet que representan la contribución de diferentes frecuencias o escalas en la señal original. Estos coeficientes pueden dividirse en dos partes: aproximación (o componente de baja frecuencia) y detalle (o componente de alta frecuencia). La aproximación captura las características generales y suavizadas de la señal, mientras que los detalles resaltan las variaciones finas y cambios rápidos.

Podría decirse que hay dos tipos de información en el conjunto de entrenamiento: señal y ruido. La 'señal' se asemeja a la información útil y generalizable en los datos, mientras que el 'ruido' representa las fluctuaciones aleatorias o patrones incidentales que no contribuyen significativamente a la capacidad del modelo para realizar predicciones en nuevos datos.

La función que utilizamos a continuación descompone las series temporales en ondículas y elimina los 3 componentes de mayor frecuencia para suavizar la señal eliminando el ruido (ondículas de alta frecuencia). La elección de eliminar los 3 componentes de mayor frecuencia se ha realizado tras un proceso de prueba y error en la variable 'Close' en entrenamiento que será generalizada al resto de variables y al conjunto test para que sea consistente con el entrenamiento. El proceso de ajuste se ha llevado a cabo solo en train para evitar el 'data leakage'.

En cuanto a la elección del tipo de transformada wavelet, se ha optado por la transformada wavelet de Daubechies (sym5) en lugar de la transformada wavelet de Haar, empleada en otros trabajos. La razón principal es que las series temporales financieras diarias tienden a ser más suaves y contienen fluctuaciones de alta frecuencia, y las wavelets de Daubechies, como 'sym5', suelen ser más apropiadas debido a su capacidad para representar y analizar de manera eficiente las fluctuaciones de alta frecuencia, reducir el ruido y preservar las tendencias en los datos. Esto se ha comprobado mediante prueba y error también.

Documentación librería pywt: https://pywavelets.readthedocs.io/en/latest/index.html
"""

import pywt
import numpy as np

def wavelet_transform(data):
  # Creamos una copia del dataframe original
  transformed_data = data.copy()
  for col in data.columns:
    # Evitamos las variables dummies
    if col in ['dummy_support', 'dummy_resistance']:
      continue
    # Descomponemos las variables en ondículas
    coeff = pywt.wavedec(data[col], 'sym5', mode='symmetric')
    # Eliminamos los 3 componentes de mayor frecuencia para suavizar las variables
    coeff[-3:] = [np.zeros_like(c) for c in coeff[-3:]]
    # Reconstruimos las variables con las ondículas restantes
    rec = pywt.waverec(coeff, 'sym5', mode='symmetric')
    if len(rec) > len(transformed_data):
            rec = rec[:-1]
    transformed_data[col] = rec
  return transformed_data

"""El proceso de prueba y error se ha llevado a cabo mediante la visualización del gráfico de 'Close'. En él se puede observar la efectividad en la eliminación del ruido en las variables. Una de las grandes ventajas de la transformada wavelet frente a otros métodos de eliminación de ruido como las medias móviles es que la señal recompuesta por la transformada wavelet es simultánea a la señal original, evitando así el lag o retraso que se produce con las medias móviles."""

import matplotlib.pyplot as plt

train_wt = wavelet_transform(train_x)

# Tras un proceso de prueba y error, la eliminación óptima de componentes de alta frecuencia es 3
y = train_x['Close']
y_wt = train_wt['Close']
x = train_x.index

plt.figure(figsize=(18,6))
plt.plot(x, y)
plt.plot(x, y_wt)
plt.legend(['Original', 'Transformada Wavelet'])

plt.show()

"""Visualizamos también como queda la transformada ajustada en train en los datos de test. La generalización resulta ser efectiva."""

import matplotlib.pyplot as plt

test_x = test_df.drop(columns=['target_return'])
test_wt = wavelet_transform(test_x)

y = test_x['Close']
y_wt = test_wt['Close']
x = test_x.index

plt.figure(figsize=(18,6))
plt.plot(x, y)
plt.plot(x, y_wt)
plt.legend(['Original', 'Transformada Wavelet'])

plt.show()

"""###<u>Normalización</u>

Antes de entrenar los modelos se va a proceder a la normalización de las variables por los siguientes motivos:

1. Escalas Consistentes: garantizar que todas las variables estén en la misma escala, evitando sesgos por diferencias en unidades.

2. Convergencia Rápida: facilitar la convergencia rápida de algoritmos, mejorando la eficiencia del entrenamiento.

3. Evitar Dominancia de Atributos: prevenir que variables con magnitudes mayores dominen el impacto en el modelo, asegurando una contribución equitativa.

4. Interpretación Facilitada: facilitar la interpretación de coeficientes en modelos lineales, haciéndolos directamente comparables.

5. Mejora del Rendimiento de los Modelos: mejorar el rendimiento general de los modelos, especialmente en cálculos numéricos y optimización.

Se ha escogido Robust Scaler como método de normalización del conjunto de datos por su resistencia a outliers, ya que, como se comentó en el EDA, se ha decidido no eliminarlos. Estos valores atípicos pueden afectar negativamente al desempeño de los modelos, ya que pueden influir significativamente en el cálculo de las medidas de tendencia central y dispersión. Robust Scaler mitiga el impacto de los valores atípicos al centrarse en estadísticas resistentes, como la mediana y el rango intercuartil (IQR), en lugar de la media y la desviación estándar empleada en otros métodos de normalización.

Dado que Robust Scaler emplea estadísticas del conjunto de datos, vamos a ajustarlo solo en base al conjunto de datos de entrenamiento y luego transformaremos el conjunto test con ese ajuste. El objetivo es evitar la contaminación del conjunto de entrenamiento que se produciría si las estadísticas como la mediana o IQR se ven influenciadas por el conjunto test.

Documentación de RobustScaler: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html
"""

from sklearn.preprocessing import RobustScaler

# Crear un objeto RobustScaler
scaler = RobustScaler()

# Ajustar y transformar los conjuntos de entrenamiento
train_norm = scaler.fit_transform(train_wt)

"""###<u>Stacked Autoencoders</u>

Como última técnica de preprocesamiento vamos a contruir unos codificadores apilados o Stacked Autoencoders. Los Stacked Autoencoders son una forma especial de redes neuronales artificiales utilizadas para la reducción de dimensionalidad en las variables de un modelo a través de la extracción de sus características profundas.

La red se entrena para aprender una codificación eficiente de los datos de entrada. Hay una fase de codificación donde reduce la dimensionalidad de los datos. Luego, tiene una fase de decodificación que intenta reconstruir los datos originales a partir de la representación codificada. El objetivo es que los autoencoders aprendan automáticamente características relevantes de los datos, extrayendo información valiosa y eliminando el ruido. Esto reduce la dimensionalidad al mismo tiempo que se mantiene la información esencial de las variables, mejorando la generalización de los modelos.

En el código creamos 5 autoencoders con una capa de entrada, una capa profunda y una capa de salida, con 150 neuronas cada una. Una vez definidos y entrenados, se apilan en un modelo secuencial de Keras. Como la mayoría de hiperparámetros se han ajustado a través de un proceso de pureba y error, se imprime por pantalla el error mínimo del entrenamiento para hacer que los autoencoders produzcan la representación más fiel posible de los datos originales. Una vez ajustados, entrenados y apilados, los Stacked Autoencoders reciben como input las variables originales y producen como output una representación de sus características profundas.

El input que reciben para el ajuste son los datos de entrenamiento después de haberles aplicado la transformada wavelet y la normalización. Se exluye el conjunto test para evitar 'data leakage'.

Documentación de tensorflow: https://www.tensorflow.org/api_docs/python/tf/all_symbols
"""

import tensorflow as tf

# Creamos una función para construir un AE
def build_single_layer_ae(input_dim, hidden_dim):
    model = tf.keras.models.Sequential([
        tf.keras.layers.InputLayer(input_shape=(input_dim,)),
        tf.keras.layers.Dense(hidden_dim, activation='elu'),
        tf.keras.layers.Dense(input_dim, activation='elu')
    ])

    model.compile(optimizer='adam', loss='mse')

    return model

# Creamos una lista de AEs para apilarlos
single_layer_aes = []
# Número de características de entrada
input_dim = 47
# Tamaño de las capas ocultas para cada uno de los 5 AE
hidden_dims = [150, 150, 150, 150, 150]

for hidden_dim in hidden_dims:
    ae = build_single_layer_ae(input_dim, hidden_dim)
    single_layer_aes.append(ae)

# Entrenamos cada AE en una capa
for ae in single_layer_aes:
    min_loss = float('inf')
    for epoch in range(300):
        history = ae.fit(train_norm, train_norm, epochs=1, batch_size=16, verbose=0)
        loss = history.history['loss'][0]
        min_loss = min(min_loss, loss)

    # Mostramos el valor mínimo de pérdida para cada AE para obtener una referencia
    print(f'Min Loss for AE: {min_loss}')

# Apilamos los AE para obtener un SAE
stacked_ae = tf.keras.models.Sequential()
stacked_ae.add(tf.keras.layers.InputLayer(input_shape=(input_dim,)))
for ae in single_layer_aes:
    stacked_ae.add(ae)

"""Para las variables independientes binarias no es necesaria ningun tipo de transformación, ya que no contienen ruido ni han de ser normalizadas. Por este motivo reinstauramos estas variables a su estado original, antes de las transformaciones."""

print(nasdaq.columns[25])
print(nasdaq.columns[26])

def dummy_adjust(array, dataframe):
  # Sustituir las columnas 25 y 26 del array por las columnas de dummies
  array[:, 25] = dataframe['dummy_support'].values
  array[:, 26] = dataframe['dummy_resistance'].values

  return array

"""###<u>Procesamiento y Exportación</u>

Definimos una función que incluya los 5 pasos del preprocesamiento.
"""

def preprocessing(raw_data):
  # Paso 1: Separar X e Y
  train_x, train_y = xy_split(raw_data)

  # Paso 2: Aplicar Wavelet Transform
  train_wt = wavelet_transform(train_x)

  # Paso 3: Normalización con Robust Scaler (el scaler ya está ajustado a train)
  train_norm = scaler.transform(train_wt)

  # Paso 4: Procesar los datos con SAE para obtener características extraídas
  train_sae = stacked_ae.predict(train_norm)

  # Paso 5: Recuperar las variables dummies originales
  train_sae = dummy_adjust(train_sae, train_wt)

  return train_x, train_y, train_wt, train_norm, train_sae

"""En los modelos se van a evaluar las diferentes técnicas de preprocesamiento, por lo que tenemos que guardar los conjuntos de cada paso. Se aplican las transformaciones paralela e independientemente a los conjuntos train, dev y test para evitar fugas de datos."""

train_x, train_y, train_wt, train_norm, train_sae = preprocessing(train_df)
dev_x, dev_y, dev_wt, dev_norm, dev_sae = preprocessing(dev_df)
test_x, test_y, test_wt, test_norm, test_sae = preprocessing(test_df)

"""Se convierten todos los objetos a dataframes para lograr una homogeneidad."""

# Lista de arrays de NumPy y sus nombres correspondientes
arrays = [('train_x', train_x), ('train_y', train_y), ('train_wt', train_wt), ('train_norm', train_norm), ('train_sae', train_sae),
          ('dev_x', dev_x), ('dev_y', dev_y), ('dev_wt', dev_wt), ('dev_norm', dev_norm), ('dev_sae', dev_sae),
          ('test_x', test_x), ('test_y', test_y), ('test_wt', test_wt), ('test_norm', test_norm), ('test_sae', test_sae)]

def convert_to_dataframe(obj):
    if isinstance(obj[1], (pd.DataFrame, pd.Series)):
        # Si ya es DataFrame, no es necesario convertirlo
        return (obj[0], obj[1])
    else:
        # Si no lo es, lo convertimos a DataFrame
        return (obj[0], pd.DataFrame(obj[1]))

arrays = [convert_to_dataframe(obj) for obj in arrays]

# Verificar los tipos de los objetos después de la conversión
for name, dataframe in arrays:
    print(f"Nombre: {name}, Tipo: {type(dataframe)}")

"""Se guardan en la base de datos persistente."""

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

def crear_tabla_desde_arrays(arrays, connection):
    cursor = connection.cursor()

    for nombre, dataframe in arrays:
        create_table_query = f"CREATE TABLE {nombre} (id INT AUTO_INCREMENT PRIMARY KEY"
        columna = "col"

        if len(dataframe.columns) > 1:
            for i, col in enumerate(dataframe.columns):
                create_table_query += f", `{col}` FLOAT"
        else:
            create_table_query += f", `{dataframe.columns[0]}` FLOAT"

        create_table_query += ");"
        cursor.execute(create_table_query)

crear_tabla_desde_arrays(arrays, connection)

from sqlalchemy import create_engine

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

def cargar_dataframe_en_tabla(df_list):
  for tabla, dataframe in df_list:
    dataframe.to_sql(tabla, con = engine, if_exists='replace', index=False)

cargar_dataframe_en_tabla(arrays)

connection.close()
engine.dispose()

"""# **Entrenamiento y Evaluación de Modelos**"""

!pip install protobuf mysql-connector-python > /dev/null
import mysql.connector

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

conn = engine.connect()

# Definición de la función para importar una tabla de SQL y convertirla en un DataFrame

def importar_tabla_a_dataframe(df_name):
    query = text(f'SELECT * FROM {df_name}')
    df = pd.read_sql(query, con=conn)
    return df

train_x = importar_tabla_a_dataframe('train_x')
train_y = importar_tabla_a_dataframe('train_y').values.ravel()  # Utilizar values y ravel() para convertir a una matriz 1D
train_wt = importar_tabla_a_dataframe('train_wt')
train_norm = importar_tabla_a_dataframe('train_norm')
train_sae = importar_tabla_a_dataframe('train_sae')

dev_x = importar_tabla_a_dataframe('dev_x')
dev_y = importar_tabla_a_dataframe('dev_y').values.ravel()
dev_wt = importar_tabla_a_dataframe('dev_wt')
dev_norm = importar_tabla_a_dataframe('dev_norm')
dev_sae = importar_tabla_a_dataframe('dev_sae')

test_x = importar_tabla_a_dataframe('test_x')
test_y = importar_tabla_a_dataframe('test_y').values.ravel()
test_wt = importar_tabla_a_dataframe('test_wt')
test_norm = importar_tabla_a_dataframe('test_norm')
test_sae = importar_tabla_a_dataframe('test_sae')

conn.close()
engine.dispose()

"""El objetivo de importar los conjuntos de los pasos intermedios del preprocesamiento es evaluar el valor añadido que cada paso aporta en aras de mejorar la capacidad predictiva de los modelos.

Para ello, primero se realiza una validación cruzada en el conjunto dev con GridSearch para obtener los hiperparámetros óptimos. Estos hiperparámetros no serán los definitivos, simplementes serán un punto de partida desde el cual intentaremos aproximarnos a los hiperparámetros realmente óptimos a través de un proceso de prueba y error.

Posteriormente, se evalua la precisión de los modelos primero en un conjunto totalmente preprocesado (Wavelet Transform + Normalización + Stacked Autoencoders), luego solo con WT + Normalización, luego solo con WT y finalmente se evalua en un conjunto sin preprocesar.

Finalmente, se selecciona el mejor modelo (con el mejor conjunto de los anteriormente mencionados) y se evalua mediante una matriz de confusión.

###<u>Regresión Logística</u>

El primer modelo será una regresión logística con scikit learn. Obtenemos los hiperparámetros de punto de partida con los que iremos experimentando.

Documentación de LogisticRegression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

Documentación de GridSearchCV: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html

Documentación de accuracy_score: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

# Definir los hiperparámetros a ajustar
param_grid = {
    'penalty': ['l1', 'l2'],
    'C': [10, 1, 0.1, 0.01, 0.001],
    'solver': ['liblinear', 'saga']
}

# Crear un objeto de regresión logística
logistic_reg = LogisticRegression(fit_intercept=True, random_state=42, solver='saga', max_iter=10000)

# Crear un objeto GridSearchCV
grid_search = GridSearchCV(logistic_reg, param_grid, cv=5, scoring='accuracy')

# Realizar la búsqueda en cuadrícula en los datos de entrenamiento
grid_search.fit(dev_sae, dev_y)

# Obtener los mejores hiperparámetros y el modelo óptimo
best_params = grid_search.best_params_
print("Mejores hiperparámetros:", best_params)

"""Ajustamos el modelo y lo evaluamos con cada conjunto de datos. Definimos random_state=42 para garantizar la reproducibilidad de los resultados.

DATOS PROCESADOS (WT + NORM + SAE):
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# logistic_reg = LogisticRegression(penalty='l1', C=1.0, fit_intercept=True, random_state=42, solver='saga', max_iter=50000)
# modelo_lr = logistic_reg.fit(train_sae, train_y)
# y_lr = modelo_lr.predict(test_sae)
# 
# accuracy = accuracy_score(test_y, y_lr)
# print(f'Precisión en datos de validación: {accuracy * 100:.2f}%')

"""DATOS PROCESADOS (WT + NORM):"""

modelo_lr1 = logistic_reg.fit(train_norm, train_y)
y_lr1 = modelo_lr1.predict(test_norm)

accuracy = accuracy_score(test_y, y_lr1)
print(f'Precisión en datos de validación: {accuracy * 100:.2f}%')

"""DATOS PROCESADOS (WT):"""

modelo_lr2 = logistic_reg.fit(train_wt, train_y)
y_lr2 = modelo_lr2.predict(test_wt)

accuracy = accuracy_score(test_y, y_lr2)
print(f'Precisión en datos de validación: {accuracy * 100:.2f}%')

"""DATOS SIN PROCESAR:"""

modelo_lr3 = logistic_reg.fit(train_x, train_y)
y_lr3 = modelo_lr3.predict(test_x)

accuracy = accuracy_score(test_y, y_lr3)
print(f'Precisión en datos de validación: {accuracy * 100:.2f}%')

"""El mejor de los modelos es el totalmente preprocesado, con una precisión del 59.48%. Si bien parece que la transformada wavelet no añade capacidad predictiva al modelo de regresión logística, la normalización y los stacked autoencoders si que lo hacen. A continuación imprimimos los coeficientes y las predicciones del mejor modelo."""

coeficientes_lr = modelo_lr.coef_[0]

coeficientes_lr

y_lr

"""Evaluamos el mejor modelo con una matriz de confusión de porcentajes. De las veces que el modelo predice una subida al día siguiente, un 60.98% la subida se acaba produciendo, dando un 39.02% de falsas entradas. En el caso de las predicciones negativas, el procentaje de acierto es de un 57.81%.

Documentación de confusion_matrix https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html
"""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Calcular la matriz de confusión
confusion = confusion_matrix(test_y, y_lr)

# Calcular los porcentajes en lugar de contar
confusion_percentage = confusion.astype('float') / confusion.sum(axis=0)[np.newaxis, :]

# Crear una visualización de la matriz de confusión en porcentajes
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_percentage, annot=True, fmt=".2%", cmap="Blues", xticklabels=["Negativo", "Positivo"], yticklabels=["Negativo", "Positivo"])
plt.xlabel('Predicciones')
plt.ylabel('Etiquetas Reales')
plt.title('Matriz de Confusión en Porcentajes')
plt.show()

"""###<u>Random Forest</u>

Realizamos la misma operativa con Random Forest.

Documentación RandomForestClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import GridSearchCV
# from sklearn.metrics import accuracy_score
# 
# # Creamos un clasificador de Random Forest con semilla para garantizar la reproducibilidad de los resultados
# rf_classifier = RandomForestClassifier(random_state=42)
# 
# # Definimos la cuadrícula de hiperparámetros para búsqueda
# param_grid = {
#     'n_estimators': [500, 700, 900],
#     'max_depth': [None, 5, 10],
#     'min_samples_split': [2, 5, 10],
#     'max_leaf_nodes': [50, 100, 200],
# }
# 
# grid_search = GridSearchCV(rf_classifier, param_grid, cv=5)
# grid_search.fit(dev_sae, dev_y)
# 
# best_params = grid_search.best_params_
# print("Mejores hiperparámetros:", best_params)

"""DATOS PROCESADOS (WT + NORM + SAE):"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

model_rf = RandomForestClassifier(n_estimators=700, criterion='gini', max_depth=5, min_samples_split=2, min_samples_leaf=42,
                                  max_features=None, max_leaf_nodes=100, random_state=42, max_samples=850)
modelo_rf = model_rf.fit(train_sae, train_y)

y_rf = modelo_rf.predict(test_sae)

accuracy = accuracy_score(test_y, y_rf)
print(f'Precisión del modelo Random Forest: {accuracy * 100:.2f}%')

"""DATOS PROCESADOS (WT + NORM):"""

modelo_rf1 = model_rf.fit(train_norm, train_y)

y_rf1 = modelo_rf1.predict(test_norm)

accuracy = accuracy_score(test_y, y_rf1)
print(f'Precisión del modelo Random Forest: {accuracy * 100:.2f}%')

"""DATOS PROCESADOS (WT):"""

modelo_rf2 = model_rf.fit(train_wt, train_y)

y_rf2 = modelo_rf.predict(test_wt)

accuracy = accuracy_score(test_y, y_rf2)
print(f'Precisión del modelo Random Forest: {accuracy * 100:.2f}%')

"""DATOS NO PROCESADOS:"""

modelo_rf3 = model_rf.fit(train_x, train_y)

y_rf3 = modelo_rf3.predict(test_x)

accuracy = accuracy_score(test_y, y_rf3)
print(f'Precisión del modelo Random Forest: {accuracy * 100:.2f}%')

"""En Random Forest el paso que más valor añadido aporta es la transformada wavelet, alcanzando un 60.28% de precisión. En este caso los Satcked Autoencoders restan precisión al modelo. Imprimimos los coeficientes."""

importances = modelo_rf1.feature_importances_
print(importances)

"""El porcentaje de acierto de las entradas positivas es de un 59.17%, mientras que el de las no-entradas es de un 62.58%."""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

confusion = confusion_matrix(test_y, y_rf1)

confusion_percentage = confusion.astype('float') / confusion.sum(axis=0)[np.newaxis, :]

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_percentage, annot=True, fmt=".2%", cmap="Blues", xticklabels=["Negativo", "Positivo"], yticklabels=["Negativo", "Positivo"])
plt.xlabel('Predicciones')
plt.ylabel('Etiquetas Reales')
plt.title('Matriz de Confusión en Porcentajes')
plt.show()

"""Graficamos la importancia de las características del modelo para ver el peso que ha tenido cada una de ellas en la predicción del modelo."""

import seaborn as sns
import matplotlib.pyplot as plt

# Obtener la importancia de las características
feature_importance = modelo_rf1.feature_importances_

# Obtener los nombres de las características
feature_names = ['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_50', 'SMA_200', 'EMA_9',
       'EMA_50', 'EMA_200', 'WMA_9', 'WMA_50', 'WMA_200', 'RSI_14',
       'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'STOCHk_14_3_3',
       'STOCHd_14_3_3', 'MOM_10', 'BBL_20_2.0', 'BBM_20_2.0', 'BBU_20_2.0',
       'BBB_20_2.0', 'BBP_20_2.0', 'dummy_support', 'dummy_resistance',
       '10y-2y yield curve', 'volatility', '5-year interest rate',
       'federal interest rate', 'junk bonds spread', 'S&P500', 'Wilshire 5000',
       'Dow Jones Industrial Average', 'Nikkei 225', 'Euro STOXX 50',
       'Hang Heng Index', 'Shanghai Stock Exchange', 'FTSE 100',
       'Gold futures', 'Crude oil futures', 'unemployment', 'inflation',
       'consumer expectation', 'industrial production', 'return']

# Configurar el estilo de seaborn ("whitegrid")
sns.set(style="whitegrid")

# Elegir un color específico para todas las barras
bar_color = 'steelblue'

# Crear un gráfico de barras horizontal con barras del mismo color
plt.figure(figsize=(10, 12))
ax = sns.barplot(x=feature_importance, y=feature_names, orient="h", color=bar_color)

plt.xlabel('Importancia de las características')
plt.ylabel('Características')
plt.title('Importancia de las características en el modelo Random Forest')

# Ajustar el espaciado entre las barras
for p in ax.patches:
    width = p.get_width()
    ax.annotate(f'{width:.4f}', (width, p.get_y() + p.get_height() / 2), ha="left", va="center")

plt.show()

"""###<u>XGBoost Classifier</u>

Volvemos a hacer lo mismo con el algoritmo XGBoost Classifier, primero validación cruzada y después evaluación de modelos.

Documentación xgb: https://xgboost.readthedocs.io/en/stable/
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# from sklearn.model_selection import GridSearchCV
# 
# # Definir la cuadrícula de hiperparámetros
# param_grid = {
#     'max_depth': [2, 3, 4],
#     'learning_rate': [0.1, 0.01, 0.001],
#     'reg_alpha': [0.0001, 0.001, 0.01],
#     'reg_lambda': [0.1, 1, 10]
# }
# 
# # Crear un modelo XGBoost
# xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42)
# 
# # Realizar la búsqueda en cuadrícula
# grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy')
# grid_search.fit(dev_sae, dev_y)
# 
# best_params = grid_search.best_params_
# print("Mejores hiperparámetros:", best_params)

"""DATOS PROCESADOS (WT + NORM + SAE):"""

import xgboost as xgb
from sklearn.metrics import accuracy_score

xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42, n_estimators=110, learning_rate=0.1, max_depth=2,
                              subsample=0.8, colsample_bytree=0.8, reg_alpha=0.001, reg_lambda=1)

modelo_xgb = xgb_model.fit(train_sae, train_y)

y_xgb = modelo_xgb.predict(test_sae)

accuracy = accuracy_score(test_y, y_xgb)
print(f'Precisión del modelo XGBoost Classifier: {accuracy * 100:.2f}%')

"""DATOS PROCESADOS (WT + NORM):"""

modelo_xgb1 = xgb_model.fit(train_norm, train_y)

y_xgb1 = modelo_xgb1.predict(test_norm)

accuracy = accuracy_score(test_y, y_xgb1)
print(f'Precisión del modelo XGBoost Classifier: {accuracy * 100:.2f}%')

"""DATOS PROCESADOS (WT):"""

modelo_xgb2 = xgb_model.fit(train_wt, train_y)

y_xgb2 = modelo_xgb2.predict(test_wt)

accuracy = accuracy_score(test_y, y_xgb2)
print(f'Precisión del modelo XGBoost Classifier: {accuracy * 100:.2f}%')

"""DATOS NO PROCESADOS:"""

modelo_xgb3 = xgb_model.fit(train_x, train_y)

y_xgb3 = modelo_xgb3.predict(test_x)

accuracy = accuracy_score(test_y, y_xgb3)
print(f'Precisión del modelo XGBoost Classifier: {accuracy * 100:.2f}%')

"""En este caso la mezcla de wavelet transform junto con stacked autoencoders incrementan la capacidad predictiva del modelo hasta el punto de ser el modelo con mejor precisión hasta ahora. En la matriz de confusión el porcentaje de aciertos en las predicciones positivas alcanza el 63.02%, mientras que en las negativas es un 60.17%."""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

confusion = confusion_matrix(test_y, y_xgb)

confusion_percentage = confusion.astype('float') / confusion.sum(axis=0)[np.newaxis, :]

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_percentage, annot=True, fmt=".2%", cmap="Blues", xticklabels=["Negativo", "Positivo"], yticklabels=["Negativo", "Positivo"])
plt.xlabel('Predicciones')
plt.ylabel('Etiquetas Reales')
plt.title('Matriz de Confusión en Porcentajes')
plt.show()

"""Abajo se imprimen las importancias asignadas por el modelo a las características. Se puede observar ciertas características con importancia igual a cero debido a la penalización de los parámetros de regularización a las características no informativas. Nótese que dummy_resistance tiene una importancia relativamente alta dentro del modelo, por lo que podemos decir que la creación de esta variable en la fase de extracción de datos ha sido relevante para la construcción de este modelo."""

import seaborn as sns
import matplotlib.pyplot as plt

# Obtener la importancia de las características
feature_importance = xgb_model.feature_importances_

# Obtener los nombres de las características
feature_names = ['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_50', 'SMA_200', 'EMA_9',
       'EMA_50', 'EMA_200', 'WMA_9', 'WMA_50', 'WMA_200', 'RSI_14',
       'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'STOCHk_14_3_3',
       'STOCHd_14_3_3', 'MOM_10', 'BBL_20_2.0', 'BBM_20_2.0', 'BBU_20_2.0',
       'BBB_20_2.0', 'BBP_20_2.0', 'dummy_support', 'dummy_resistance',
       '10y-2y yield curve', 'volatility', '5-year interest rate',
       'federal interest rate', 'junk bonds spread', 'S&P500', 'Wilshire 5000',
       'Dow Jones Industrial Average', 'Nikkei 225', 'Euro STOXX 50',
       'Hang Heng Index', 'Shanghai Stock Exchange', 'FTSE 100',
       'Gold futures', 'Crude oil futures', 'unemployment', 'inflation',
       'consumer expectation', 'industrial production', 'return']

# Configurar el estilo de seaborn ("whitegrid")
sns.set(style="whitegrid")

# Elegir un color específico para todas las barras
bar_color = 'steelblue'


plt.figure(figsize=(10, 12))
ax = sns.barplot(x=feature_importance, y=feature_names, orient="h", color=bar_color)

plt.xlabel('Importancia de las características')
plt.ylabel('Características')
plt.title('Importancia de las características en el modelo XGBoost')

# Ajustar el espaciado entre las barras
for p in ax.patches:
    width = p.get_width()
    ax.annotate(f'{width:.4f}', (width, p.get_y() + p.get_height() / 2), ha="left", va="center")

plt.show()

"""###<u>Support Vector Machine</u>

Debido a la exigente demanda computacional que ha supuesto el entrenamiento de este modelo en particular, y también con la intención de entrenar las redes neuronales en TPU en la siguiente sección, se ha procedido a la compra de 100 unidades informáticas en Google Colab para poder entrenar los modelos en GPU y TPU. El objetivo también es demostrar la posible escalabilidad del proyecto para algoritmos más exigentes o para conjuntos de datos más grandes.

Lo primero es cambiar el entorno de ejecución. Pasamos de CPU con RAM de 13GB a GPU V100 con 54.8GB. Para comprobar que está correctamente conectado, ejecutamos el siguiente código:
"""

# Nos aseguramos de que el entorno de ejecución esté correctamente conectado a la GPU

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

"""Comprobamos la capacidad de la RAM."""

# Comprobamos la capacidad de la RAM

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

"""Scikit-learn es una biblioteca de aprendizaje automático en Python que incluye implementaciones de SVM, pero no es una biblioteca específicamente diseñada para acelerar el entrenamiento de modelos en GPU. Para sacar el máximo rendimiento a la GPU y acelerar significativamente el entrenamiento de SVM lineales, se ha considerado utilizar cuML (Rapids.ai) como biblioteca específica de SVM acelerada por GPU. Para ello primero hay que realizar las siguientes instalaciones.

Documentación cuML: https://docs.rapids.ai/api/cuml/stable/
"""

!git clone https://github.com/rapidsai/rapidsai-csp-utils.git > /dev/null
!python rapidsai-csp-utils/colab/pip-install.py > /dev/null

import cudf
import cuml
from cuml.svm import LinearSVC
from cuml.metrics import accuracy_score

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# from sklearn.model_selection import GridSearchCV
# 
# #Crear los objetos GPU
# X_train_gpu = cudf.DataFrame(dev_sae).values
# y_train_gpu = cudf.Series(dev_y).values
# 
# # Define los valores de hiperparámetros que deseas explorar en la búsqueda en cuadrícula
# param_grid = {
#     'C': [0.01, 0.1, 1.0, 10.0],
#     'penalty': ['l1', 'l2'],
#     'loss': ['hinge', 'squared_hinge'],
# }
# 
# # Crea el modelo LinearSVC
# svm_classifier = LinearSVC(max_iter=10000)
# 
# # Crea un objeto GridSearchCV para la búsqueda en cuadrícula
# grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid, cv=5)
# 
# # Ajusta el modelo a los datos de entrenamiento en GPU
# grid_search.fit(X_train_gpu.get(), y_train_gpu.get())
# 
# best_params = grid_search.best_params_
# print("Mejores hiperparámetros:", best_params)

"""DATOS PROCESADOS (WT + NORM + SAE):

Nótese que los tiempos de ejecución son muy cortos gracias a la GPU.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# X_train_gpu = cudf.DataFrame(train_sae).values
# y_train_gpu = cudf.Series(train_y).values
# X_test_gpu = cudf.DataFrame(test_sae).values
# 
# # Crear un modelo de LinearSVC
# svm_classifier = LinearSVC(C=0.15, penalty='l1', loss='squared_hinge', max_iter=10000)
# 
# # Entrenar el modelo con los datos de entrenamiento
# modelo_svm = svm_classifier.fit(X_train_gpu, y_train_gpu)
# 
# # Realizar predicciones en los datos de prueba
# y_svm = modelo_svm.predict(X_test_gpu)
# 
# accuracy = accuracy_score(test_y, y_svm)
# print(f'Precisión del modelo LinearSVC en GPU: {accuracy * 100:.2f}%')

"""DATOS PROCESADOS (WT + NORM):"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# X_train_gpu = cudf.DataFrame(train_norm).values
# X_test_gpu = cudf.DataFrame(test_norm).values
# 
# modelo_svm1 = svm_classifier.fit(X_train_gpu, y_train_gpu)
# 
# y_svm1 = modelo_svm1.predict(X_test_gpu)
# 
# accuracy = accuracy_score(test_y, y_svm1)
# print(f'Precisión del modelo SVM Classifier en GPU: {accuracy * 100:.2f}%')

"""DATOS PROCESADOS (WT):"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# X_train_gpu = cudf.DataFrame(train_wt).values
# X_test_gpu = cudf.DataFrame(test_wt).values
# 
# modelo_svm2 = svm_classifier.fit(X_train_gpu, y_train_gpu)
# 
# y_svm2 = modelo_svm2.predict(X_test_gpu)
# 
# accuracy = accuracy_score(test_y, y_svm2)
# print(f'Precisión del modelo SVM Classifier en GPU: {accuracy * 100:.2f}%')

"""DATOS SIN PROCESAR:"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# X_train_gpu = cudf.DataFrame(train_x).values
# X_test_gpu = cudf.DataFrame(test_x).values
# 
# modelo_svm2 = svm_classifier.fit(X_train_gpu, y_train_gpu)
# 
# y_svm2 = modelo_svm2.predict(X_test_gpu)
# 
# accuracy = accuracy_score(test_y, y_svm2)
# print(f'Precisión del modelo SVM Classifier en GPU: {accuracy * 100:.2f}%')

"""En este modelo tanto la normalización como la wavelet transform añaden capacidad predictiva al modelo. Sin embargo, al igual que en Random Forest, la predictividad baja al añadir stacked autoencoders al preprocesamiento. Abajo el mejor de los modelos (WT + NORM) presenta un porcentaje de acierto sobre las señales de compra de un 63.80%."""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

confusion = confusion_matrix(test_y, y_svm1.get())

confusion_percentage = confusion.astype('float') / confusion.sum(axis=0)[np.newaxis, :]

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_percentage, annot=True, fmt=".2%", cmap="Blues", xticklabels=["Negativo", "Positivo"], yticklabels=["Negativo", "Positivo"])
plt.xlabel('Predicciones')
plt.ylabel('Etiquetas Reales')
plt.title('Matriz de Confusión en Porcentajes')
plt.show()

"""Comprobamos las predicciones de todos los modelos hasta ahora."""

y_lr

y_rf1

y_xgb

y_svm1

test_y

"""Las unimos en un mismo dataframe."""

results_pred = pd.DataFrame({
    'LR_pred': y_lr,
    'RF_pred': y_rf1,
    'XGB_pred': y_xgb,
    'SVM_pred': y_svm1.get(),
    'Y_real': test_y
})

results_pred['XGB_pred'] = results_pred['XGB_pred'].astype(float)

results_pred

"""Las exportamos a la base de datos SQL porque a continuación vamos a cambiar el entorno de ejecución a TPU para entrenar las redes neuronales y no queremos que se pierdan las variables locales."""

def crear_tabla_desde_dataframe(df, tabla, connection):
    cursor = connection.cursor()

    create_table_query = f"CREATE TABLE {tabla} (id INT AUTO_INCREMENT PRIMARY KEY"

    for columna in df.columns:
        create_table_query += f", `{columna}` FLOAT"

    create_table_query += ");"
    cursor.execute(create_table_query)

crear_tabla_desde_dataframe(results_pred, 'results_pred', connection)

from sqlalchemy import create_engine

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

def cargar_dataframe_en_tabla(df, tabla):
    df.to_sql(tabla, con = engine, if_exists='replace', index=False)

cargar_dataframe_en_tabla(results_pred, 'results_pred')

connection.close()
engine.dispose()

"""###<u>Long Short-Term Memory</u>

Para entrenar las redes neuronales recurrentes LSTM primero comprobamos que el entorno de ejecución está correctamente conectado a la TPU de Google.
"""

import tensorflow as tf

# Detectar si estamos usando una TPU
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    print('Connected to a TPU.')
except ValueError:
    tpu = None
    print('Not connected to a TPU.')

"""Nos cercioramos de que la capacidad de la RAM es de 37.8 GB."""

# Comprobamos la capacidad de la RAM

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

"""Dado que LSTM es una red neuronal recurrente, es decir, que tiene capacidad de memoria, hay que alimentarla con matrices tridimensionales en vez de bidimensionales. Para ello, definimos una función que divida los datos en grupos de 30 sesiones sucesivas (este número se ha determinado mediante prueba y error)."""

import numpy as np

def data_delay(x_data, y_data, delay):
    x_data = np.array(x_data)
    y_data = np.array(y_data)
    X = np.array([x_data[i - delay:i, :].copy() for i in range(delay, len(x_data))])
    y = np.array(y_data[delay:])

    print('Dimensiones de X (número de secuencias, longitud de la secuencia, número de características): ', X.shape)
    print('Dimensiones de y: ', y.shape)

    return X, y

X_train, Y_train = data_delay(train_sae, train_y, 30)

"""Aplicar la función de transformación a los datos de dev, que se utilizarán para evaluar el modelo en el entrenamiento de las redes, y a los datos test para poder evaluar el modelo en general."""

X_val, Y_val = data_delay(dev_sae, dev_y, 30)

X_test, Y_test = data_delay(test_sae, test_y, 30)

"""Definimos el modelo con tensorflow. Una capa de entrada con las dimensiones especificadas (30 días, 47 variables), 3 capas ocultas de tipo LSTM con 200 neuronas cada una y una última capa de salida con la función 'sigmoid' para que el output de la predicción sea un 1 o un 0. Entre las capas ocultas se ha introducido un ratio Dropout de 30% para evitar el sobreajuste (overfitting). Se establece 'unroll=True' para optimizar el rendimiento de entrenamiento desenrollando las iteraciones de tiempo durante la fase de compilación.

La función de pérdida especificada es la entropía binaria y la métrica es la precisión. El objeto 'early_stopping' ahorra recursos computacionales y previene el sobreajuste, ya que para el entrenamiento cuando no se ha producido una mejora de al menos 0.001 en la precisión en el conjunto de validación durante 50 epochs. Cuando llega a este punto, se reinstauran los pesos del modelo que mejor precisión ha obtenido. Una vez definido todos estos parámetros mediante prueba y error, entrenamos el modelo en 1000 epochs como máximo y con un tamaño de batch de 130.

Documentación tensorflow: https://www.tensorflow.org/api_docs

Documentación LSTM: https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM

DATOS PROCESADOS (WT + NORM + SAE):
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, InputLayer, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping

# Creamos un modelo LSTM
model = Sequential()

drop_rate = 0.3

# Agregamos las capas de neuronas
model.add(InputLayer(input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(LSTM(200, return_sequences=True, unroll=True))
model.add(Dropout(drop_rate))
model.add(LSTM(200, return_sequences=True, unroll=True))
model.add(Dropout(drop_rate))
model.add(LSTM(200, unroll=True))
model.add(Dense(1, activation='sigmoid'))  # Capa de salida binaria

# Compilamos el modelo
model.compile(optimizer=tf.keras.optimizers.Adam(),
              loss='binary_crossentropy',
              metrics=['accuracy'])

early_stopping = EarlyStopping(
    monitor='val_accuracy',  # Monitorizar la precisión de validación
    min_delta=0.001,         # Cambio mínimo para considerar una mejora
    patience=50,             # Número de épocas a esperar antes de detenerse
    restore_best_weights=True
)

# Entrenamos el modelo
history = model.fit(X_train, Y_train, epochs=1000, batch_size=130, callbacks=[early_stopping], validation_data=(X_val, Y_val))

"""Evaluamos el modelo entrenado en test."""

# Evaluar el modelo en el conjunto de prueba
loss, accuracy = model.evaluate(X_test, Y_test)
print(f'Precisión en el conjunto de prueba: {accuracy * 100:.2f}%')

"""Obtenemos las predicciones de este modelo."""

# Obtener las predicciones para el conjunto de prueba
y_lstm = model.predict(X_test)

# Convertir las probabilidades en etiquetas de clase (0 o 1)
y_lstm = (y_pred > 0.5).astype(int)

print(len(y_lstm))

"""DATOS PROCESADOS (WT + NORM):

Preprocesamos los datos con la función e indicamos 'verbose=0' para que no se imprima el entrenamiento en epochs y salga directamente el resultado.
"""

X_train, Y_train = data_delay(train_norm, train_y, 30)
X_val, Y_val = data_delay(dev_norm, dev_y, 30)
X_test, Y_test = data_delay(test_norm, test_y, 30)

lstm1 = model.fit(X_train, Y_train, epochs=1000, batch_size=130, callbacks=[early_stopping], validation_data=(X_val, Y_val), verbose=0)

loss, accuracy = model.evaluate(X_test, Y_test)
print(f'Precisión en el conjunto de prueba: {accuracy * 100:.2f}%')

"""DATOS PROCESADOS (WT):"""

X_train, Y_train = data_delay(train_wt, train_y, 30)
X_val, Y_val = data_delay(dev_wt, dev_y, 30)
X_test, Y_test = data_delay(test_wt, test_y, 30)

lstm2 = model.fit(X_train, Y_train, epochs=1000, batch_size=130, callbacks=[early_stopping], validation_data=(X_val, Y_val), verbose=0)

loss, accuracy = model.evaluate(X_test, Y_test)
print(f'Precisión en el conjunto de prueba: {accuracy * 100:.2f}%')

"""DATOS NO PROCESADOS:"""

X_train, Y_train = data_delay(train_x, train_y, 30)
X_val, Y_val = data_delay(dev_x, dev_y, 30)
X_test, Y_test = data_delay(test_x, test_y, 30)

lstm3 = model.fit(X_train, Y_train, epochs=1000, batch_size=130, callbacks=[early_stopping], validation_data=(X_val, Y_val), verbose=0)

loss, accuracy = model.evaluate(X_test, Y_test)
print(f'Precisión en el conjunto de prueba: {accuracy * 100:.2f}%')

"""En este caso parece que el único paso del preprocesamiento que ha aportado valor predictivo es la normalización. De todas formas, la capacidad predictiva de las redes neuronales es pobre y puede que la poca diferencia que hay entre el modelo con stacked autoencoders y sin stacked autoencoders se deba al azar. En la matriz de confusión se observa un porcentaje de acierto del 55.10%."""

from keras.layers import LSTM
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

y_lstm = model.predict(X_test)

# Aplicar un umbral de 0.5 para convertir las probabilidades en 1 o 0
y_lstm_binary = (y_lstm >= 0.5).astype(int).ravel()

test_y_lstm = test_y[30:]

confusion = confusion_matrix(test_y_lstm, y_lstm_binary)

confusion_percentage = confusion.astype('float') / confusion.sum(axis=0)[np.newaxis, :]

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_percentage, annot=True, fmt=".2%", cmap="Blues", xticklabels=["Negativo", "Positivo"], yticklabels=["Negativo", "Positivo"])
plt.xlabel('Predicciones')
plt.ylabel('Etiquetas Reales')
plt.title('Matriz de Confusión en Porcentajes')
plt.show()

"""A continuación se imprime la función de pérdida y la precisión del conjunto de entrenamiento y validación durante el entrenamiento (el eje x son las epochs). Se puede observar claramente que el modelo tiende al sobreajuste, pero los métodos introducidos para reducir el sobreajuste mostraron una capacidad predictiva en test aún peor, por lo que se ha decido dejarlo así."""

history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot();
print("Minimum validation loss: {}".format(history_df['val_loss'].min()))

history_df = pd.DataFrame(history.history)
history_df.loc[:, ['accuracy', 'val_accuracy']].plot();
print("Maximum validation accuracy: {}".format(history_df['val_accuracy'].max()))

"""###<u>KNN + XGBoost</u>

Ahora vamos a utilizar una combinación del mejor de los algoritmos anteriores y K-nearest neighbors (KNN). La idea es preprocesar los datos de entrenamiento con un modelo de clasificación no supervisada. De esta manera se dividirán las sesiones en clases por su similitud con otras sesiones. Posteriormente, se entrenará un algoritmo XGBoost para cada una de esas clases. Las sesiones de test serán clasificadas en base a los grupos de train, y se empleará un algoritmo XGBoost entrenado u otro en base a la clase a la que pertenezca esa sesion de test.

Documentación KMeans: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
"""

knn_x = np.array(train_sae)

"""Para obtener el número óptimo de K o clases, realizamos el método del codo."""

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Rango de valores de k que deseas probar
k_values = range(1, 20)

# Lista para almacenar las inercias (suma de distancias al cuadrado)
inertias = []

# Realizar K-Means clustering para diferentes valores de k
for k in k_values:
    kmeans = KMeans(n_clusters=k, n_init=10)
    kmeans.fit(knn_x)
    inertias.append(kmeans.inertia_)

# Graficar la inercia en función de k
plt.figure(figsize=(8, 5))
plt.plot(k_values, inertias, marker='o')
plt.title('Método del Codo')
plt.xlabel('Número de Clusters (k)')
plt.ylabel('Inercia')
plt.grid(True)
plt.show()

"""Parece que a partir de 3 o 4 k la pendiente de la función disminuye, pero como no está muy claro vamos a realizar también la prueba de la shilouette, en la que por fin despejamos las dudas y determinamos 3 como el número óptimo de clusters."""

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Rango de valores de k que deseas probar
k_values = range(2, 11)

# Lista para almacenar los valores de silhouette para cada k
silhouette_scores = []

for k in k_values:
    kmeans = KMeans(n_clusters=k, n_init=10)
    cluster_labels = kmeans.fit_predict(knn_x)

    # Calcular el valor de silhouette para este k
    silhouette_avg = silhouette_score(knn_x, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Graficar el valor de silhouette en función de k
plt.figure(figsize=(8, 5))
plt.plot(k_values, silhouette_scores, marker='o')
plt.title('Método Silhouette')
plt.xlabel('Número de Clusters (k)')
plt.ylabel('Valor de Silhouette')
plt.grid(True)
plt.show()

"""Ajustamos el modelo y vemos el número de sesiones por cada una de las 3 clases."""

from sklearn.cluster import KMeans

# Creamos un objeto KMeans con el número deseado de clústeres (k)
k = 3
kmeans = KMeans(n_clusters=k, n_init='auto', random_state = 42)

# Ajustamos el modelo K-Means a los datos
kmeans.fit(knn_x)

# Obtenemos las etiquetas de clúster asignadas a cada muestra
train_labels = kmeans.labels_

# Declaramos una variable con los centros de los clusters
cluster_centers = kmeans.cluster_centers_

cluster_counts = np.bincount(train_labels)

# cluster_counts[i] contiene el número de elementos en el clúster i
for i, count in enumerate(cluster_counts):
    print(f'Número de elementos en el clúster {i}: {count}')

"""Como ahora vamos a pasar a un problema de clasificación supervisada, vamos a necesitar las etiquetas de los datos. Para ello definimos la siguiente función, que juntará las sesiones por sus clases y añadirá las etiquetas (variable objetivo)."""

def data_transform(x, labels, y):
    knn_x_label = np.column_stack((x[:, :47], labels, y))

    cluster_1 = []
    cluster_2 = []
    cluster_3 = []

    for array in knn_x_label:
        if array[-2] == 1:  # Usar el índice -2 para acceder a la columna 47
            cluster_1.append(array)
        elif array[-2] == 2:
            cluster_2.append(array)
        else:
            cluster_3.append(array)

    cluster_1 = np.array(cluster_1)
    cluster_2 = np.array(cluster_2)
    cluster_3 = np.array(cluster_3)

    cluster_1 = np.delete(cluster_1, -2, axis=1)
    cluster_2 = np.delete(cluster_2, -2, axis=1)
    cluster_3 = np.delete(cluster_3, -2, axis=1)

    cluster_1_y = cluster_1[:, -1]
    cluster_2_y = cluster_2[:, -1]
    cluster_3_y = cluster_3[:, -1]

    cluster_1_x = cluster_1[:, :-1]
    cluster_2_x = cluster_2[:, :-1]
    cluster_3_x = cluster_3[:, :-1]

    return cluster_1_x, cluster_2_x, cluster_3_x, cluster_1_y, cluster_2_y, cluster_3_y

train_1_x, train_2_x, train_3_x, train_1_y, train_2_y, train_3_y = data_transform(knn_x, train_labels, train_y)

"""Comprobamos que la función ha procesado los datos correctamente."""

print('Dimensiones de train_1_x:', train_1_x.shape)
print('Dimensiones de train_2_x:', train_2_x.shape)
print('Dimensiones de train_3_x:', train_3_x.shape)
print('Dimensiones de train_1_y:', train_1_y.shape)
print('Dimensiones de train_2_y:', train_2_y.shape)
print('Dimensiones de train_3_y:', train_3_y.shape)

"""Al clasificar el conjunto train en base a los clusters ajustados en el conjunto de entrenamiento se observa que todos pertenecen al mismo cluster. Esto evitará que tengamos que entrenar el modelo 3 veces, ya que solo necesitaremos entrenarlo con los datos de entrenamiento del cluster 0."""

knn_test = np.array(test_sae)
test_labels = kmeans.predict(test_sae)

cluster_counts = np.bincount(test_labels)

for i, count in enumerate(cluster_counts):
    print(f'Número de elementos en el clúster {i}: {count}')

"""Comprobamos los conjuntos de train y test."""

print(train_3_x.shape)
train_3_x[0]

print(train_3_y.shape)
train_3_y

print(knn_test.shape)
knn_test[0]

print(test_y.shape)
test_y

"""Al entrenar y ajustar los hiperparámetros del modelo XGBoost nos damos cuenta de que podemos alcanzar la misma precisión que alcanzó el modelo con 1752 registros pero esta vez con tan solo los 680 registros más similares a los datos de test. Esto puede ser muy útil a la hora de entrenar conjuntos de datos de miles de registros, ya que se puede ahorrar recursos computacionales solo entrenándo los modelos con los registros más similares."""

import xgboost as xgb
from sklearn.metrics import accuracy_score

xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42, n_estimators=71, learning_rate=0.1, max_depth=1,
                              subsample=0.6, colsample_bytree=0.6, reg_alpha=0.001, reg_lambda=1)

modelo_xgb = xgb_model.fit(train_3_x, train_3_y)

y_xgb3 = modelo_xgb.predict(knn_test)

accuracy = accuracy_score(test_y, y_xgb3)
print(f'Precisión del modelo XGBoost Classifier: {accuracy * 100:.2f}%')

"""La precisión es mucho menor con los grupos con los que hay menos similitudes."""

modelo_xgb = xgb_model.fit(train_1_x, train_1_y)

y_xgb1 = modelo_xgb.predict(knn_test)

accuracy = accuracy_score(test_y, y_xgb1)
print(f'Precisión del modelo XGBoost Classifier: {accuracy * 100:.2f}%')

modelo_xgb = xgb_model.fit(train_2_x, train_2_y)

y_xgb2 = modelo_xgb.predict(knn_test)

accuracy = accuracy_score(test_y, y_xgb2)
print(f'Precisión del modelo XGBoost Classifier: {accuracy * 100:.2f}%')

"""Imprimimos la matriz de confusión para ver la precisión de las predicciones positivas y negativas."""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

confusion = confusion_matrix(test_y, y_xgb3)

confusion_percentage = confusion.astype('float') / confusion.sum(axis=0)[np.newaxis, :]

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_percentage, annot=True, fmt=".2%", cmap="Blues", xticklabels=["Negativo", "Positivo"], yticklabels=["Negativo", "Positivo"])
plt.xlabel('Predicciones')
plt.ylabel('Etiquetas Reales')
plt.title('Matriz de Confusión en Porcentajes')
plt.show()

"""# Resultados

Importamos el dataframe anterior a LSTM.
"""

!pip install protobuf mysql-connector-python > /dev/null
import mysql.connector

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

from sqlalchemy import create_engine, text
import pandas as pd

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

conn = engine.connect()

query = text(f'SELECT * FROM results_pred')
results_pred = pd.read_sql(query, con=conn)

conn.close()
connection.close()

results_pred

"""Añadimos las predicciones de KNN+XGBoost y LSTM al dataframe de predicciones."""

y_lstm_padded = np.pad(y_lstm_binary, (0, len(results_pred) - len(y_lstm_binary)), mode='constant', constant_values=-1)

results_pred['LSTM_pred'] = y_lstm_padded

results_pred['KNNXGB_pred'] = pd.Series(y_xgb3, index=results_pred.index)

results_pred.replace(-1, np.nan, inplace=True)

results_pred

"""Los valores nulos de LSTM (producidos por la longitud de 30 días de la matriz) se sustituyen por -1 en vez de NaN para evitar problemas de incompatibilidad. Se convierte la matriz a número enteros."""

import numpy as np

results_pred['LSTM_pred'] = results_pred['LSTM_pred'].shift(30)

results_pred['LSTM_pred'].fillna(-1, inplace=True)

for col in results_pred.columns:
    results_pred[col] = pd.to_numeric(results_pred[col], errors='coerce', downcast='integer')


results_pred

"""Para evaluar las combinaciones de varios algoritmos se van a juntar empezando por los que mejor capacidad predictiva tienen. La idea es que solo se opera al día siguiente si todos los algoritmos dan una señal de entrada positiva."""

results_pred['KNNXGB_XGB'] = np.where((results_pred['XGB_pred'] == 1) & (results_pred['KNNXGB_pred'] == 1), 1, 0)
results_pred['KNNXGB_XGB_RF'] = np.where((results_pred['RF_pred'] == 1) & (results_pred['KNNXGB_XGB'] == 1), 1, 0)
results_pred['KNNXGB_XGB_RF_SVM'] = np.where((results_pred['SVM_pred'] == 1) & (results_pred['KNNXGB_XGB_RF'] == 1), 1, 0)
results_pred['KNNXGB_XGB_RF_SVM_LR'] = np.where((results_pred['LR_pred'] == 1) & (results_pred['KNNXGB_XGB_RF_SVM'] == 1), 1, 0)
results_pred['NO_pred'] = 1

results_pred

results_arrays = [results_pred[col].values for col in results_pred.columns]

"""###<u>Precisión</u>

Evaluamos la precisión añadiendo una estrategia que consistiera en operar todos los días (NO_pred).
"""

from sklearn.metrics import accuracy_score

for col in results_pred.columns:
    if col == 'Y_real':
      continue
    elif col == 'LSTM_pred':
      lstm_y = results_pred[col][30:]
      real_y = results_pred['Y_real'][30:] # los 30 primeros son los nulos, no los últimos
      accuracy = accuracy_score(lstm_y, real_y)
      print(f'Precisión de {col}: {accuracy * 100:.2f}%')
    else:
      accuracy = accuracy_score(results_pred[col], results_pred['Y_real'])
      print(f'Precisión de {col}: {accuracy * 100:.2f}%')

"""Calculamos el total de incrementos en la vela del día siguiente que capta cada algoritmo."""

for column in results_pred.columns:
    if column != 'Y_real':
        correct_predictions = ((results_pred[column] == 1) & (results_pred['Y_real'] == 1)).sum()
        total_positive_actual = (results_pred['Y_real'] == 1).sum()

        accuracy = correct_predictions / total_positive_actual * 100 if total_positive_actual > 0 else 0

        print(f'Porcentaje de veces que Y_real es 1 y {column} predice 1: {accuracy:.2f}%')

"""Calculamos la fiabilidad que tiene cada algoritmo: el porcentaje de veces que predice que la vela al día siguiente va a subir y acaba subiendo."""

for column in results_pred.columns:
    if column != 'Y_real':
        correct_predictions = ((results_pred[column] == 1) & (results_pred['Y_real'] == 1)).sum()
        total_positive_predictions = (results_pred[column] == 1).sum()

        accuracy = correct_predictions / total_positive_predictions * 100 if total_positive_predictions > 0 else 0

        print(f'Porcentaje de veces que {column} predice 1 e Y_real es 1: {accuracy:.2f}%')

"""Para el cálculo de la rentabilidad importamos el dataframe nasdaq."""

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

conn = engine.connect()

query = text(f'SELECT * FROM nasdaq_eda')
nasdaq = pd.read_sql(query, con=conn)

conn.close()
connection.close()

nasdaq

nasdaq_test = nasdaq[-501:]

nasdaq_test = nasdaq_test.reset_index(drop=True)


nasdaq_test

"""Hacemos merge con las predicciones."""

nasdaq_pred = nasdaq_test.merge(results_pred, left_index=True, right_index=True)
nasdaq_pred.set_index('Date', inplace=True)

nasdaq_pred

"""###<u>Rentabilidad</u>

Primero calculamos la rentabilidad con interés compuesto total obtenida desde el incio del período (1/7/2021) hasta el final de cada año con una estrategia Buy&Hold (comprar el NASDAQ y mantenerlo en cartera sin venderlo).
"""

# Calcular los rendimientos diarios
returns = nasdaq_pred['Close'].pct_change().dropna()

# Crear un DataFrame para almacenar las rentabilidades anuales acumuladas
annual_returns = pd.DataFrame(index=returns.index.year.unique(), columns=['Annual Return'])

# Inicializar una variable para almacenar la rentabilidad acumulada
cumulative_return = 0

# Calcular y acumular la rentabilidad acumulada por año
for year in annual_returns.index:
    # Filtrar los rendimientos hasta el final del año actual
    year_returns = returns[returns.index.year <= year]

    # Calcular la rentabilidad acumulada utilizando la fórmula del interés compuesto
    cumulative_return = (1 + year_returns).prod() - 1

    # Asignar el valor al DataFrame de rentabilidades anuales acumuladas
    annual_returns.loc[year, 'Annual Return'] = cumulative_return

# Formatear la columna 'Annual Return' como porcentaje con dos decimales
annual_returns['Annual Return'] = annual_returns['Annual Return'].map('{:.2%}'.format)

annual_returns

"""Imprimimos la rentabilidad con interés compuesto cada año, es decir, desde el inicio de cada año hasta el final de cada año con la estrategia Buy&Hold."""

returns = nasdaq_pred['Close'].pct_change().dropna()

annual_returns = pd.DataFrame(index=returns.index.year.unique(), columns=['Annual Return'])

# Calcular la rentabilidad del último día del año respecto al primer día de ese año
for year in annual_returns.index:
    year_returns = returns[returns.index.year == year]
    annual_return = (1 + year_returns).prod() - 1
    annual_returns.loc[year, 'Annual Return'] = annual_return

annual_returns['Annual Return'] = annual_returns['Annual Return'].map('{:.2%}'.format)


annual_returns

"""Ahora calculamos la rentabilidad por interés compuesto acumulada desde el inicio del período hasta el final de cada año con la estrategia seguida por cada uno de nuestros algoritmos."""

# Crear un diccionario para almacenar las rentabilidades anuales
rentabilidades_anuales = {}

nasdaq_pred['Rendimiento Diario'] = nasdaq_pred['Close'] / nasdaq_pred['Open'] - 1

# Calcular las rentabilidades anuales para cada 'pred'
for pred in results_pred.columns:
    # Filtrar los días en que se compra y vende
    dias_compra_venta = nasdaq_pred[nasdaq_pred[pred].shift(1) == 1].copy()

    # Calcular el rendimiento acumulado diario para los días de compra y venta
    dias_compra_venta['Rendimiento Acumulado Diario'] = (1 + dias_compra_venta['Rendimiento Diario']).cumprod() - 1

    # Agrupar por año y calcular el rendimiento anual
    rendimiento_anual = dias_compra_venta.groupby(dias_compra_venta.index.year)['Rendimiento Acumulado Diario'].last()

    # Almacenar el rendimiento anual en el diccionario
    rentabilidades_anuales[pred] = rendimiento_anual

# Crear un DataFrame a partir del diccionario
rentabilidades_acumuladas_totales = pd.DataFrame(rentabilidades_anuales)

rentabilidades_acumuladas_totales = rentabilidades_acumuladas_totales.applymap('{:.2%}'.format)

rentabilidades_acumuladas_totales

"""Hacemos lo mismo pero dividiendo por años: desde el inicio al final de cada año.


"""

rentabilidades_anuales = {}

nasdaq_pred['Rendimiento Diario'] = nasdaq_pred['Close'] / nasdaq_pred['Open'] - 1

for pred in results_pred.columns:
    dias_compra_venta = nasdaq_pred[nasdaq_pred[pred].shift(1) == 1].copy()

    dias_compra_venta['Rendimiento Diario'] = dias_compra_venta['Close'] / dias_compra_venta['Open'] - 1

    dias_compra_venta['Rendimiento Acumulado Diario'] = (1 + dias_compra_venta['Rendimiento Diario']).cumprod() - 1

    agupamiento_anual = dias_compra_venta.groupby(dias_compra_venta.index.year)

    rentabilidades_pred = []

    for year, group in agupamiento_anual:
        rendimiento_anual = ((1 + group['Rendimiento Diario']).prod() - 1)

        rentabilidades_pred.append(rendimiento_anual)

    rentabilidades_anuales[pred] = rentabilidades_pred

rentabilidades_acumuladas_anuales = pd.DataFrame(rentabilidades_anuales, index=dias_compra_venta.index.year.unique())

rentabilidades_acumuladas_anuales = rentabilidades_acumuladas_anuales.applymap('{:.2%}'.format)

rentabilidades_acumuladas_anuales

"""###<u>Riesgo</u>

Calculamos el Value at Risk (VaR) para la estrategia Buy&Hold y para los algoritmos.

VaR
"""

returns = nasdaq_pred['Close'].pct_change().dropna()
alpha = 0.05  # Nivel de confianza del 95%
var = returns.quantile(alpha)
print(f'VaR al {alpha * 100}%: {var:.2%}')

nasdaq_pred['Rendimiento Diario'] = nasdaq_pred['Close'] / nasdaq_pred['Open'] - 1

alpha = 0.05

for pred in results_pred.columns:
    dias_compra_venta = nasdaq_pred[nasdaq_pred[pred].shift(1) == 1].copy()

    # Calcular el VaR para el rendimiento diario
    var = dias_compra_venta['Rendimiento Diario'].quantile(alpha)
    print(f'VaR al {alpha * 100}% para {pred}: {var:.2%}')

"""Maximum Drawdown

Calculamos la pérdida máxima en cada una de las estrategias.
"""

cumulative_returns = (1 + returns).cumprod()
peak = cumulative_returns.cummax()
drawdown = (cumulative_returns - peak) / peak
max_drawdown = drawdown.min()
print(f'Maximum Drawdown: {max_drawdown:.2%}')

for pred in results_pred.columns:
    dias_compra_venta = nasdaq_pred[nasdaq_pred[pred].shift(1) == 1].copy()

    dias_compra_venta['Rendimiento Acumulado Diario'] = dias_compra_venta['Rendimiento Diario'].cumsum()

    # Calcular el Máximo Drawdown para cada 'pred'
    cumulative_returns = (1 + dias_compra_venta['Rendimiento Diario']).cumprod()
    peak = cumulative_returns.cummax()
    drawdown = (cumulative_returns - peak) / peak
    max_drawdown = drawdown.min()
    print(f'Maximum Drawdown para {pred}: {max_drawdown:.2%}')

"""Sharpe Ratio

Calculamos el Sharpe Ratio para ver la rentabilidad ajustada al riesgo. El ratio libre de riesgo escogido es la rentabilidad del bono americano a 10 años (US10YT) el 13 de octubre de 2023: 4.62%.
"""

returns = nasdaq_pred['Close'].pct_change().dropna()
cumulative_returns = (1 + returns).cumprod() - 1

risk_free_rate = 0.0462  # Tasa libre de riesgo

sharpe_ratio = (cumulative_returns[-1] - risk_free_rate) / cumulative_returns.std()

print(f'Sharpe Ratio: {sharpe_ratio:.2f}')

for pred in results_pred.columns:
    dias_compra_venta = nasdaq_pred[nasdaq_pred[pred].shift(1) == 1].copy()

    dias_compra_venta['Rendimiento Diario'] = dias_compra_venta['Close'] / dias_compra_venta['Open'] - 1
    dias_compra_venta['Rendimiento Acumulado Diario'] = (1 + dias_compra_venta['Rendimiento Diario']).cumprod() - 1

    # Calcular el Sharpe Ratio
    cumulative_returns = dias_compra_venta['Rendimiento Acumulado Diario']
    sharpe_ratio = (cumulative_returns.iloc[-1] - risk_free_rate) / cumulative_returns.std()

    print(f'Sharpe Ratio para {pred}: {sharpe_ratio:.2f}')

"""Guardamos el dataframe que combina las predicciones con el NASDAQ en la base de datos SQL."""

nasdaq_pred.reset_index(inplace=True)
nasdaq_pred.drop('Rendimiento Diario', axis=1, inplace=True)

nasdaq_pred

config = {
    'user': 'sql7644199',
    'password': 'DuWlZYlgBP',
    'host': 'db4free.net',
    'database': 'sql7644199',
    }

try:
    connection = mysql.connector.connect(**config)

    if connection.is_connected():
        print('Conexión a la base de datos establecida')
    else:
        print('No se pudo conectar a la base de datos')

except mysql.connector.Error as e:
    print(f'Error al conectar a la base de datos: {e}')

def crear_tabla_desde_dataframe(df, tabla, connection):
    cursor = connection.cursor()

    create_table_query = f"CREATE TABLE {tabla} (id INT AUTO_INCREMENT PRIMARY KEY"

    for columna in df.columns:
        create_table_query += f", `{columna}` FLOAT"

    create_table_query += ");"
    cursor.execute(create_table_query)

# Llamada a la función con las listas anteriores
crear_tabla_desde_dataframe(nasdaq_pred, 'nasdaq_pred', connection)

from sqlalchemy import create_engine

engine = create_engine('mysql+mysqlconnector://sql7644199:DuWlZYlgBP@db4free.net/sql7644199')

def cargar_dataframe_en_tabla(df, tabla):
    df.to_sql(tabla, con = engine, if_exists='replace', index=False)


cargar_dataframe_en_tabla(nasdaq_pred, 'nasdaq_pred')

connection.close()
engine.dispose()

"""###<u>Conclusiones</u>

1.   El modelo con mayor precisión es la combinación de KNN+XGB, XGB y Random Forest con una precisión del 63.07%
2.   El modelo que mayor número de positivos reales capta es Random Forest con un 76.63%.
3.   El modelo que más fiable a la hora de dar una señal positiva es la combinación de KNN+XGB, XGB, Random Forest, SVM y Logistic Regression, siendo un 68.52% las veces que predice uno y efectivamente acaba siendo uno.  
4.   El modelo más rentable al cabo de 2021, 2022 y 2023 acaba siendo la combinación de KNNXGB, XGB y Random Forest, con un 155,60% de rentabilidad.
5.   Los modelos con mejor VaR al 5% son KNN+XGB, XGB y Random Forest con o sin SVM, con un -1.34%.
6.   La menor máxima caída (maximum drawdown) la experimentaron todos los modelos que contenían la combinación de KNN+XGB y XGB: -3.60%.
7.   La mejor rentabilidad ajustada a la volatilidad fue obtenida por el modelo de regresión logística, con un Sharpe Ratio igual a 3.06.
"""